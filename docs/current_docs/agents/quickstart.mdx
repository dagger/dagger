---
slug: /ai-agents/quickstart
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Quickstart

Some basic opening text here about what the quickstart is going to accomplish.

## Installation and configuration

Link to installation and LLM provider configuration instructions.

## Give an LLM tools

### Check the current default LLM

Run `dagger` to enter a dagger shell, then:

```bash
llm | model
```

If you've correctly configured the LLM provider, you should see the model name. To change the model, run `.model {model name}`

### Create a variable that points to a weather module

The `weather` variable will reference the weather module on GitHub

```plaintext
weather=$(github.com/kpenfound/dag/weather)
```

### Enter prompt mode and let the LLM use the weather module

Enter prompt mode with `>`

Let the LLM use the weather module:
```plaintext
what is the current weather in paris?
```

You should see the LLM find the function `Weather.City` in the weather module and call it with the argument `Paris`.

## Write your first agent

Create an agent module that uses the weather module

### Initialize a new module

Exit the dagger shell session if you haven't already (Ctrl+D)
In an empty directory create a module with the SDK of your choice. You can use Go, Python, Typescript, PHP, or Java:

```bash
dagger init --sdk {your choice} --name my-agent
```

### Install the weather module as a dependency:

```bash
dagger install github.com/kpenfound/dag/weather
```

### Create a new function with an LLM

<Tabs groupId="language">
<TabItem value="Go">

Edit the agent (`main.go`) and replace the generated functions with this new one:
```go
func (m *MyAgent) Chat(ctx context.Context, prompt string) (string, error) {
	return dag.Llm().
		WithWeather(dag.Weather()).
		WithPrompt(prompt).
		LastReply(ctx)
}
```

</TabItem>
<TabItem value="Python">

Edit the agent (`src/myagent/main.py`) and replace the generated functions with this new one:
```python
@function
async def chat(self, prompt: str) -> str:
    return (
        await dag.llm()
    		.with_weather(dag.weather())
    		.with_prompt(prompt)
    		.last_reply()
    )
```

</TabItem>
<TabItem value="Typescript">

Edit the agent (`src/index.ts`) and replace the generated functions with this new one:
```typescript
@func()
async chat(prompt: string): Promise<string> {
	return dag.Llm().
		withWeather(dag.weather()).
		withPrompt(prompt).
		lastReply()
}
```

</TabItem>
</Tabs>

### Now run your new function in the Dagger shell

Run `dagger` to enter the shell, and then run your new function:

```bash
chat "do I need to wear a jacket today in Paris?"
```

## Next steps

Look at the [Dagger AI Agents](/ai-agents#examples) page for more examples and ideas for your agent.
