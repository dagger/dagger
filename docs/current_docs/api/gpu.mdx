---
slug: /api/gpu
---

# Using a GPU

Dagger has experimental support for Nvidia GPUs.

:::important
The GPU support is currently experimental and only works with Nvidia GPUs.
:::

In order to use a GPU, Dagger needs a [custom runner](../configuration/custom-runner/), with the GPU preconfigured.

Assuming Dagger is already installed, here is a simple command that can replace the current engine with a GPU-enabled runner on your local machine:

```shell
VERSION=$(dagger version | cut -d' ' -f2)
docker rm -f dagger-engine-${VERSION} 2>/dev/null && docker run --gpus all -d --privileged -e _EXPERIMENTAL_DAGGER_GPU_SUPPORT=true --name dagger-engine-${VERSION} registry.dagger.io/engine:${VERSION}-gpu -- --debug
```

Let's explore two other ways to use a Nvidia GPU on a remote infrastructure: [Fly.io](https://fly.io/) and [Lambda Labs](https://lambdalabs.com/).

Note that you can easily reuse those instructions on other Cloud providers, as long as you're using an Instance that has access to an Nvidia GPU.

## On Fly.io

Assuming you already have access to an active account, here is a way to provision a Dagger Engine with GPU support on Fly:

In order for your local Dagger to have access to the remote Engine running on Fly, you need to have access to Fly private network via a VPN. If you haven't set it up, [follow the instructions for Fly Private networking](https://fly.io/docs/networking/private-networking/).

Note that this command is using by default the region `ord` and an Nvidia GPU `L40s`, but you can easily fork the code and adapt it to your own needs, [checkout the code here](https://github.com/samalba/dagger-modules/blob/7273f9528f91021cf0e41766349e1f3f9f5a90d7/nvidia-gpu/main.go#L34).

```shell
export FLYIO_TOKEN=<REPLACE WITH A FLY.IO TOKEN>
export FLYIO_ORG=<REPLACE WITH YOUR FLY.IO ORG NAME>
dagger -m github.com/samalba/dagger-modules/nvidia-gpu call deploy-dagger-on-fly --token env:FLYIO_TOKEN --org env:FLYIO_ORG
```

This functions returns a message in your terminal, with an environment variable to export, for example:

```shell
export _EXPERIMENTAL_DAGGER_RUNNER_HOST=tcp://dagger-v0-14-0-smart-gerhard-2024-12-06.internal:2345
```

Simply copy paste the export command in your active terminal. From now on, the dagger client is instructed to execute all the calls to the remote Engine, running on Fly.

As a simple example, let's test that we have access to the GPU:

```shell
dagger -m github.com/samalba/dagger-modules/nvidia-gpu call has-gpu
```

If the GPU is properly configured, this function returns `true`.

:::Important
In case you want to stop using the remote Engine on Fly and return to your local Dagger engine, simply run the following:

```shell
unset _EXPERIMENTAL_DAGGER_RUNNER_HOST
# Make sure the Fly app name matches the one that was provisioned earlier
dagger -m github.com/samalba/dagger-modules/nvidia-gpu call destroy-dagger-on-fly --token env:FLYIO_TOKEN --app dagger-v0-14-0-smart-gerhard-2024-12-06
```
:::

## On Lambda Labs

Lambda provides more traditional virtual machine with a GPU attached. In order to use this example, provision a single instance from the dashboard with any GPU from the list.

Note that the region or having a Filesystem attached does not alter any of the following examples.

Once the instance is booted, connect to it using ssh (copy the ssh command from the dashboard):

```shell
ssh ubuntu@<INSTANCE IP>
```

Lambda provides an instance with `Ubuntu 22.04.5 LTS`.

The default user `ubuntu` does not have access to the docker socket, let's fix it:

```shell
sudo usermod -aG docker ubuntu
```

In order for the permissions to take effect, you need to restart the shell session, simply exit and re-connect using the SSH command above.

Install Dagger using [the normal instructions for Linux](../install):

```shell
curl -fsSL https://dl.dagger.io/dagger/install.sh | BIN_DIR=$HOME/.local/bin sh
```

By default, the Dagger Engine does not have GPU support enabled, let's fix it:

```shell
VERSION=$(dagger version | cut -d' ' -f2)
docker rm -f dagger-engine-${VERSION} 2>/dev/null && docker run --gpus all -d --privileged -e _EXPERIMENTAL_DAGGER_GPU_SUPPORT=true --name dagger-engine-${VERSION} registry.dagger.io/engine:${VERSION}-gpu -- --debug
```

Now let's try to access the GPU:

```shell
dagger -m github.com/samalba/dagger-modules/nvidia-gpu call has-gpu
```

This function should return `true`, which confirms that Dagger can access the local GPU.

## Let's do some inference using Ollama

The following example with work on any Dagger Engine with a Nvidia GPU, no matter if you followed the Fly or Lambda Labs examples above, or if you're using a local Linux machine.

```shell
dagger -m github.com/samalba/dagger-modules/nvidia-gpu call ollama-run --prompt "What color is the sky?"
```

This function setup an Ollama server, pull a model and run a prompt. It returns the response query from the prompt passed as argument.

Feel free to fork the module and adapt it to your needs, it's provided as a simple example to show what's possible with an Nvidia GPU.
