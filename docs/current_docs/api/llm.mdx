---
slug: /api/llm
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# LLM Integration

Dagger's `LLM` core type includes API methods to attach objects to a Large Language Model (LLM), send prompts, and receive responses.

## Prompts

Use the `LLM.withPrompt()` API method to append prompts to the LLM context:

<Tabs groupId="shell">
<TabItem value="System shell">
```shell
dagger <<EOF
llm |
  with-directory https://github.com/dagger/dagger#main:/docs |
  with-prompt "You have a directory." |
  with-prompt "Use the tools in the directory to count the number of Markdown files."
EOF
```
</TabItem>
<TabItem value="Dagger Shell">
```shell title="First type 'dagger' for interactive mode."
llm |
  with-directory https://github.com/dagger/dagger#main:/docs |
  with-prompt "You have a directory." |
  with-prompt "Use the tools in the directory to count the number of Markdown files."
```
</TabItem>
</Tabs>

For longer or more complex prompts, use the `LLM.withPromptFile()` API method to read the prompt from a text file:

<Tabs groupId="shell">
<TabItem value="System shell">
```shell
dagger <<EOF
llm |
  with-directory https://github.com/dagger/dagger#main:/docs |
  with-prompt-file prompt.txt
EOF
```
</TabItem>
<TabItem value="Dagger Shell">
```shell title="First type 'dagger' for interactive mode."
llm |
  with-directory https://github.com/dagger/dagger#main:/docs |
  with-prompt-file $(host | file ./prompt.txt)
```
</TabItem>
</Tabs>

Dagger supports the use of variables in prompts. This allows you to interpolate results of other operations into an LLM prompt:

<Tabs groupId="shell">
<TabItem value="System shell">
```shell
dagger <<EOF
source=\$(container |
  from alpine |
  with-directory /src https://github.com/dagger/dagger |
  directory /src)
contents=\$(llm |
  with-directory \$source |
  with-prompt "You have a directory with source code." |
  with-prompt "The directory also has some tools available." |
  with-prompt "Use the tools in the directory to read the first paragraph of the README.md file in the directory." |
  with-prompt "Reply with only the selected text." |
  last-reply)
llm |
  with-prompt "Here is some text: \$contents. Translate it to French." |
  last-reply
EOF
```
</TabItem>
<TabItem value="Dagger Shell">
```shell title="First type 'dagger' for interactive mode."
source=$(container |
  from alpine |
  with-directory /src https://github.com/dagger/dagger |
  directory /src)
contents=$(llm |
  with-directory $source |
  with-prompt "You have a directory with source code." |
  with-prompt "The directory also has some tools available." |
  with-prompt "Use the tools in the directory to read the first paragraph of the README.md file in the directory." |
  with-prompt "Reply with only the selected text." |
  last-reply)
llm |
  with-prompt "Here is some text: $contents. Translate it to French." |
  last-reply
```
</TabItem>
</Tabs>

## Responses

Use the `LLM.lastReply()` API method to obtain the last reply from the LLM:

<Tabs groupId="shell">
<TabItem value="System shell">
```shell
dagger <<EOF
llm |
  with-container \$(container | from alpine | with-exec apk add curl) |
  with-prompt "You have a container with curl installed." |
  with-prompt "Use curl to browse docs.dagger.io and summarize in one paragraph the types of documentation available" |
  last-reply
EOF
```
</TabItem>
<TabItem value="Dagger Shell">
```shell title="First type 'dagger' for interactive mode."
llm |
  with-container $(container | from alpine | with-exec apk add curl) |
  with-prompt "You have a container with curl installed." |
  with-prompt "Use curl to browse docs.dagger.io and summarize in one paragraph the types of documentation available" |
  last-reply
```
</TabItem>
</Tabs>

:::tip
To get the complete message history, use the `LLM.History()` API method.
:::

## Environments

Dagger [modules](../features/modules.mdx) are collections of Dagger Functions. When you give a Dagger module to the `LLM` core type, every Dagger Function is turned into a tool that the LLM can call.

Environments configure a number of inputs and outputs for the LLM. For example, an environment might provide a `Directory`, a `Container`, and a `string` variable. The LLM can use the scalars and the functions of these objects to complete the assigned task.

The documentation for the Objects are provided to the LLM, so make sure to provide helpful documentation in your Dagger Functions. The LLM should be able to figure out how to use the tools on its own. If you provide too much help, it will be redundant with the function descriptions.

Consider the following Dagger Function:

<Tabs groupId="language">
<TabItem value="Go">
```go file=../quickstart/agent/snippets/go/main.go
```
</TabItem>
<TabItem value="Python">
```python file=../quickstart/agent/snippets/python/src/coding_agent/main.py
```
</TabItem>
<TabItem value="TypeScript">
```typescript file=../quickstart/agent/snippets/typescript/src/index.ts
```
</TabItem>
<TabItem value="PHP">
```php file=../quickstart/agent/snippets/php/src/CodingAgent.php
```
</TabItem>
<TabItem value="Java">
```java file=../quickstart/agent/snippets/java/src/main/java/io/dagger/modules/codingagent/CodingAgent.java
```
</TabItem>
</Tabs>

Here, the `ToyWorkspace` is given as an input to the `Env`. It contains a number of Dagger Functions: `Read()`, `Write()`, and `Build()`. When this environment is attached to an `LLM`, the LLM can call any of these Dagger Functions to change the state of the `ToyWorkspace` and complete the assigned task.

In the `Env`, a `ToyWorkspace` called `after` is specified as a desired output of the LLM. This means that the LLM should return the `ToyWorkspace` object after it has completed its task. The resulting `ToyWorkspace` object is then available for further processing or for use in other Dagger Functions.
