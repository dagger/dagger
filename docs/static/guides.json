{"guides":[{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/194031-kubernetes.mdx","frontMatter":{"slug":"/194031/kubernetes","displayed_sidebar":"current","category":"guides","tags":["kubernetes"],"authors":["Gerhard Lazu","Vikram Vaswani"],"date":"2023-11-30"},"content":"# Run Dagger on Kubernetes\n\n## Introduction\n\nThis guide outlines how to run, and connect to, the Dagger Engine on Kubernetes.\n\n## Assumptions\n\nThis guide assumes that you have:\n\n- The [Dagger CLI](../cli/465058-install.mdx) installed locally.\n- [Helm](https://helm.sh) v3.x available locally.\n- A running Kubernetes cluster (tested with Kubernetes v1.28).\n\n## Step 1: Deploy Dagger with Helm\n\nDeploy Dagger on your Kubernetes cluster with Helm:\n\n```shell\nhelm upgrade --install --namespace=dagger --create-namespace \\\n    dagger oci://registry.dagger.io/dagger-helm\n```\n\nWait for the Dagger Engine to become ready:\n\n```shell\nkubectl wait --for condition=Ready --timeout=60s pod \\\n    --selector=name=dagger-dagger-helm-engine --namespace=dagger\n```\n\nYou can find more information on what was deployed using the following command:\n\n```shell\nkubectl describe daemonset/dagger-dagger-helm-engine --namespace=dagger\n```\n\n## Step 2: Connect Dagger CLI to Dagger Engine pod\n\nGet a Dagger Engine pod name:\n\n```shell\nDAGGER_ENGINE_POD_NAME=\"$(kubectl get pod \\\n    --selector=name=dagger-dagger-helm-engine --namespace=dagger \\\n    --output=jsonpath='{.items[0].metadata.name}')\"\nexport DAGGER_ENGINE_POD_NAME\n```\n\nNext, set the `_EXPERIMENTAL_DAGGER_RUNNER_HOST` variable so that the Dagger CLI knows to connect to the Dagger Engine that you deployed as a Kubernetes pod:\n\n```shell\n_EXPERIMENTAL_DAGGER_RUNNER_HOST=\"kube-pod://$DAGGER_ENGINE_POD_NAME?namespace=dagger\"\nexport _EXPERIMENTAL_DAGGER_RUNNER_HOST\n```\n\nFinally, run an operation that shows the kernel info of the Kubernetes node where this Dagger Engine runs:\n\n```shell\ndagger query <<EOF\n{\n    container {\n        from(address:\"alpine\") {\n            withExec(args: [\"uname\", \"-a\"]) { stdout }\n        }\n    }\n}\nEOF\n```\n\nThis is what a successful response should look like:\n\n```shell\n┣─╮\n│ ▽ init\n│ █ [0.64s] connect\n│ ┣ [0.52s] starting engine\n│ ┣ [0.12s] starting session\n│ ┃ OK!\n│ ┻\n█ [2.44s] dagger query\n┣ [0.00s] loading module\n█ [2.44s] query\n┃ {\n┃     \"container\": {\n┃         \"from\": {\n┃             \"withExec\": {\n┃                 \"stdout\": \"Linux buildkitsandbox 6.1.0-12-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.52-1 (2023-09-07) x86_64 Linux\\n\"\n┃             }\n┃         }\n┃     }\n┃ }\n┣─╮\n│ ▽ from alpine\n│ █ [1.26s] resolve image config for docker.io/library/alpine:latest\n│ █ [0.18s] pull docker.io/library/alpine:latest\n│ ┣ [0.03s] resolve docker.io/library/alpine@sha256:34871e7290500828b39e22294660bee86d966bc0017544e848dd9a255cdf59e0\n│ ┣ [0.61s] ███████████████████ sha256:c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c\n│ ┣ [0.18s] extracting sha256:c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c\n│ ┣─╮ pull docker.io/library/alpine:latest\n│ ┻ │\n█◀──╯ [0.24s] exec uname -a\n┃     Linux buildkitsandbox 6.1.0-12-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.52-1 (2023-09-07) x86_64 Linux\n┻\n• Engine: dagger-dagger-helm-engine-bvbtk (version v0.9.3)\n⧗ 3.10s ✔ 12\n```\n\nThe line above starting with `Engine:` confirms the Dagger Engine that the CLI connected to.\n\nTo double-check that the operations are running on the Kubernetes cluster, follow the pod logs:\n\n```shell\nkubectl logs pod/$DAGGER_ENGINE_POD_NAME --namespace=dagger --follow\n```\n\n## Conclusion\n\nThis guide demonstrated the simplest approach to using Dagger on Kubernetes. For more complex scenarios, such as setting up a Continuous Integration (CI) environment with Dagger on Kubernetes, use the following resources:\n\n- Understand [CI architecture patterns on Kubernetes with Dagger](./237420-ci-architecture-kubernetes.mdx)\n- See an example of [running Dagger on Amazon EKS with GitHub Actions Runner and Karpenter](./934191-eks-github-karpenter.mdx)\n- [Dagger Cloud](https://docs.dagger.io/cloud)\n- [Dagger GraphQL API](https://docs.dagger.io/api/975146/concepts)\n- Dagger [Go](https://docs.dagger.io/sdk/go), [Node.js](https://docs.dagger.io/sdk/nodejs) and [Python](https://docs.dagger.io/sdk/python) SDKs\n\n:::info\nIf you need help troubleshooting your Dagger deployment on Kubernetes, let us know in [Discord](https://discord.com/invite/dagger-io) or create a [GitHub issue](https://github.com/dagger/dagger/issues/new/choose).\n:::","contentTitle":"Run Dagger on Kubernetes","excerpt":"Introduction","timestamp":1701302400000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/213240-tekton.mdx","frontMatter":{"slug":"/213240/tekton","displayed_sidebar":"current","category":"guides","tags":["tekton","kubernetes"],"authors":["Kyle Penfound"],"date":"2023-10-26"},"content":"import Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n# Use Dagger with Tekton\n\n## Introduction\n\n[Tekton](https://tekton.dev/) is an open source framework for CI/CD implementation, with built-in support for Kubernetes and other CI/CD tools. This guide explains how to run Dagger pipelines using Tekton.\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a basic understanding of Kubernetes and Tekton.\n- You have a project with a Dagger pipeline in a Git repository. If not, follow the steps in Appendix A to [create and populate a GitHub repository with a simple project and Dagger pipeline](#appendix-a-create-a-github-repository-with-an-example-project).\n\n## Step 1: Install Tekton\n\nThe first step is to install [Tekton](https://tekton.dev/) in the Kubernetes cluster and the [Tekton CLI](https://tekton.dev/docs/cli/).\n\nFollow the [Tekton installation instructions](https://tekton.dev/docs/getting-started/) and the [Tekton CLI installation steps](https://tekton.dev/docs/cli/), adjusting them as needed to your own requirements. Once you've successfully installed Tekton and the Tekton CLI, continue to the next step.\n\n## Step 2: Install the `git-clone` Task\n\nNext, install the `git-clone` Task from Tekton Hub. This Task adds repository cloning capabilities to your Tekton Pipeline, and is used by the Tekton Pipeline defined later in this guide.\n\n```shell\ntkn hub install task git-clone\n```\n\n## Step 3: Create a Tekton Pipeline, Task and PipelineRun\n\nFollow the steps below:\n\n1. Define a new Tekton Pipeline as follows, in a file named `git-pipeline.yaml`.\n\n  ```yaml file=./snippets/tekton/git-pipeline.yaml\n  ```\n\n  This Pipeline references two Tasks:\n    - The `git-clone` Task, to check out the Git repository for the project into a Tekton Workspace;\n    - A custom `dagger` Task, to run the Dagger pipeline for the project (defined below).\n\n1. Define a new Tekton Task as follows, in a file named `dagger-task.yaml`.\n\n  ```yaml file=./snippets/tekton/dagger-task.yaml\n  ```\n\n  This Task installs the dependencies needed to execute the Dagger pipeline for the project (which was checked out in the previous Tekton Pipeline) and then executes the pipeline using the `npm run ci` command.\n\n1. Define a new Tekton PipelineRun as follows, in a file named `git-pipeline-run.yaml`.\n\n  ```yaml file=./snippets/tekton/git-pipeline-run.yaml\n  ```\n\n  This PipelineRun corresponds to the Tekton Pipeline created previously. It executes the Tekton Pipeline with a given set of input parameters: the Git repository URL and an optional Dagger Cloud token. Update the `YOUR_REPOSITORY_URL` placeholder with the correct repository URL and, if you have a Dagger Cloud token, replace the `YOUR_DAGGER_CLOUD_TOKEN` placeholder with the token or a Kubernetes secret holding the token.\n\nA few important points to note:\n\n- In the Task, the Dagger Engine runs as a sidecar and shares a socket with the Task itself. The Task uses `dind` as its runtime in order to have Docker available.\n- Setting the `DAGGER_CLOUD_TOKEN` environment variable in the Task and the `dagger-cloud-token` parameter in the Pipeline and PipelineRun is only necessary if integrating with [Dagger Cloud](https://dagger.cloud/).\n\n## Step 4: Run the Pipeline\n\n1. Apply the configuration:\n\n  ```shell\n  kubectl apply -f dagger-task.yaml\n  kubectl apply -f git-pipeline-yaml\n  ```\n\n1. Run the Tekton Pipeline:\n\n  ```shell\n  kubectl create -f git-pipeline-run.yaml\n  ```\n\n  The output will look something like `pipelinerun.tekton.dev/clone-read-run-mwvkm created`\n  To see the logs from the PipelineRun, obtain the PipelineRun name from the output and run `tkn pipelinerun logs clone-read-run-<id> -f`.\n\nOnce the PipelineRun has successfully completed, run it again. Dagger's caching should result in a significantly faster second execution.\n\n## Conclusion\n\nThis example demonstrated how to integrate Dagger into a Tekton Pipeline using a custom Task.\n\nTo learn more about Dagger, use the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References. For more information on Tekton, refer to the [official documentation](https://tekton.dev/docs/).\n\n## Appendix A: Create a GitHub repository with an example project\n\nThis guide assumes that you have a Git repository with a project and a Dagger pipeline. If you don't, follow the steps below to create a GitHub repository and add a simple project and Dagger pipeline to it.\n\n1. Clone the [Dagger starter application](https://github.com/dagger/hello-dagger):\n\n  ```shell\n  git clone git@github.com:dagger/hello-dagger.git\n  ```\n\n1. Create a new `ci` subdirectory:\n\n  ```shell\n  cd hello-dagger\n  mkdir ci && cd ci\n  ```\n\n1. Add a simple Dagger pipeline as `index.mjs`:\n\n  ```javascript title=\"index.mjs\" file=./snippets/tekton/index.mjs\n  ```\n\n1. Commit the changes:\n\n  ```shell\n  cd ..\n  git add .\n  git commit -m \"Initial commit\"\n  ```\n\n1. Log in to GitHub using the GitHub CLI:\n\n  ```shell\n  gh auth login\n  ```\n\n1. Create a repository in your GitHub account and push the changes to it:\n\n  ```shell\n  gh repo create myapp --push --remote upstream --source . --public\n  ```","contentTitle":"Use Dagger with Tekton","excerpt":"Introduction","timestamp":1698278400000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/757394-use-services.mdx","frontMatter":{"slug":"/757394/use-services","displayed_sidebar":"current","category":"guides","tags":["go","python","nodejs"],"authors":["Alex Suraci"],"date":"2023-10-12"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nimport Embed from '@site/src/components/embed.js'\n\n# Use Services in Dagger\n\n:::warning\nDagger v0.9.0 includes a breaking change for binding service containers. The `Container.withServiceBinding` API now takes a `Service` instead of a `Container`, so you must call `Container.asService` on its argument. See the section on [binding service containers](#bind-service-containers) for examples.\n:::\n\n## Introduction\n\nDagger [v0.4.0](https://github.com/dagger/dagger/releases/tag/v0.4.0) introduced service containers, aka container-to-container networking. This feature enables users to spin up additional long-running services (as containers) and communicate with those services from their Dagger pipelines. Dagger v0.9.0 further improved this implementation, enabling support for container-to-host networking and host-to-container networking.\n\nSome common use cases for services and service containers are:\n\n- Run a test database\n- Run end-to-end integration tests\n- Run sidecar services\n\nThis guide teaches you the basics of using services and service containers in Dagger.\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go, Python, or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/), or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx), or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n\n## Key concepts\n\nDagger's service containers have the following characteristics:\n\n- Each service container has a canonical, content-addressed hostname and an optional set of exposed ports\n- Service containers can bind to other containers as services\n\nService containers come with the following built-in features:\n\n- Service containers are started just-in-time, de-duplicated, and stopped when no longer needed\n- Service containers are health checked prior to running clients\n- Service containers are given an alias for the client container to use as its hostname\n\n## Working with service hostnames and ports\n\nEach service container has a canonical, content-addressed hostname and an optional set of exposed ports.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\nYou can query a service container's canonical hostname by calling the `Service.Hostname()` SDK method.\n\n```go file=./snippets/use-services/use-hostnames/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\nYou can query a service container's canonical hostname by calling the `Service.hostname()` SDK method.\n\n```typescript file=./snippets/use-services/use-hostnames/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\nYou can query a service container's canonical hostname by calling the `Service.hostname()` SDK method.\n\n```python file=./snippets/use-services/use-hostnames/main.py\n```\n\n</TabItem>\n</Tabs>\n\nYou can also define the ports on which the service container will listen. Dagger checks the health of each exposed port prior to running any clients that use the service, so that clients don't have to implement their own polling logic.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\nThis example uses the `WithExposedPort()` method to set ports on which the service container will listen. Note also the `Endpoint()` helper method, which returns an address pointing to a particular port, optionally with a URL scheme. You can either specify a port or let Dagger pick the first exposed port.\n\n```go file=./snippets/use-services/expose-ports/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\nThis example uses the `withExposedPort()` method to set ports on which the service container will listen. Note also the `endpoint()` helper method, which returns an address pointing to a particular port, optionally with a URL scheme. You can either specify a port or let Dagger pick the first exposed port.\n\n```typescript file=./snippets/use-services/expose-ports/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\nThis example uses the `with_exposed_port()` method to set ports on which the service container will listen. Note also the `endpoint()` helper method, which returns an address pointing to a particular port, optionally with a URL scheme. You can either specify a port or let Dagger pick the first exposed port.\n\n```python file=./snippets/use-services/expose-ports/main.py\n```\n\n</TabItem>\n</Tabs>\n\nIn practice, you are more likely to set your own hostname aliases with service bindings, which are covered in the next section.\n\n## Working with services\n\nYou can use services in Dagger in three ways:\n\n- [Bind service containers](#bind-service-containers)\n- [Expose service containers to the host](#expose-service-containers-to-the-host)\n- [Expose host services to client containers](#expose-host-services-to-containers)\n\n:::note\nServices are automatically started when needed and stopped when no longer needed. Dagger cancels each service run after a 10 second grace period to avoid frequent restarts. For more information, read [how service binding works](#reference-how-service-binding-works-for-container-services) or [start/stop services explicitly](#start-and-stop-services) if you need more control.\n:::\n\n### Bind service containers\n\n:::warning\nDagger v0.9.0 includes a breaking change for binding service containers. The examples below have been updated.\n:::\n\nDagger enables users to bind a service running in a container to another (client) container with an alias that the client container can use as a hostname to communicate with the service.\n\nBinding a service to a container or the host creates a dependency in your Dagger pipeline. The service container needs to be running when the client container runs. The bound service container is started automatically whenever its client container runs.\n\nHere's an example of an HTTP service automatically starting in tandem with a client container. The service binding enables the client container to access the HTTP service using the alias `www`.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/bind-service-containers-1/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/bind-service-containers-1/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/bind-service-containers-1/main.py\n```\n\n</TabItem>\n</Tabs>\n\n:::tip\nServices in service containers should be configured to listen on the IP address 0.0.0.0 instead of 127.0.0.1. This is because 127.0.0.1 is only reachable within the container itself, so other services (including the Dagger health check) won't be able to connect to it. Using 0.0.0.0 allows connections to and from any IP address, including the container's private IP address in the Dagger network.\n:::\n\nWhen a service is bound to a container, it also conveys to any outputs of that container, such as files or directories. The service will be started whenever the output is used, so you can also do things like this:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/bind-service-containers-2/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/bind-service-containers-2/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/bind-service-containers-2/main.py\n```\n\n</TabItem>\n</Tabs>\n\n### Expose service containers to the host\n\nStarting with Dagger v0.9.0, you can expose service container ports directly to the host. This enables clients on the host to communicate with services running in Dagger.\n\nOne use case is for testing, where you need to be able to spin up ephemeral databases to run tests against. You might also use this to access a web UI in a browser on your desktop.\n\nHere's an example of how to use Dagger services on the host. In this example, the host makes HTTP requests to an HTTP service running in a container.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/expose-service-containers-to-host/main.go\n```\n\nThe Dagger pipeline calls `Host.Tunnel(service).Start()` to create a new `Service`. By default, Dagger lets the operating system randomly choose which port to use based on the available ports on the host's side. Finally, a call to `Service.Endpoint()` gets the final address with whichever port is bound.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/expose-service-containers-to-host/index.ts\n```\n\nThe Dagger pipeline calls `Host.Tunnel(service).Start()` to create a new `Service`. By default, Dagger lets the operating system randomly choose which port to use based on the available ports on the host's side. Finally, a call to `Service.Endpoint()` gets the final address with whichever port is bound.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/expose-service-containers-to-host/main.py\n```\n\nThe Dagger pipeline calls `host.tunnel(service).start()` to create a new `Service`. By default, Dagger lets the operating system randomly choose which port to use based on the available ports on the host's side. Finally, a call to `Service.endpoint()` gets the final address with whichever port is bound.\n\n</TabItem>\n</Tabs>\n\n### Expose host services to containers\n\nStarting with Dagger v0.9.0, you can bind containers to host services. This enables client containers in Dagger pipelines to communicate with services running on the host.\n\n:::note\nThis implies that a service is already listening on a port on the host, out-of-band of Dagger.\n:::\n\nHere's an example of how a container running in a Dagger pipeline can access a service on the host. In this example, a container in a Dagger pipeline queries a MariaDB database service running on the host. Before running the pipeline, use the following command to start a MariaDB database service on the host:\n\n```shell\ndocker run --rm --detach -p 3306:3306 --name my-mariadb --env MARIADB_ROOT_PASSWORD=secret  mariadb:10.11.2\n```\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/expose-host-services-to-container/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/expose-host-services-to-container/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/expose-host-services-to-container/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThis Dagger pipeline creates a service that proxies traffic through the host to the configured port. It then sets the service binding on the client container to the host.\n\n:::note\nTo connect client containers to Unix sockets on the host instead of TCP, see `Host.unixSocket`.\n:::\n\n## Persist service state\n\nDagger cancels each service run after a 10 second grace period to avoid frequent restarts. To avoid relying on the grace period, use a cache volume to persist a service's data, as in the following example:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/persist-service-state/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/persist-service-state/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/persist-service-state/main.py\n```\n\n</TabItem>\n</Tabs>\n\n:::info\nThis example uses Redis's `SAVE` command to ensure data is synced. By default, Redis flushes data to disk periodically.\n:::\n\n## Start and stop services\n\nServices are designed to be expressed as a Directed Acyclic Graph (DAG) with explicit bindings allowing services to be started lazily, just like every other DAG node. But sometimes, you may need to explicitly manage the lifecycle. Starting with Dagger v0.9.0, you can explicitly start and stop services in your pipelines.\n\nHere's an example which demonstrates explicitly starting a Docker daemon for use in a test suite:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/start-stop-services/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/start-stop-services/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/start-stop-services/main.py\n```\n\n</TabItem>\n</Tabs>\n\n## Example: MariaDB database service for application tests\n\nThe following example demonstrates service containers in action, by creating and binding a MariaDB database service container for use in application unit/integration testing.\n\nThe application used in this example is [Drupal](https://www.drupal.org/), a popular open-source PHP CMS. Drupal includes a large number of unit tests, including tests which require an active database connection. All Drupal 10.x tests are written and executed using the [PHPUnit](https://phpunit.de/) testing framework. Read more about [running PHPUnit tests in Drupal](https://www.drupal.org/docs/automated-testing/phpunit-in-drupal/running-phpunit-tests).\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/use-db-service/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/use-db-service/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/use-db-service/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThis example begins by creating a MariaDB service container and initializing a new MariaDB database. It then creates a Drupal container (client) and installs required dependencies into it. Next, it adds a binding for the MariaDB service (`db`) in the Drupal container and sets a container environment variable (`SIMPLETEST_DB`) with the database DSN. Finally, it runs Drupal's kernel tests (which [require a database connection](https://www.drupal.org/docs/automated-testing/phpunit-in-drupal/running-phpunit-tests#non-unit-tests)) using PHPUnit and prints the test summary to the console.\n\n:::tip\nExplicitly specifying the service container port with `WithExposedPort()` (Go), `withExposedPort()` (Node.js) or `with_exposed_port()` (Python) is particularly important here. Without it, Dagger will start the service container and immediately allow access to service clients. With it, Dagger will wait for the service to be listening first.\n:::\n\n## Reference: How service binding works for container services\n\nIf you're not interested in what's happening in the background, you can skip this section and just trust that services are running when they need to be. If you're interested in the theory, keep reading.\n\nConsider this example:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/test-service-lifecycle-1/main.go\n```\n\nHere's what happens on the last line:\n\n1. The client requests the `ping` container's stdout, which requires the container to run.\n1. Dagger sees that the `ping` container has a service binding, `redisSrv`.\n1. Dagger starts the `redisSrv` container, which recurses into this same process.\n1. Dagger waits for health checks to pass against `redisSrv`.\n1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/test-service-lifecycle-1/index.ts\n```\n\nHere's what happens on the last line:\n\n1. The client requests the `ping` container's stdout, which requires the container to run.\n1. Dagger sees that the `ping` container has a service binding, `redisSrv`.\n1. Dagger starts the `redisSrv` container, which recurses into this same process.\n1. Dagger waits for health checks to pass against `redisSrv`.\n1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/test-service-lifecycle-1/main.py\n```\n\nHere's what happens on the last line:\n\n1. The client requests the `ping` container's stdout, which requires the container to run.\n1. Dagger sees that the `ping` container has a service binding, `redis_srv`.\n1. Dagger starts the `redis_srv` container, which recurses into this same process.\n1. Dagger waits for health checks to pass against `redis_srv`.\n1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.\n\n</TabItem>\n</Tabs>\n\n:::note\nDagger cancels each service run after a 10 second grace period to avoid frequent restarts.\n:::\n\nServices are based on containers, but they run a little differently. Whereas regular containers in Dagger are de-duplicated across the entire Dagger Engine, service containers are only de-duplicated within a Dagger client session. This means that if you run separate Dagger sessions that use the exact same services, they will  each get their own \"instance\" of the service. This process is carefully tuned to preserve caching at each client call-site, while prohibiting \"cross-talk\" from one Dagger session's client to another Dagger session's service.\n\nContent-addressed services are very convenient. You don't have to come up with names and maintain instances of services; just use them by value. You also don't have to manage the state of the service; you can just trust that it will be running when needed and stopped when not.\n\n:::tip\nIf you need multiple instances of a service, just attach something unique to each one, such as an instance ID.\n:::\n\nHere's a more detailed client-server example of running commands against a Redis service:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/test-service-lifecycle-2/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/test-service-lifecycle-2/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/test-service-lifecycle-2/main.py\n```\n\n</TabItem>\n</Tabs>\n\nNote that this example relies on the 10-second grace period, which you should try to avoid. It would be better to chain both commands together, which ensures that the service stays running for both:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-services/test-service-lifecycle-3/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/use-services/test-service-lifecycle-3/index.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-services/test-service-lifecycle-3/main.py\n```\n\n</TabItem>\n</Tabs>\n\n:::note\nDepending on the 10-second grace period is risky because there are many factors which could cause a 10-second delay between calls to Dagger, such as excessive CPU load, high network latency between the client and Dagger, or Dagger operations that require a variable amount of time to process.\n:::\n\n## Conclusion\n\nThis tutorial walked you through the basics of using service containers with Dagger. It explained how container-to-container networking and the service lifecycle is implemented in Dagger. It also provided examples of exposing service containers to the host, exposiing host services to containers and persisting service state using Dagger.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Use Services in Dagger","excerpt":"Dagger v0.9.0 includes a breaking change for binding service containers. The Container.withServiceBinding API now takes a Service instead of a Container, so you must call Container.asService on its argument. See the section on binding service containers for examples.","timestamp":1697068800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/237420-ci-architecture-kubernetes.mdx","frontMatter":{"slug":"/237420/ci-architecture-kubernetes","displayed_sidebar":"current","category":"guides","tags":["kubernetes"],"authors":["Joel Longtine","Gerhard Lazu","Vikram Vaswani"],"date":"2023-09-22"},"content":"# Understand CI Architecture Patterns for Dagger on Kubernetes\n\n## Introduction\n\nThis guide outlines how to set up a Continuous Integration (CI) environment with the Dagger Engine on Kubernetes. It describes and explains a few architecture patterns and components, together with optional optimizations.\n\n## Assumptions\n\nThis guide assumes that you have:\n\n- A good understanding of how Kubernetes works, and of key Kubernetes components and add-ons.\n- A good understanding of how Dagger works. If not, [read the Dagger Quickstart](../quickstart/index.mdx).\n\n## Architecture Patterns\n\n### Base pattern: Persistent nodes\n\nThe base pattern consists of persistent Kubernetes nodes with ephemeral CI runners.\n\nThe minimum required components are:\n\n- *Kubernetes cluster*, consisting of support nodes and runner nodes.\n  - Runner nodes host CI runners and Dagger Engines.\n  - Support nodes host support and management tools, such as certificate management, runner controller & other functions.\n- *Certificates manager*, required by Runner controller for Admission Webhook.\n- *Runner controller*, responsible for managing CI runners in response to CI job requests.\n  - CI runners are the workhorses of a CI/CD system. They execute the jobs that are defined in the CI/CD pipeline.\n- *Dagger Engine* on each runner node, running alongside one or more CI runners.\n  - Responsible for running Dagger pipelines and caching intermediate and final build artifacts.\n\nIn this architecture:\n\n- Kubernetes nodes are persistent.\n- CI runners are ephemeral.\n- Each CI runner has access only to the cache of the local Dagger Engine.\n- The Dagger Engine is deployed as a DaemonSet, to use resources in the most efficient manner and enable reuse of the local Dagger Engine cache to the greatest extent possible.\n\n![Kubernetes base architecture](/img/current_docs/guides/ci-architecture-kubernetes/pattern-base.png)\n\n### Optimization 1: Ephemeral, auto-scaled nodes\n\nThe base architecture pattern described previously can be optimized by the addition of a *node auto-scaler*. This can automatically adjust the size of node groups based on the current workload. If there are a lot of CI jobs running, the auto-scaler can automatically add more runner nodes to the cluster to handle the increased workload. Conversely, if there are few jobs running, it can remove unnecessary runner nodes.\n\nThis optimization reduces the total compute cost since runner nodes are added & removed based on the number of concurrent CI jobs.\n\nIn this architecture:\n\n- Kubernetes nodes provisioned on-demand start with a \"clean\" Dagger Engine containing no cached data.\n- Cached build artifacts from subsequent runs will persist only for the lifetime of the runner node.\n\n![Kubernetes architecture with ephmeral nodes](/img/current_docs/guides/ci-architecture-kubernetes/pattern-ephemeral.png)\n\n### Optimization 2: Shared Cloud Cache\n\nThe previous pattern makes it possible to scale the Dagger deployment, but comes with the following trade-offs:\n\n1. Runner nodes are automatically de-provisioned when they are not needed. During de-provisioning, the Dagger Engines get deleted too. As a result, data and operations cached in previous runs will be deleted and subsequent runs will not benefit from previous runs. To resolve this, the cached data and operations are stored in a *cloud caching service* and made available to new Dagger Engines when they are provisioned.\n2. The deployment will only scale to a certain point, given that each Dagger Engine can only scale vertically to provide better performance. In order to make the system horizontally scalable, a caching service makes the same data and operations available to as many Dagger Engines as needed.\n\nIn this architecture:\n\n- A shared cloud cache stores data from all Dagger Engines running in the cluster.\n- Auto-provisioned nodes start with access to cached data of previous runs.\n\n![Kubernetes architecture with shared cache](/img/current_docs/guides/ci-architecture-kubernetes/pattern-cache.png)\n\n:::tip\nSee a specific [implementation of the above pattern using GitHub Actions, Amazon Elastic Kubernetes Service (EKS), Karpenter and Dagger Cloud](./934191-eks-github-karpenter.mdx).\n:::\n\n## Recommendations\n\nWhen deploying Dagger on a Kubernetes cluster, it's important to understand the design constraints you're operating under, so you can optimize your configuration to suit your workload requirements. Here are two key recommendations.\n\n#### Runner nodes with moderate to large NVMe drives\n\nThe Dagger Engine cache is used to store intermediate build artifacts, which can significantly speed up your CI jobs. However, this cache can grow very large over time. By choosing nodes with large NVMe drives, you ensure that there is plenty of space for this cache. NVMe drives are also much faster than traditional SSDs, which can further improve performance. These types of drives are usually ephemeral to the node and much less expensive relative to EBS-type volumes.\n\n#### Runner nodes appropriately sized for your workloads\n\nAlthough this will obviously vary based on workloads, a minimum of 2 vCPUs and 8GB of RAM is a good place to start. One approach is to set up the GitHub Actions runners with various sizes so that the Dagger Engine can consume resources from the runners on the same node if needed.\n\n## Conclusion\n\nThis guide described a few common architecture patterns to consider when setting up a Continuous Integration (CI) environment using Dagger on Kubernetes.\n\nUse the following resources to learn more about the topics discussed in this guide:\n\n- See an example of [running Dagger on Amazon EKS with GitHub Actions Runner and Karpenter](./934191-eks-github-karpenter.mdx)\n- [Dagger Cloud](https://docs.dagger.io/cloud)\n- [Dagger GraphQL API](https://docs.dagger.io/api/975146/concepts)\n- Dagger [Go](https://docs.dagger.io/sdk/go), [Node.js](https://docs.dagger.io/sdk/nodejs) and [Python](https://docs.dagger.io/sdk/python) SDKs\n\n:::info\nIf you need help troubleshooting your Dagger deployment on Kubernetes, let us know in [Discord](https://discord.com/invite/dagger-io) or create a [GitHub issue](https://github.com/dagger/dagger/issues/new/choose).\n:::","contentTitle":"Understand CI Architecture Patterns for Dagger on Kubernetes","excerpt":"Introduction","timestamp":1695340800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/934191-eks-github-karpenter.mdx","frontMatter":{"slug":"/934191/eks-github-karpenter","displayed_sidebar":"current","category":"guides","tags":["kubernetes","github","eks","dagger-cloud"],"authors":["Joel Longtine","Gerhard Lazu","Vikram Vaswani"],"date":"2023-09-22"},"content":"# Run Dagger on Amazon EKS with GitHub Actions Runner and Karpenter\n\n## Introduction\n\nThis guide outlines how to set up a Continuous Integration (CI) environment with the Dagger Engine on Kubernetes using GitHub Actions, Amazon Elastic Kubernetes Service (EKS), Karpenter and Dagger Cloud.\n\n## Assumptions\n\nThis guide assumes that you have:\n\n- A good understanding of how Kubernetes works, and of key Kubernetes components and add-ons.\n- A good understanding of how Dagger works. If not, [read the Dagger Quickstart](../quickstart/index.mdx).\n- An Amazon EKS Kubernetes cluster with [cert-manager](https://cert-manager.io/), [Karpenter](https://karpenter.sh/), [Helm](https://helm.sh) and [GitHub Actions Runner Controller (ARC)](https://github.com/actions/actions-runner-controller) installed.\n- A GitHub account. If not, [sign up for a free GitHub account](https://github.com/signup).\n- A Dagger Cloud account. If not, [sign up for Dagger Cloud](https://dagger.io/cloud).\n\n:::note\nThese steps below may vary depending on your specific setup and requirements. Always refer to the official documentation of the tools and services listed for the most accurate and up-to-date information.\n:::\n\n### Step 1: Understand the architecture\n\n![Kubernetes implementation](/img/current_docs/guides/eks-github-karpenter/implementation.png)\n\nHere is a brief description of the architecture and components:\n\n- The application source code is hosted in a GitHub repository.\n- The runner nodes are part of an Amazon EKS cluster.\n- GitHub provides the [GitHub Actions Runner Controller (ARC)](https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller), a Kubernetes controller that manages deploying and scaling GitHub Actions runners as Pods in a Kubernetes cluster.\n- A Dagger Engine runs on every Kubernetes node where a GitHub Actions Runner is deployed.\n- Based on GitHub Actions jobs queued, ARC creates on-demand runners.\n- Dagger Engines communicate with the Dagger Cloud to read from & write to the shared cache.\n- Karpenter, a Kubernetes-native node auto-scaler, uses the AWS EKS API to dynamically add or remove runner nodes depending on workload requirements.\n\n### Step 2: Set up services and components\n\nThe next step is to set up the required services and components. Ensure that you have:\n\n- An Amazon EKS Kubernetes cluster with [cert-manager](https://cert-manager.io/), [Karpenter](https://karpenter.sh/) and [GitHub Actions Runner Controller (ARC)](https://github.com/actions/actions-runner-controller) installed.\n- A GitHub account\n- A [Dagger Cloud account](https://dagger.io/cloud), which provides distributed caching, pipeline visibility, and operational insights.\n\n### Step 3: Create a set of taints and tolerations for the GitHub Actions runners\n\n[Taints and tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in Kubernetes are a way to ensure that certain nodes are reserved for specific tasks. By setting up taints on runner nodes, you can prevent other workloads from being scheduled on them. Tolerations are then added to the runners so that they can be scheduled on these tainted nodes. This ensures that the runners have dedicated resources for their tasks.\n\nA sample GitHub Actions runner deployment configuration is shown below.\n\n- Replace the `YOUR_GITHUB_ORGANIZATION` placeholder with your GitHub organization name. If you do not have a GitHub organization, you can use your GitHub username instead.\n- This configuration also uses the `DAGGER_CLOUD_ENVIRONMENT` environment variable to connect this Dagger Engine to Dagger Cloud. Replace `YOUR_DAGGER_CLOUD_TOKEN` with your own Dagger Cloud token.\n\n```yaml file=./snippets/kubernetes/runner_deployment.yml\n```\n\n:::note\nThis configuration uses the `_EXPERIMENTAL_DAGGER_RUNNER_HOST` environment variable to point to the Dagger Engine DaemonSet socket that is mounted into the GitHub Actions runners. This ensures that the runners will use the local Dagger Engine for pipeline execution.\n:::\n\n### Step 4: Deploy the Dagger Engine DaemonSet with Helm v3\n\nA Dagger Engine is required on each of the GitHub Actions runner nodes. A DaemonSet ensures that all matching nodes run an instance of Dagger Engine. To ensure that the Dagger Engines are co-located with the GitHub Actions runners, the Dagger Engine Daemonset should be configured with the same taints and tolerations as the GitHub Actions runners.\n\nUse our Helm chart to create the Dagger Engine DaemonSet on the cluster:\n\n```shell\nhelm upgrade --install --namespace=dagger --create-namespace \\\n    dagger oci://registry.dagger.io/dagger-helm\n```\n\nThis Dagger Engine DaemonSet configuration is designed to:\n\n- best utilize local Non-Volatile Memory Express (NVMe) hard drives of the worker nodes\n- reduce the amount of network latency and bandwidth requirements\n- simplify routing of Dagger SDK and CLI requests\n\n### Step 5: Test the deployment\n\nAt this point, the deployment is configured and ready for use. Test it by triggering the GitHub Actions workflow, by committing a new change to the source code repository. Your CI pipelines will be now connected to your Dagger Engines.\n\nIf you don't already have a GitHub repository, clone the [repository for the Dagger starter application](https://github.com/dagger/hello-dagger) and add the sample GitHub actions workflow shown below to it. Refer to the inline comments for configuration you may wish to change.\n\n```yaml title=\".github/workflows/dagger-on-kubernetes.yaml\" file=./snippets/kubernetes/github_workflow.yml\n```\n\n:::note\nRemember to add the `ci/index.mjs` file containing the Dagger pipeline too (an example is available in the [Dagger quickstart](../quickstart/635927-caching.mdx)). Alternatively, pick your preferred language and adapt the GitHub Actions workflow example above.\n:::\n\n:::tip\nTo validate that your Dagger Engines are working as expected, you can check your Dagger Cloud dashboard, which shows detailed information about your pipeline executions.\n:::\n\n## Conclusion\n\nThis guide described how to set up a Continuous Integration (CI) environment using Dagger on Kubernetes with GitHub Actions, Amazon Elastic Kubernetes Service (EKS), Karpenter and Dagger Cloud.\n\nUse the following resources to learn more about the topics discussed in this guide:\n\n- [CI Architecture Patterns for Dagger on Kubernetes](./237420-ci-architecture-kubernetes.mdx)\n- [Dagger Cloud](https://docs.dagger.io/cloud)\n- [Dagger GraphQL API](https://docs.dagger.io/api/975146/concepts)\n- Dagger [Go](https://docs.dagger.io/sdk/go), [Node.js](https://docs.dagger.io/sdk/nodejs) and [Python](https://docs.dagger.io/sdk/python) SDKs\n\n:::info\nIf you need help troubleshooting your Dagger deployment on Kubernetes, let us know in [Discord](https://discord.com/invite/dagger-io) or create a [GitHub issue](https://github.com/dagger/dagger/issues/new/choose).\n:::","contentTitle":"Run Dagger on Amazon EKS with GitHub Actions Runner and Karpenter","excerpt":"Introduction","timestamp":1695340800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/324301-argo-workflows.mdx","frontMatter":{"slug":"/324301/argo-workflows","displayed_sidebar":"current","category":"guides","tags":["argo","kubernetes"],"authors":["Kyle Penfound"],"date":"2023-08-22"},"content":"import Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n# Use Dagger with Argo Workflows\n\n## Introduction\n\n[Argo Workflows](https://argoproj.github.io/argo-workflows/) is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. This guide explains how to run Dagger pipelines in Argo Workflows.\n\n## Requirements\n\nThis guide assumes that you have a basic understanding of Kubernetes and Argo Workflows, and that your Kubernetes cluster has been configured following the [Run Dagger on Kubernetes](https://docs.dagger.io/194031/kubernetes) guide.\n\n## Step 1: Install Argo Workflows\n\nThe first step is to install [Argo Workflows](https://argoproj.github.io/argo-workflows/) in the Kubernetes cluster.\n\nFollow the [Argo Workflows quickstart](https://github.com/argoproj/argo-workflows/blob/master/docs/quick-start.md) steps, adjusting them as needed to your own requirements. Once you've successfully installed Argo Workflows in your cluster, continue to the next step.\n\n## Step 2: Run a sample workflow\n\nThe sample workflow will clone and run the CI for the [greetings-api](https://github.com/kpenfound/greetings-api) demo project. This project uses the Dagger Go SDK for CI.\n\nCreate a file called `workflow.yaml` with the following content:\n\n```yaml file=./snippets/argo-workflows/workflow.yaml\n```\n\nA few important points to note:\n\n- The workflow uses hardwired artifacts to clone the Git repository and to install the Dagger CLI.\n- `unix:///var/run/dagger/buildkitd.sock` is mounted and specified with the `_EXPERIMENTAL_DAGGER_RUNNER_HOST` environment variable.\n- The Dagger CLI `dagger_v0.8.7_linux_amd64.tar.gz` is downloaded and installed. Confirm the version and architecture are accurate for your cluster and project.\n- The image `golang:1.21.0-bookworm` is used as the runtime for the pipeline because the example project requires Go.\n- Setting the `DAGGER_CLOUD_TOKEN` environment variable is only necessary if integrating with [Dagger Cloud](https://dagger.cloud/).\n\nThe workflow uses a PersistentVolumeClaim for the runtime dependencies of the pipeline, such as the Dagger Go SDK.\n\nCreate the PersistentVolumeClaim configuration in a file called `gomodcache.yaml`:\n\n```yaml file=./snippets/argo-workflows/gomodcache.yaml\n```\n\nApply the configuration:\n\n `kubectl apply -n argo -f ./gomodcache.yaml`\n\nWhen you're satisfied with the workflow configuration, run it with Argo:\n\n`argo submit -n argo --watch ./workflow.yaml`\n\nThe `--watch` argument provides an ongoing status feed of the workflow request in Argo. To see the logs from your workflow, note the pod name and in another terminal run `kubectl logs -f POD_NAME`\n\nOnce the workflow has successfully completed, run it again with `argo submit -n argo --watch ./workflow.yaml`. Dagger's caching should result in a significantly faster second execution.\n\n## Conclusion\n\nThis example demonstrated how to integrate Dagger with Argo Workflows. However, this is a basic example and it's likely that you will want to also integrate Argo Workflows and Argo Events into your CI/CD pipeline. These topics are outside the scope of this guide, but you can find numerous third-party tutorials on these topics, such as this [guide on implementing CI/CD pipeline using Argo Workflows and Argo Events](https://medium.com/atlantbh/implementing-ci-cd-pipeline-using-argo-workflows-and-argo-events-6417dd157566).\n\nTo learn more about Dagger, use the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References. For more information on Argo Workflows, refer to the [official documentation](https://argoproj.github.io/argo-workflows/#documentation).","contentTitle":"Use Dagger with Argo Workflows","excerpt":"Introduction","timestamp":1692662400000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/592101-custom-callbacks.mdx","frontMatter":{"slug":"/592101/custom-callbacks","displayed_sidebar":"current","category":"guides","tags":["nodejs","go","python"],"authors":["Vikram Vaswani"],"date":"2023-08-18"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Use Custom Callbacks in a Dagger Pipeline\n\n## Introduction\n\nAll Dagger SDKs support adding callbacks to the pipeline invocation chain. Using a callback enables greater code reusability and modularity, and also avoids \"breaking the chain\" when constructing a Dagger pipeline.\n\nThis guide explains the basics of creating and using custom callbacks in a Dagger pipeline. You will learn how to:\n\n- Create a custom callback\n- Chain the callback into a Dagger pipeline\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n\n## Example\n\nAll Dagger SDKs support adding a callback via the `With()` API method. The callback must return a function that receives a `Container` from the chain, and returns a `Container` back to it.\n\nAssume that you have the following Dagger pipeline:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/custom-callbacks/mounts-without-callback/main.go\n```\n\nHere, the `AddMounts()` function accepts a container, mounts two directories, and returns it to the `main()` function. Within the `main()` function, the call to `AddMounts()` breaks the Dagger pipeline construction chain.\n\nThis pipeline can be rewritten to use a callback and the `With()` API method, as below:\n\n```go file=./snippets/custom-callbacks/mounts-with-callback/main.go\n```\n\nHere, the `Mounts()` callback function returns a function that receives a `Container` from the chain, and returns a `Container` back to it. It can then be attached to the Dagger pipeline in the normal manner, as an argument to `With()`.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/custom-callbacks/mounts-without-callback/index.mts\n```\n\nHere, the `addMounts()` function accepts a container, mounts two directories, and returns the container. Within the main program, the call to `addMounts()` breaks the Dagger pipeline construction chain.\n\nThis pipeline can be rewritten to use a callback and the `with()` API method, as below:\n\n```javascript file=./snippets/custom-callbacks/mounts-with-callback/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/custom-callbacks/mounts-without-callback/main.py\n```\n\nHere, the `add_mounts()` function accepts a container, mounts two directories, and returns the container. Within the main program, the call to `add_mounts()` breaks the Dagger pipeline construction chain.\n\nThis pipeline can be rewritten to use a callback and the `with()` API method, as below:\n\n```python file=./snippets/custom-callbacks/mounts-with-callback/main.py\n```\n\n</TabItem>\n</Tabs>\n\nHere's another example, this one demonstrating how to add multiple environment variables to a container using a callback:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/custom-callbacks/environment-variables/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/custom-callbacks/environment-variables/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/custom-callbacks/environment-variables/main.py\n```\n\n</TabItem>\n</Tabs>\n\n## Conclusion\n\nThis guide explained how to create and chain custom callback functions in your Dagger pipeline.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Use Custom Callbacks in a Dagger Pipeline","excerpt":"Introduction","timestamp":1692316800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/882813-build-test-publish-java-spring.mdx","frontMatter":{"slug":"/882813/build-test-publish-java-spring","displayed_sidebar":"current","category":"guides","tags":["java","spring"],"authors":["Vikram Vaswani"],"date":"2023-07-05"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Build, Test and Publish a Spring Application with Dagger\n\n## Introduction\n\nDagger SDKs are currently available for Go, Node.js and Python, but you can use them to create CI/CD pipelines for applications written in any programming language. This guide explains how to use Dagger to continuously build, test and publish a Java application using Spring. You will learn how to:\n\n- Create a Dagger pipeline to:\n  - Build your Spring application with all required dependencies\n  - Run unit tests for your Spring application\n  - Publish the final application image to Docker Hub\n- Run the Dagger pipeline on the local host using the Dagger CLI\n- Run the Dagger pipeline on every repository commit using GitHub Actions\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a basic understanding of how Dagger works. If not, [read the Dagger Quickstart](../quickstart/index.mdx).\n- You have a basic understanding of GitHub Actions. If not, [learn about GitHub Actions](https://docs.github.com/en/actions).\n- You have Docker installed and running in your development environment. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have a Docker Hub account. If not, [register for a free Docker Hub account](https://hub.docker.com/signup).\n- You have a GitHub account. If not, [register for a free GitHub account](https://github.com/signup).\n- You have a GitHub repository containing a Spring application. This repository should also be cloned locally in your development environment. If not, follow the steps in Appendix A to [create and populate a local and GitHub repository with a Spring sample application](#appendix-a-create-a-github-repository-with-an-example-spring-application).\n\n## Step 1: Create the Dagger pipeline\n\nThe first step is to create a Dagger pipeline to build and test a container image of the application, and publish it to Docker Hub\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. In the application directory, install the Dagger SDK:\n\n  ```shell\n  go mod init main\n  go get dagger.io/dagger@latest\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.go` and add the following code to it.\n\n  ```go file=./snippets/build-test-publish-java-spring/main.go\n  ```\n\n  This Dagger pipeline performs a number of different operations:\n    - It imports the Dagger SDK and checks for Docker Hub registry credentials in the host environment. It also creates a Dagger client with `dagger.Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `SetSecret()` method to set the Docker Hub registry password as a secret for the Dagger pipeline and configures a Maven cache volume with the `CacheVolume()` method. This cache volume is used to persist the state of the Maven cache between runs, thereby eliminating time spent on re-downloading Maven packages.\n    - It uses the client's `Host().Directory()` method to obtain a reference to the source code directory on the host.\n    - It uses the client's `Container().From()` method to initialize three new containers, each of which is returned as a `Container` object:\n        - A MariaDB database service container from the `mariadb:10.11.2` image, for application unit tests;\n        - A Maven container with all required tools and dependencies from the `maven:3.9-eclipse-temurin-17` image, to build and package the application JAR file;\n        - An OpenJDK Eclipse Temurin container from the `eclipse-temurin:17-alpine` image, to create an optimized deployment package.\n    - For the MariaDB database container:\n        - It chains multiple `WithEnvVariable()` methods to configure the database service, and uses the `WithExposedPort()` and `AsService()` methods to ensure that the service is available to clients.\n    -  For the Maven container:\n        - It uses the `WithMountedDirectory()` and `WithMountedCache()` methods to mount the host directory and the cache volume into the Maven container at the `/src` and `/root/.m2` mount points, and the `WithWorkdir()` method to set the working directory in the container.\n        - It adds a service binding for the database service to the Maven container using the `WithServiceBinding()` method and sets the JDBC URL for the application test suite as an environment using the `With_EnvVariable()` method.\n        - Finally, it uses the `WithExec()` method to execute the `mvn -Dspring.profiles.active=mysql clean package` command, which builds, tests and creates a JAR package of the application.\n    -  For the Eclipse Temurin container:\n        - Once the JAR package is ready, it copies only the build artifact directory to the Eclipse Temurin container using the `WithDirectory()` method, and sets the container entrypoint to start the Spring application using the `WithEntrypoint()` method.\n    - It uses the `WithRegistryAuth()` method to set the registry credentials (including the password set as a secret previously) and then invokes the `Publish()` method to publish the Eclipse Temurin container image to Docker Hub. It also prints the SHA identifier of the published image.\n\n1. Run the following command to update `go.sum`:\n\n  ```shell\n  go mod tidy\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. Begin by installing the Dagger SDK as a development dependency:\n\n    ```shell\n    npm install @dagger.io/dagger@latest --save-dev\n    ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `index.mjs` and add the following code to it.\n\n    ```javascript file=./snippets/build-test-publish-java-spring/index.mjs\n    ```\n\n  This Dagger pipeline performs a number of different operations:\n    - It imports the Dagger SDK and checks for Docker Hub registry credentials in the host environment. It also creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `setSecret()` method to set the Docker Hub registry password as a secret for the Dagger pipeline and configures a Maven cache volume with the `cacheVolume()` method. This cache volume is used to persist the state of the Maven cache between runs, thereby eliminating time spent on re-downloading Maven packages.\n    - It uses the client's `host().directory()` method to obtain a reference to the source code directory on the host.\n    - It uses the client's `container().from()` method to initialize three new containers, each of which is returned as a `Container` object:\n        - A MariaDB database service container from the `mariadb:10.11.2` image, for application unit tests;\n        - A Maven container with all required tools and dependencies from the `maven:3.9-eclipse-temurin-17` image, to build and package the application JAR file;\n        - An OpenJDK Eclipse Temurin container from the `eclipse-temurin:17-alpine` image, to create an optimized deployment package.\n    - For the MariaDB database container:\n        - It chains multiple `withEnvVariable()` methods to configure the database service, and uses the `withExposedPort()` and `service()` methods to ensure that the service is available to clients.\n    - For the Maven container:\n        - It uses the `withMountedDirectory()` and `withMountedCache()` methods to mount the host directory and the cache volume into the Maven container at the `/src` and `/root/.m2` mount points, and the `withWorkdir()` method to set the working directory in the container.\n        - It adds a service binding for the database service to the Maven container using the `withServiceBinding()` method and sets the JDBC URL for the application test suite as an environment using the `withEnvVariable()` method.\n        - Finally, it uses the `withExec()` method to execute the `mvn -Dspring.profiles.active=mysql clean package` command, which builds, tests and creates a JAR package of the application.\n    -  For the Eclipse Temurin container:\n        - Once the JAR package is ready, it copies only the build artifact directory to the Eclipse Temurin container using the `withDirectory()` method, and sets the container entrypoint to start the Spring application using the `withEntrypoint()` method.\n    - It uses the `withRegistryAuth()` method to set the registry credentials (including the password set as a secret previously) and then invokes the `publish()` method to publish the Eclipse Temurin container image to Docker Hub. It also prints the SHA identifier of the published image.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. Begin by creating a virtual environment and installing the Dagger SDK:\n\n  ```shell\n  pip install dagger-io\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.py` and add the following code to it.\n\n  ```python file=./snippets/build-test-publish-java-spring/main.py\n  ```\n\n  This Dagger pipeline performs a number of different operations:\n    - It imports the Dagger SDK and checks for Docker Hub registry credentials in the host environment. It also creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `set_secret()` method to set the Docker Hub registry password as a secret for the Dagger pipeline and configures a Maven cache volume with the `cache_volume()` method. This cache volume is used to persist the state of the Maven cache between runs, thereby eliminating time spent on re-downloading Maven packages.\n    - It uses the client's `host().directory()` method to obtain a reference to the source code directory on the host.\n    - It uses the client's `container().from_()` method to initialize three new containers, each of which is returned as a `Container` object:\n        - A MariaDB database service container from the `mariadb:10.11.2` image, for application unit tests;\n        - A Maven container with all required tools and dependencies from the `maven:3.9-eclipse-temurin-17` image, to build and package the application JAR file;\n        - An OpenJDK Eclipse Temurin container from the `eclipse-temurin:17-alpine` image, to create an optimized deployment package.\n    - For the MariaDB database container:\n        - It chains multiple `with_env_variable()` methods to configure the database service, and uses the `with_exposed_port()` and `service()` methods to ensure that the service is available to clients.\n    - For the Maven container:\n        - It uses the `with_mounted_directory()` and `with_mounted_cache()` methods to mount the host directory and the cache volume into the Maven container at the `/src` and `/root/.m2` mount points, and the `with_workdir()` method to set the working directory in the container.\n        - It adds a service binding for the database service to the Maven container using the `with_service_binding()` method and sets the JDBC URL for the application test suite as an environment using the `with_env_variable()` method.\n        - Finally, it uses the `with_exec()` method to execute the `mvn -Dspring.profiles.active=mysql clean package` command, which builds, tests and creates a JAR package of the application.\n    -  For the Eclipse Temurin container:\n        - Once the JAR package is ready, it copies only the build artifact directory to the Eclipse Temurin container using the `with_directory()` method, and sets the container entrypoint to start the Spring application using the `with_entrypoint()` method.\n    - It uses the `with_registry_auth()` method to set the registry credentials (including the password set as a secret previously) and then invokes the `publish()` method to publish the Eclipse Temurin container image to Docker Hub. It also prints the SHA identifier of the published image.\n\n</TabItem>\n</Tabs>\n\n## Step 2: Test the Dagger pipeline on the local host\n\nConfigure the registry credentials using environment variables on the local host. Replace the `USERNAME` and `PASSWORD` placeholders with your Docker Hub credentials.\n\n```shell\nexport DOCKERHUB_USERNAME=USERNAME\nexport DOCKERHUB_PASSWORD=PASSWORD\n```\n\nOnce credentials are configured, test the Dagger pipeline by running the command below:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\ndagger run go run ci/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```shell\ndagger run node ci/index.mjs\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\ndagger run python ci/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThe `dagger run` command executes the script in a Dagger session and displays live progress. At the end of the process, the built container is published on Docker Hub and a message similar to the one below appears in the console output:\n\n```shell\nImage published at: docker.io/.../myapp@sha256:...\n```\n\n## Step 3: Create a GitHub Actions workflow\n\nDagger executes your pipelines entirely as standard OCI containers. This means that the same pipeline will run the same, whether on on your local machine or a remote server.\n\nThis also means that it's very easy to move your Dagger pipeline from your local host to GitHub Actions - all that's needed is to commit and push the pipeline script from your local clone to your GitHub repository, and then define a GitHub Actions workflow to run it on every commit.\n\n1. Commit and push the pipeline script and related changes to the application's GitHub repository:\n\n  ```shell\n  git add .\n  git commit -a -m \"Added pipeline\"\n  git push\n  ```\n\n1. In the GitHub repository, create a new workflow file at `.github/workflows/main.yml` with the following content:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```yaml file=./snippets/build-test-publish-java-spring/github-go.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```yaml file=./snippets/build-test-publish-java-spring/github-nodejs.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```yaml file=./snippets/build-test-publish-java-spring/github-python.yml\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This workflow runs on every commit to the repository `main` branch. It consists of a single job with six steps, as below:\n    1. The first step uses the [Checkout action](https://github.com/marketplace/actions/checkout) to check out the latest source code from the `main` branch to the GitHub runner.\n    1. The second step uses the [Docker Login action](https://github.com/marketplace/actions/docker-login) to authenticate to Docker Hub from the GitHub runner. This is necessary because [Docker rate-limits unauthenticated registry pulls](https://docs.docker.com/docker-hub/download-rate-limit/).\n    1. The third step downloads and installs the required programming language on the GitHub runner.\n    1. The fourth and fifth steps download and install the Dagger SDK and the Dagger CLI on the GitHub runner.\n    1. The final step executes the Dagger pipeline.\n\nThe Docker Login action and the Dagger pipeline both expect to find Docker Hub credentials in the `DOCKERHUB_USERNAME` and `DOCKERHUB_PASSWORD` variables. Create these variables as GitHub secrets as follows:\n\n1. Navigate to the `Settings` -> `Secrets and variables` -> `Actions` page of the GitHub repository.\n1. Click `New repository secret` to create a new secret.\n1. Configure the secret with the following inputs:\n    - Name: `DOCKERHUB_USERNAME`\n    - Secret: Your Docker Hub username\n1. Click `Add secret` to save the secret.\n1. Repeat the process for the `DOCKERHUB_PASSWORD` variable.\n\n![Create GitHub secret](/img/current_docs/guides/build-test-publish-java-spring/create-github-secret.png)\n\n## Step 4: Test the Dagger pipeline on GitHub\n\nTest the Dagger pipeline by committing a change to the GitHub repository.\n\nIf you are using the Spring Petclinic example application described in [Appendix A](#appendix-a-create-a-github-repository-with-an-example-spring-application), the following commands modify and commit a simple change to the application's welcome page:\n\n```shell\ngit pull\nsed -i -e \"s/Welcome/Welcome from Dagger/g\" src/main/resources/messages/messages.properties\ngit add src/main/resources/messages/messages.properties\ngit commit -a -m \"Update welcome message\"\ngit push\n```\n\nThe commit triggers the GitHub Actions workflow defined in Step 3. The workflow runs the various steps of the job, including the pipeline script.\n\nAt the end of the process, a new version of the built container image is published to Docker Hub. A message similar to the one below appears in the GitHub Actions log:\n\n```shell\nImage published at: docker.io/.../myapp@sha256:...\n```\n\nTest the container, replacing `IMAGE-ADDRESS` with the image address returned by the pipeline.\n\n```shell\ndocker run --rm --detach --net=host --name mariadb -e MYSQL_USER=user -e MYSQL_PASSWORD=password -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=db mariadb:10.11.2\ndocker run --rm --net=host -e MYSQL_URL=jdbc:mysql://user:password@localhost/db IMAGE-ADDRESS\n```\n\nBrowse to host port 8080. If you are using the Spring Petclinic example application described in [Appendix A](#appendix-a-create-a-github-repository-with-an-example-spring-application), you see the page shown below:\n\n![Application welcome page](/img/current_docs/guides/build-test-publish-java-spring/test-container.png)\n\n## Conclusion\n\nDagger SDKs are currently available for Go, Node.js and Python, but you can use Dagger to create CI/CD pipelines for applications written in any programming language. This tutorial demonstrated by creating a Dagger pipeline to build, test and publish a Spring application. A similar approach can be followed for any application, regardless of which programming language it's written in.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.\n\n## Appendix A: Create a GitHub repository with an example Spring application\n\nThis tutorial assumes that you have a GitHub repository with a Spring application. If not, follow the steps below to create a GitHub repository and commit an example Express application to it.\n\n:::info\nThis section assumes that you have the GitHub CLI. If not, [install the GitHub CLI](https://github.com/cli/cli#installation) before proceeding.\n:::\n\n1. Log in to GitHub using the GitHub CLI:\n\n  ```shell\n  gh auth login\n  ```\n\n1. Create a directory for the Spring application:\n\n  ```shell\n  mkdir myapp\n  cd myapp\n  ```\n\n1. Clone the [Spring Petclinic sample application](https://github.com/spring-projects/spring-petclinic):\n\n  ```shell\n  git clone git@github.com:spring-projects/spring-petclinic.git .\n  ```\n\n1. Update the `.gitignore` file:\n\n  ```shell\n  echo node_modules >> .gitignore\n  echo package*.json >> .gitignore\n  echo .venv >> .gitignore\n  git add .\n  git commit -m \"Updated .gitignore\"\n  ```\n\n1. Remove existing GitHub Action workflows:\n\n  ```shell\n  rm -rf .github/workflows/*\n  git add .\n  git commit -m \"Removed workflows\"\n  ```\n\n1. Create a private repository in your GitHub account and push the code to it:\n\n  ```shell\n  gh repo create myapp --push --source . --private --remote github\n  ```","contentTitle":"Build, Test and Publish a Spring Application with Dagger","excerpt":"Introduction","timestamp":1688515200000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/573829-aws-lambda.mdx","frontMatter":{"slug":"/183109/aws-lambda","displayed_sidebar":"current","category":"guides","tags":["nodejs","go","python","aws-lambda","aws"],"authors":["Vikram Vaswani"],"date":"2023-06-27"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Deploy AWS Lambda Functions with Dagger\n\n## Introduction\n\nThis tutorial teaches you how to create a local Dagger pipeline to update and deploy an existing AWS Lambda function using a ZIP archive.\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a basic understanding of the JavaScript programming language.\n- You have a basic understanding of the AWS Lambda service. If not, learn about [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html).\n- You have a Go, Node.js or Python development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have an AWS account with appropriate privileges to create and manage AWS Lambda resources. If not, [register for an AWS account](https://aws.amazon.com/).\n- You have an existing AWS Lambda function with a publicly-accessible URL in Go, Node.js or Python, deployed as a ZIP archive. If not, follow the steps in Appendix A to [create an example AWS Lambda function](#appendix-a-create-an-example-aws-lambda-function).\n\n## Step 1: Create a Dagger pipeline\n\nThe first step is to create a Dagger pipeline to build a ZIP archive of the function and deploy it to AWS Lambda.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. In the function directory, install the Dagger SDK:\n\n  ```shell\n  go get dagger.io/dagger\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.go` and add the following code to it.\n\n  ```go file=./snippets/aws-lambda/main.go\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger SDK.\n    - It checks for AWS credentials and configuration in the host environment.\n    - It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `SetSecret()` method to set the AWS credentials as secrets for the Dagger pipeline.\n    - It uses the client's `Host().Directory()` method to obtain a reference to the current directory on the host, excluding the `ci` directory. This reference is stored in the `source` variable.\n    - It uses the client's `Container().From()` method to initialize a new container image from a base  `node:18-alpine` image. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `WithDirectory()` method to return the container image with the host directory written at the `/src` path, and the `WithWorkdir()` method to set the working directory in the container image.\n    - It chains together a series of `WithExec()` method calls to install dependencies and build a ZIP deployment archive containing the function and all its dependencies.\n    - It uses the client's `Container().From()` method to initialize a new `aws-cli` AWS CLI container image.\n    - It uses the `Container` object's `WithSecretVariable()` and `WithEnvVariable()` methods to inject the AWS credentials (as secrets) and configuration into the container environment, so that they can be used by the AWS CLI.\n    - It copies the ZIP archive containing the new AWS Lambda function code from the previous `node:18-alpine` container image into the `aws-cli` container image.\n    - It uses `WithExec()` method calls to execute AWS CLI commands in the container image to upload and deploy the ZIP archive and get the function's public URL. If these operations complete successfully, it prints a success message with the URL to the console.\n\n1. Run the following command to update `go.sum`:\n\n  ```shell\n  go mod tidy\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. In the function directory, install the Dagger SDK:\n\n  ```shell\n  npm install @dagger.io/dagger@latest --save-dev\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `index.mjs` and add the following code to it.\n\n  ```javascript file=./snippets/aws-lambda/index.mjs\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger SDK.\n    - It checks for AWS credentials and configuration in the host environment.\n    - It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `setSecret()` method to set the AWS credentials as secrets for the Dagger pipeline.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from()` method to initialize a new container image from a base  `node:18-alpine` image. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `withDirectory()` method to return the container image with the host directory written at the `/src` path, and the `withWorkdir()` method to set the working directory in the container image.\n    - It chains together a series of `withExec()` method calls to install dependencies and build a ZIP deployment archive containing the function and all its dependencies.\n    - It uses the client's `container().from()` method to initialize a new `aws-cli` AWS CLI container image.\n    - It uses the `Container` object's `withSecretVariable()` and `withEnvVariable()` methods to inject the AWS credentials (as secrets) and configuration into the container environment, so that they can be used by the AWS CLI.\n    - It copies the ZIP archive containing the new AWS Lambda function code from the previous `node:18-alpine` container image into the `aws-cli` container image.\n    - It uses `withExec()` method calls to execute AWS CLI commands in the container image to upload and deploy the ZIP archive and get the function's public URL. If these operations complete successfully, it prints a success message with the URL to the console.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. In the function directory, create a virtual environment and install the Dagger SDK:\n\n  ```shell\n  pip install dagger-io\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.py` and add the following code to it.\n\n  ```python file=./snippets/aws-lambda/main.py\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger SDK.\n    - It checks for AWS credentials and configuration in the host environment.\n    - It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `set_secret()` method to set the AWS credentials as secrets for the Dagger pipeline.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `packages`, `.venv` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from_()` method to initialize a new container image from a base  `node:18-alpine` image. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `with_directory()` method to mount the host directory into the container image at the `/src` mount point, and the `with_workdir()` method to set the working directory in the container image.\n    - It chains together a series of `with_exec()` method calls to install dependencies and build a ZIP deployment archive containing the function and all its dependencies.\n    - It uses the client's `container().from_()` method to initialize a new `aws-cli` AWS CLI container image.\n    - It uses the `Container` object's `with_secret_variable()` and `with_env_variable()` methods to inject the AWS credentials (as secrets) and configuration into the container environment, so that they can be used by the AWS CLI.\n    - It copies the ZIP archive containing the new AWS Lambda function code from the previous `node:18-alpine` container image into the `aws-cli` container image.\n    - It uses `with_exec()` method calls to execute AWS CLI commands in the container image to upload and deploy the ZIP archive and get the function's public URL. If these operations complete successfully, it prints a success message with the URL to the console.\n\n</TabItem>\n</Tabs>\n\n:::tip\nMost `Container` object methods return a revised `Container` object representing the new state of the container. This makes it easy to chain methods together. Dagger evaluates pipelines \"lazily\", so the chained operations are only executed when required - in this case, when the container is published. Learn more about [lazy evaluation in Dagger](../api/975146-concepts.mdx#lazy-evaluation).\n:::\n\n## Step 2: Test the Dagger pipeline\n\nConfigure the credentials and default region for the AWS CLI as environment variables on the local host by executing the commands below. Replace the `KEY-ID` and `SECRET` placeholders with the AWS access key and secret respectively, and the `REGION` placeholder with the default AWS region.\n\n```shell\nexport AWS_ACCESS_KEY_ID=KEY-ID\nexport AWS_SECRET_ACCESS_KEY=SECRET\nexport AWS_DEFAULT_REGION=REGION\n```\n\nOnce the AWS CLI environment variables are set, you're ready to test the Dagger pipeline. Do so by making a change to the function and then executing the pipeline to update and deploy the revised function on AWS Lambda.\n\nIf you are using the example application function in [Appendix A](#appendix-a-create-an-example-aws-lambda-function), the following command modifies the function code to display a list of commits (instead of issues) from the Dagger GitHub repository:\n\n```shell\nsed -i -e 's|/dagger/issues|/dagger/commits|g' lambda.py\n```\n\nAfter modifying the function code, execute the Dagger pipeline:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\ndagger run go run ci/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```shell\ndagger run node ci/index.mjs\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\ndagger run python ci/main.py\n```\n\n</TabItem>\n</Tabs>\n\nDagger performs the operations defined in the pipeline script, logging each operation to the console. At the end of the process, the ZIP archive containing the revised function code is deployed to AWS Lambda and a message similar to the one below appears in the console output:\n\n```shell\nFunction updated at: https://...\n```\n\nBrowse to the public URL endpoint displayed in the output to verify the output of the revised AWS Lambda function.\n\n## Conclusion\n\nThis tutorial walked you through the process of creating a local Dagger pipeline to update and deploy a function on AWS Lambda. It used the Dagger SDKs and explained key concepts, objects and methods available in the SDKs to construct a Dagger pipeline.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.\n\n## Appendix A: Create an example AWS Lambda function\n\nThis tutorial assumes that you have an AWS Lambda function written in Go, Node.js or Python and configured with a publicly-accessible URL. If not, follow the steps below to create an example function.\n\n:::info\nThis section assumes that you have the AWS CLI and a GitHub personal access token. If not, [install the AWS CLI](https://aws.amazon.com/cli/), learn how to [configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) and learn how to [obtain a GitHub personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).\n:::\n\n1. Create a service role for AWS Lambda executions:\n\n  ```shell\n  aws iam create-role --role-name my-lambda-role --assume-role-policy-document '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}'\n  aws iam attach-role-policy --role-name my-lambda-role --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n  ```\n\n  Note the role ARN (the `Role.Arn` field) in the output of the first command, as you will need it in subsequent steps.\n\n1. Create a directory named `myfunction` for the function code.\n\n    ```shell\n    mkdir myfunction\n    cd myfunction\n    ```\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  Within that directory, run the following commands to create a new Go module and add dependencies:\n\n  ```shell\n  go mod init main\n  go get github.com/aws/aws-lambda-go/lambda\n  ```\n\n  Within the same directory, create a file named `lambda.go` and fill it with the following code:\n\n  ```go file=./snippets/aws-lambda/lambda.go\n  ```\n\n  Build the function:\n\n  ```shell\n  GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o lambda lambda.go\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  Within that directory, run the following commands to initialize a new Node.js project and add dependencies:\n\n  ```shell\n  npm init -y\n  npm install node-fetch\n  ```\n\n  Within the same directory, create a file named `lambda.js` and fill it with the following code:\n\n  ```javascript file=./snippets/aws-lambda/lambda.js\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  Within that directory, run the following commands to install project dependencies and create a requirements file:\n\n  ```shell\n  pip install --target ./packages requests\n  pip freeze --path ./packages > requirements.txt\n  ```\n\n  Within the same directory, create a file named `lambda.py` and fill it with the following code:\n\n  ```python file=./snippets/aws-lambda/lambda.py\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This simple function performs an HTTP request to the GitHub API to return a list of issues from the Dagger GitHub repository. It expects to find a GitHub personal access token in the function environment and it uses this token for request authentication.\n\n1. Deploy the function to AWS Lambda. Replace the `ROLE-ARN` placeholder with the service role ARN obtained previously and the `TOKEN` placeholder with your GitHub API token.\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```shell\n  zip function.zip lambda\n  aws lambda create-function --function-name myFunction --zip-file fileb://function.zip --runtime go1.x --handler lambda --timeout 10 --role ROLE-ARN\n  aws lambda update-function-configuration --function-name myFunction --environment Variables={GITHUB_API_TOKEN=TOKEN}\n  aws lambda add-permission --function-name myFunction --statement-id FunctionURLAllowPublicAccess --action lambda:InvokeFunctionUrl --principal \"*\" --function-url-auth-type NONE\n  aws lambda create-function-url-config --function-name myFunction --auth-type NONE\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```shell\n  zip -p -r function.zip .\n  aws lambda create-function --function-name myFunction --zip-file fileb://function.zip --runtime nodejs18.x --handler lambda.handler --timeout 10 --role ROLE-ARN\n  aws lambda update-function-configuration --function-name myFunction --environment Variables={GITHUB_API_TOKEN=TOKEN}\n  aws lambda add-permission --function-name myFunction --statement-id FunctionURLAllowPublicAccess --action lambda:InvokeFunctionUrl --principal \"*\" --function-url-auth-type NONE\n  aws lambda create-function-url-config --function-name myFunction --auth-type NONE\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```shell\n  cd packages\n  zip -r ../function.zip .\n  cd ..\n  zip function.zip lambda.py\n  aws lambda create-function --function-name myFunction --zip-file fileb:///tmp/function.zip --runtime python3.10 --handler lambda.handler --timeout 10 --role ROLE-ARN\n  aws lambda update-function-configuration --function-name myFunction --environment Variables={GITHUB_API_TOKEN=TOKEN}\n  aws lambda add-permission --function-name myFunction --statement-id FunctionURLAllowPublicAccess --action lambda:InvokeFunctionUrl --principal \"*\" --function-url-auth-type NONE\n  aws lambda create-function-url-config --function-name myFunction --auth-type NONE\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This sequence of commands creates a ZIP deployment archive, deploys it as a new AWS Lambda function named `myFunction`, and creates a publicly-accessible URL endpoint. The public URL endpoint is listed in the output of the last command.\n\n1. Browse to the public URL endpoint to test the AWS Lambda function. Confirm that it displays a JSON-encoded list of issues from the Dagger GitHub repository.","contentTitle":"Deploy AWS Lambda Functions with Dagger","excerpt":"Introduction","timestamp":1687824000000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/183109-aws-codebuild-codepipeline.mdx","frontMatter":{"slug":"/183109/aws-codebuild-codepipeline","displayed_sidebar":"current","category":"guides","tags":["nodejs","go","python","aws-codepipelines","aws-codebuild","aws"],"authors":["Vikram Vaswani"],"date":"2023-06-13"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Use Dagger with AWS CodeBuild and AWS CodePipeline\n\n## Introduction\n\nThis tutorial teaches you how to use Dagger to continuously build and publish a Node.js application with AWS CodePipeline. You will learn how to:\n\n- Create an AWS CodeBuild project and connect it to an AWS CodeCommit repository\n- Create a Dagger pipeline using a Dagger SDK\n- Integrate the Dagger pipeline with AWS CodePipeline to automatically build and publish the application on every repository commit\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a basic understanding of the JavaScript programming language.\n- You have a basic understanding of the AWS CodeCommit, AWS CodeBuild and AWS CodePipeline service. If not, learn about [AWS CodeCommit](https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html), [AWS CodeBuild](https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html) and [AWS CodePipeline](https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html).\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have an account with a container registry, such as Docker Hub, and privileges to push images to it. If not, [register for a free Docker Hub account](https://hub.docker.com/signup).\n- You have an AWS account with appropriate privileges to create and manage AWS CodeBuild and AWS CodePipeline resources. If not, [register for an AWS account](https://aws.amazon.com/).\n- You have an AWS CodeCommit repository containing a Node.js Web application. This repository should also be cloned locally in your development environment. If not, follow the steps in Appendix A to [create and populate a local and AWS CodeCommit repository with an example Express application](#appendix-a-create-an-aws-codecommit-repository-with-an-example-express-application).\n\n:::tip\nThis guide uses AWS CodeCommit as the source provider, but AWS CodeBuild also supports GitHub, GitHub Enterprise, BitBucket and Amazon S3 as source providers.\n:::\n\n## Step 1: Create an AWS CodeBuild project\n\nThe first step is to create an AWS CodeBuild project, as described below.\n\n1. Log in to the [AWS console](https://console.aws.amazon.com).\n1. Navigate to the \"CodeBuild\" section.\n1. Navigate to the \"Build projects\" page.\n1. Click \"Create build project\".\n1. On the \"Create build project\" page, input the following details, adjusting them as required for your project:\n    - In the \"Project configuration\" section:\n        - Project name: `myapp-codebuild-project`\n    - In the \"Source\" section:\n        - Source: `AWS CodeCommit`\n        - Reference type: `Branch`\n        - Branch: `main`\n    - In the \"Environment\" section:\n        - Environment image: `Managed image`\n        - Operating system: `Amazon Linux 2`\n        - Runtime(s): `Standard`\n        - Image: `aws/codebuild/amazonlinux2-x86_64-standard:5.0` (or latest available for your architecture)\n        - Image version: `Always use the latest image for this runtime version`\n        - Environment type: `Linux`\n        - Privileged: Enabled\n        - Service role: `New service role`\n        - Environment variables:\n          - `REGISTRY_ADDRESS`: Your registry address (`docker.io` for Docker Hub)\n          - `REGISTRY_USERNAME`: Your registry username\n          - `REGISTRY_PASSWORD`: Your registry password\n    - In the \"Buildspec\" section:\n        - Build specifications: `Use a buildspec file`\n    - In the \"Artifacts\" section:\n        - Type: `No artifacts`\n    - In the \"Logs\" section:\n        - CloudWatch logs: `Enabled`\n1. Click \"Create build project\".\n\nAWS CodeBuild creates a new build project.\n\nThe following images visually illustrate the AWS CodeBuild project configuration:\n\n![Create CodeBuild project - project](/img/current_docs/guides/aws-codebuild-codepipeline/codebuild-project.png)\n\n![Create CodeBuild project - source](/img/current_docs/guides/aws-codebuild-codepipeline/codebuild-source.png)\n\n![Create CodeBuild project - image](/img/current_docs/guides/aws-codebuild-codepipeline/codebuild-image.png)\n\n![Create CodeBuild project - environment](/img/current_docs/guides/aws-codebuild-codepipeline/codebuild-env.png)\n\n![Create CodeBuild project - buildspec](/img/current_docs/guides/aws-codebuild-codepipeline/codebuild-spec.png)\n\n## Step 2: Create the Dagger pipeline\n\nThe next step is to create a Dagger pipeline to build a container image of the application and publish it to the registry.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. In the application directory, install the Dagger SDK:\n\n  ```shell\n  go mod init main\n  go get dagger.io/dagger@latest\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.go` and add the following code to it.\n\n  ```go file=./snippets/aws-codebuild-codepipeline/main.go\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger SDK.\n    - It checks for registry credentials in the host environment.\n    - It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `SetSecret()` method to set the registry password as a secret for the Dagger pipeline.\n    - It uses the client's `Host().Directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `Container().From()` method to initialize a new container image from a base image. The additional `platform` argument to the `Container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:18` image and the architecture is `linux/amd64`. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `WithDirectory()` method to return the container image with the host directory written at the `/src` path, and the `WithWorkdir()` method to set the working directory in the container image.\n    - It chains the `WithExec()` method again to install dependencies with `npm install`, build a production image of the application with `npm run build`, and set the default entrypoint argument to `npm start` using the `WithDefaultArgs()` method.\n    - It uses the `WithRegistryAuth()` method to authenticate the Dagger pipeline against the registry using the credentials from the host environment (including the password set as a secret previously)\n    - It invokes the `Publish()` method to publish the container image to the registry. It also prints the SHA identifier of the published image.\n\n1. Run the following command to update `go.sum`:\n\n  ```shell\n  go mod tidy\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. In the application directory, install the Dagger SDK:\n\n  ```shell\n  npm install @dagger.io/dagger@latest--save-dev\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `index.mjs` and add the following code to it.\n\n  ```javascript file=./snippets/aws-codebuild-codepipeline/index.mjs\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger SDK.\n    - It checks for registry credentials in the host environment.\n    - It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `setSecret()` method to set the registry password as a secret for the Dagger pipeline.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from()` method to initialize a new container image from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:18` image and the architecture is `linux/amd64`. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `withDirectory()` method to return the container image with the host directory written at the `/src` path, and the `withWorkdir()` method to set the working directory in the container image.\n    - It chains the `withExec()` method again to install dependencies with `npm install`, build a production image of the application with `npm run build`, and set the default entrypoint argument to `npm start` using the `withDefaultArgs()` method.\n    - It uses the `withRegistryAuth()` method to authenticate the Dagger pipeline against the registry using the credentials from the host environment (including the password set as a secret previously)\n    - It invokes the `publish()` method to publish the container image to the registry. It also prints the SHA identifier of the published image.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. In the application directory, create a virtual environment and install the Dagger SDK:\n\n  ```shell\n  pip install dagger-io\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.py` and add the following code to it.\n\n  ```python file=./snippets/aws-codebuild-codepipeline/main.py\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger SDK.\n    - It checks for registry credentials in the host environment.\n    - It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `set_secret()` method to set the registry password as a secret for the Dagger pipeline.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from_()` method to initialize a new container image from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:18` image and the architecture is `linux/amd64`. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `with_directory()` method to mount the host directory into the container image at the `/src` mount point, and the `with_workdir()` method to set the working directory in the container image.\n    - It chains the `with_exec()` method again to install dependencies with `npm install`, build a production image of the application with `npm run build`, and set the default entrypoint argument to `npm start` using the `with_default_args()` method.\n    - It uses the `with_registry_auth()` method to authenticate the Dagger pipeline against the registry using the credentials from the host environment (including the password set as a secret previously)\n    - It invokes the `publish()` method to publish the container image to the registry. It also prints the SHA identifier of the published image.\n\n</TabItem>\n</Tabs>\n\n:::tip\nMost `Container` object methods return a revised `Container` object representing the new state of the container. This makes it easy to chain methods together. Dagger evaluates pipelines \"lazily\", so the chained operations are only executed when required - in this case, when the container is published. Learn more about [lazy evaluation in Dagger](../api/975146-concepts.mdx#lazy-evaluation).\n:::\n\n## Step 3: Add the build specification file\n\nAWS CodeBuild relies on a [build specification file](https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html) to execute the build. This build specification file defines the stages of the build, and the commands to be run in each stage.\n\n1. In the application directory, create a new file at `buildspec.yml` with the following content:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```yaml file=./snippets/aws-codebuild-codepipeline/buildspec-go.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```yaml file=./snippets/aws-codebuild-codepipeline/buildspec-nodejs.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```yaml file=./snippets/aws-codebuild-codepipeline/buildspec-python.yml\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This build specification defines four steps, as below:\n    - The first step installs the Dagger SDK on the CI runner.\n    - The second step installs the Dagger CLI on the CI runner.\n    - The third step executes the Dagger pipeline.\n    - The fourth step displays a message with the date and time of build completion.\n\n1. Commit the Dagger pipeline and build specification file to the repository:\n\n  ```shell\n  git add buildspec.yml\n  git add ci/*\n  git commit -a -m \"Added Dagger pipeline and build specification\"\n  git push\n  ```\n\n## Step 4: Create an AWS CodePipeline for Dagger\n\nThe final step is to create an AWS CodePipeline to run the Dagger pipeline whenever the source repository changes, as described below.\n\n1. Log in to the [AWS console](https://console.aws.amazon.com).\n1. Navigate to the \"CodePipeline\" section.\n1. Navigate to the \"Pipelines\" page.\n1. Click \"Create pipeline\".\n1. On the \"Create new pipeline\" sequence of pages, input the following details, adjusting them as required for your project:\n    - In the \"Pipeline settings\" section:\n        - Pipeline name: `myapp-pipeline`\n        - Service role: `New service role`\n    - In the \"Source\" section:\n        - Source provider: `AWS CodeCommit`\n        - Repository name: `myapp`\n        - Branch name: `main`\n        - Change detection options: `Amazon CloudWatch Events`\n        - Output artifact format: `CodePipeline default`\n    - In the \"Build\" section:\n        - Build provider: `AWS CodeBuild`\n        - Region: Set value to your region\n        - Project name: `myapp-codebuild-project`\n        - Build type: `Single build`\n    - In the \"Deploy\" section:\n        - Click the `Skip deploy stage` button\n1. On the \"Review\" page, review the inputs and click \"Create pipeline\".\n\nAWS CodePipeline creates a new pipeline.\n\nThe following image visually illustrates the AWS CodePipeline configuration:\n\n![Create CodePipeline](/img/current_docs/guides/aws-codebuild-codepipeline/codepipeline.png)\n\n:::info\nEnvironment variables defined as part of the AWS CodeBuild project configuration are available to AWS CodePipeline as well.\n:::\n\n## Step 5: Test the Dagger pipeline\n\nTest the Dagger pipeline by committing a change to the repository.\n\nIf you are using the example application described in [Appendix A](#appendix-a-create-an-aws-codecommit-repository-with-an-example-express-application), the following commands modify and commit a change to the application's index page:\n\n```shell\ngit pull\necho -e \"export default function Hello() {\\n  return <h1>Hello from Dagger on AWS</h1>;\\n }\" > src/pages/index.js\ngit add src/pages/index.js\ngit commit -m \"Update index page\"\ngit push\n```\n\nThe commit triggers the AWS CodePipeline defined in Step 4. The AWS CodePipeline runs the various steps of the job, including the Dagger pipeline script. At the end of the process, the built container is published to the registry and a message similar to the one below appears in the AWS CodePipeline logs:\n\n```shell\nPublished image to: .../myapp@sha256...\n```\n\nTest the published image by executing the commands below (replace the `IMAGE-ADDRESS` placeholder with the address of the published image):\n\n```shell\ndocker run --rm -p 3000:3000 --name myapp IMAGE-ADDRESS\n```\n\nBrowse to `http://localhost:3000` to see the application running. If you deployed the example application with the modification above, you see the following output:\n\n```shell\nHello from Dagger on AWS\n```\n\n:::tip\nPipelines that pull public images from Docker Hub may occasionally fail with the error \"You have reached your pull rate limit. You may increase the limit by authenticating and upgrading...\". This error occurs due to [Docker Hub's rate limits](https://www.docker.com/increase-rate-limits/). You can resolve this error by adding explicit Docker Hub authentication as the first step in your build specification file, or by copying public images to your own private registry and pulling from there instead. More information is available in this [Amazon blog post providing advice related to Docker Hub rate limits](https://aws.amazon.com/blogs/containers/advice-for-customers-dealing-with-docker-hub-rate-limits-and-a-coming-soon-announcement/).\n:::\n\n## Conclusion\n\nThis tutorial walked you through the process of creating a Dagger pipeline to continuously build and publish a Node.js application using AWS services such as AWS CodeBuild and AWS CodePipeline. It used the Dagger SDKs and explained key concepts, objects and methods available in the SDKs to construct a Dagger pipeline. It also demonstrated the process of integrating the Dagger pipeline with AWS CodePipeline to automatically monitor changes to your source repository and trigger new builds in response.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.\n\n## Appendix A: Create an AWS CodeCommit repository with an example Next.js application\n\nThis tutorial assumes that you have an AWS CodeCommit repository with a Node.js Web application. If not, follow the steps below to create an AWS CodeCommit repository and commit an example Next.js application to it.\n\n1. Create a directory for the Next.js application:\n\n  ```shell\n  mkdir myapp\n  cd myapp\n  ```\n\n1. Create a skeleton Express application:\n\n  ```shell\n  npx create-next-app --js --src-dir --eslint --no-tailwind --no-app --import-alias \"@/*\" .\n  ```\n\n1. Initialize a local Git repository for the application:\n\n  ```shell\n  git init\n  ```\n\n1. Add a `.gitignore` file and commit the application code:\n\n  ```shell\n  echo node_modules >> .gitignore\n  git add .\n  git commit -a -m \"Initial commit\"\n  ```\n\n1. Log in to the [AWS console](https://console.aws.amazon.com/) and perform the following steps:\n    - [Create a new AWS CodeCommit repository](https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-create-repository.html).\n    - [Configure SSH authentication](https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-without-cli.html) for the AWS CodeCommit repository.\n    - [Obtain the SSH clone URL](https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-view-repository-details.html#how-to-view-repository-details-console) for the AWS CodeCommit repository.\n\n1. Add the AWS CodeCommit repository as a remote and push the application code to it. Replace the `SSH-URL` placeholder with the SSH clone URL for the repository.\n\n  ```shell\n  git remote add origin SSH-URL\n  git push -u origin --all\n  ```","contentTitle":"Use Dagger with AWS CodeBuild and AWS CodePipeline","excerpt":"Introduction","timestamp":1686614400000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/620301-azure-pipelines-container-instances.mdx","frontMatter":{"slug":"/620301/azure-pipelines-container-instances","displayed_sidebar":"current","category":"guides","tags":["nodejs","go","python","azure","azure-pipelines","azure-container-instances"],"authors":["Vikram Vaswani"],"date":"2023-05-30"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Use Dagger with Azure Pipelines and Azure Container Instances\n\n## Introduction\n\nThis tutorial teaches you how to use Dagger to continuously build and deploy a Node.js application to Azure Container Instances with Azure Pipelines. You will learn how to:\n\n- Configure an Azure resource group and service principal\n- Create a Dagger pipeline using a Dagger SDK\n- Run the Dagger pipeline on your local host to manually build and deploy the application to Azure Container Instances\n- Use the same Dagger pipeline with Azure Pipelines to automatically build and deploy the application to Azure Container Instances on every repository commit\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a basic understanding of the JavaScript programming language.\n- You have a basic understanding of Azure DevOps and Azure Container Instances. If not, learn about [Azure DevOps](https://learn.microsoft.com/en-us/azure/devops) and [Azure Container Instances](https://azure.microsoft.com/en-us/products/container-instances).\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have the Azure CLI installed. If not, [install the Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).\n- You have a Docker Hub account. If not, [register for a free Docker Hub account](https://hub.docker.com/signup).\n- You have an Azure subscription with \"Owner\" (or higher) privileges. If not, [register for an Azure account](https://azure.microsoft.com/en-us/free/).\n- You have an Azure DevOps project containing a Node.js Web application. This repository should also be cloned locally in your development environment. If not, follow the steps in Appendix A to [create and populate a local and Azure DevOps repository with an example Express application](#appendix-a-create-an-azure-devops-repository-with-an-example-express-application).\n\n## Step 1: Create an Azure resource group and service principal\n\nThe first step is to create an Azure resource group for the container instance, as well as an Azure service principal for the Dagger pipeline.\n\n1. Log in to Azure using the Azure CLI:\n\n  ```shell\n  az login\n  ```\n\n1. Create a new Azure resource group (in this example, a group named `mygroup` in the `useast` location):\n\n  ```shell\n  az group create --location eastus --name my-group\n  ```\n\n  Note the resource group ID (`id` field) in the output, as you will need it when creating the service principal.\n\n1. Create a service principal for the application (here, a principal named `mydaggerprincipal`) and assign it the \"Contributor\" role. Replace the `RESOURCE-GROUP-ID` placeholder in the command with the resource group ID obtained from the previous command.\n\n  ```shell\n  az ad sp create-for-rbac --name my-dagger-principal  --role Contributor --scopes RESOURCE-GROUP-ID\n  ```\n\n  :::info\n  The \"Contributor\" role gives the service principal access to manage all resources in the group, including container instances.\n  :::\n\n  The output of the previous command contains the credentials for the service principal, including the client ID (`appId` field), tenant ID (`tenant` field) and client secret (`password` field). Note these values carefully, as they will not be shown again and you will need them in subsequent steps.\n\n## Step 2: Create the Dagger pipeline\n\nThe next step is to create a Dagger pipeline to do the heavy lifting: build a container image of the application, release it to Docker Hub and deploy it on Azure Container Instances using the service principal from the previous step.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. In the application directory, install the Dagger SDK and the Azure SDK client libraries:\n\n  ```shell\n  go mod init main\n  go get dagger.io/dagger@latest\n  go get github.com/Azure/azure-sdk-for-go/sdk/azcore\n  go get github.com/Azure/azure-sdk-for-go/sdk/azidentity\n  go get github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/containerinstance/armcontainerinstance/v2\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.go` and add the following code to it. Modify the region (`useast`) and resource group name (`my-group`) if you specified different values when creating the Azure resource group in Step 1.\n\n  ```go file=./snippets/azure-pipelines-container-instances/main.go\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger and Azure SDK libraries.\n    - It checks for various required credentials in the host environment.\n    - It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `SetSecret()` method to set the Docker Hub registry password as a secret for the Dagger pipeline.\n    - It uses the client's `Host().Directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `Container().From()` method to initialize a new container from a base image. The additional `Platform` argument to the `Container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:18` image and the architecture is `linux/amd64`. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `WithDirectory()` method to return the container image with the host directory written at the `/src` path, and the `WithWorkdir()` method to set the working directory in the container image.\n    - It chains the `WithExec()` method to copy the contents of the working directory to the `/home/node` directory in the container image and then uses the `WithWorkdir()` method to change the working directory in the container image to `/home/node`.\n    - It chains the `WithExec()` method again to install dependencies with `npm install` and sets the container entrypoint using the `WithEntrypoint()` method.\n    - It uses the container object's `WithRegistryAuth()` method to set the registry credentials (including the password set as a secret previously) and then invokes the `Publish()` method to publish the container image to Docker Hub. It also prints the SHA identifier of the published image.\n    - It creates an Azure client (using the Azure credentials set in the host environment)\n    - It defines a deployment request to create or update a container in the Azure Container Instances service. This deployment request includes the container name, image, port configuration, location and other details.\n    - It submits the deployment request to the Azure Container Instances service and waits for a response. If successful, it prints the public IP address of the running container image.\n\n1. Run the following command to update `go.sum`:\n\n  ```shell\n  go mod tidy\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. In the application directory, install the Dagger SDK and the Azure SDK client libraries as development dependencies:\n\n  ```shell\n  npm install @dagger.io/dagger@latest @azure/arm-containerinstance @azure/identity --save-dev\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `index.mjs` and add the following code to it. Modify the region (`useast`) and resource group name (`my-group`) if you specified different values when creating the Azure resource group in Step 1.\n\n  ```javascript file=./snippets/azure-pipelines-container-instances/index.mjs\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger and Azure SDK libraries.\n    - It checks for various required credentials in the host environment.\n    - It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `setSecret()` method to set the Docker Hub registry password as a secret for the Dagger pipeline.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from()` method to initialize a new container from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:18` image and the architecture is `linux/amd64`. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `withDirectory()` method to return the container image with the host directory written at the `/src` path, and the `withWorkdir()` method to set the working directory in the container image.\n    - It chains the `withExec()` method to copy the contents of the working directory to the `/home/node` directory in the container image and then uses the `withWorkdir()` method to change the working directory in the container image to `/home/node`.\n    - It chains the `withExec()` method again to install dependencies with `npm install` and sets the container entrypoint using the `withEntrypoint()` method.\n    - It uses the container object's `withRegistryAuth()` method to set the registry credentials (including the password set as a secret previously) and then invokes the `publish()` method to publish the container image to Docker Hub. It also prints the SHA identifier of the published image.\n    - It creates an Azure client (using the Azure credentials set in the host environment)\n    - It defines a deployment request to create or update a container in the Azure Container Instances service. This deployment request includes the container name, image, port configuration, location and other details.\n    - It submits the deployment request to the Azure Container Instances service and waits for a response. If successful, it prints the public IP address of the running container image.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. In the application directory, create a virtual environment and install the Dagger SDK and the Azure SDK client libraries:\n\n  ```shell\n  pip install dagger-io aiohttp azure-identity azure-mgmt-containerinstance\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.py` and add the following code to it. Modify the region (`useast`) and resource group name (`my-group`) if you specified different values when creating the Azure resource group in Step 1.\n\n  ```python file=./snippets/azure-pipelines-container-instances/main.py\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger and Azure SDK libraries.\n    - It checks for various required credentials in the host environment.\n    - It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `set_secret()` method to set the Docker Hub registry password as a secret for the Dagger pipeline.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from_()` method to initialize a new container from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:18` image and the architecture is `linux/amd64`. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `with_directory()` method to mount the host directory into the container image at the `/src` mount point, and the `with_workdir()` method to set the working directory in the container image.\n    - It chains the `with_exec()` method to copy the contents of the working directory to the `/home/node` directory in the container image and then uses the `with_eorkdir()` method to change the working directory in the container image to `/home/node`.\n    - It chains the `with_exec()` method again to install dependencies with `npm install` and sets the container entrypoint using the `with_entrypoint()` method.\n    - It uses the container object's `with_registry_auth()` method to set the registry credentials (including the password set as a secret previously) and then invokes the `publish()` method to publish the container image to Docker Hub. It also prints the SHA identifier of the published image.\n    - It creates an Azure client (using the Azure credentials set in the host environment)\n    - It defines a deployment request to create or update a container in the Azure Container Instances service. This deployment request includes the container name, image, port configuration, location and other details.\n    - It submits the deployment request to the Azure Container Instances service and waits for a response. If successful, it prints the public IP address of the running container image.\n\n</TabItem>\n</Tabs>\n\n:::tip\nMost `Container` object methods return a revised `Container` object representing the new state of the container. This makes it easy to chain methods together. Dagger evaluates pipelines \"lazily\", so the chained operations are only executed when required - in this case, when the container is published. Learn more about [lazy evaluation in Dagger](../api/975146-concepts.mdx#lazy-evaluation).\n:::\n\n## Step 3: Test the Dagger pipeline on the local host\n\nConfigure credentials for the Docker Hub registry and the Azure SDK on the local host by executing the commands below, replacing the placeholders as follows:\n\n- Replace the `TENANT-ID`, `CLIENT-ID` and `CLIENT-SECRET` placeholders with the service principal credentials obtained at the end of Step 1.\n- Replace the `SUBSCRIPTION-ID` placeholder with your Azure subscription ID.\n- Replace the `USERNAME` and `PASSWORD` placeholders with your Docker Hub username and password respectively.\n\n```shell\nexport AZURE_TENANT_ID=TENANT-ID\nexport AZURE_CLIENT_ID=CLIENT-ID\nexport AZURE_CLIENT_SECRET=CLIENT-SECRET\nexport AZURE_SUBSCRIPTION_ID=SUBSCRIPTION-ID\nexport DOCKERHUB_USERNAME=USERNAME\nexport DOCKERHUB_PASSWORD=PASSWORD\n```\n\nOnce credentials are configured, test the Dagger pipeline by running the command below:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\ndagger run go run ci/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```shell\ndagger run node ci/index.mjs\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\ndagger run python ci/main.py\n```\n\n</TabItem>\n</Tabs>\n\nDagger performs the operations defined in the pipeline script, logging each operation to the console. At the end of the process, the built container is deployed to Azure Container Instances and a message similar to the one below appears in the console output:\n\n  ```shell\n  Deployment for image docker.io/.../my-app@sha256... now available at ...\n  ```\n\nBrowse to the URL shown in the deployment message to see the running application.\n\nIf you deployed the example application from [Appendix A](#appendix-a-create-an-azure-devops-repository-with-an-example-express-application), you should see a page similar to that shown below:\n\n![Result of running pipeline from local host](/img/current_docs/guides/azure-pipelines-container-instances/local-deployment.png)\n\n## Step 4: Create an Azure Pipeline for Dagger\n\nDagger executes your pipelines entirely as standard OCI containers. This means that the same pipeline will run the same, whether on on your local machine or a remote server.\n\nThis also means that it's very easy to move your Dagger pipeline from your local host to Azure Pipelines - all that's needed is to commit and push the Dagger pipeline script from your local clone to your Azure DevOps repository, and then define an Azure Pipeline to run it on every commit.\n\n1. Commit and push the Dagger pipeline script to the application's repository:\n\n  ```shell\n  git add .\n  git commit -a -m \"Added pipeline\"\n  git push\n  ```\n\n1. Create a new Azure Pipeline:\n\n  ```shell\n  az pipelines create --name dagger --repository my-app --branch master --repository-type tfsgit --yml-path azure-pipelines.yml --skip-first-run true\n  ```\n\n1. Configure credentials for the Docker Hub registry and the Azure SDK in the Azure Pipeline by executing the commands below, replacing the placeholders as follows:\n\n    - Replace the `TENANT-ID`, `CLIENT-ID` and `CLIENT-SECRET` placeholders with the service principal credentials obtained at the end of Step 1.\n    - Replace the `SUBSCRIPTION-ID` placeholder with your Azure subscription ID.\n    - Replace the `USERNAME` and `PASSWORD` placeholders with your Docker Hub username and password respectively.\n\n  ```shell\n  az pipelines variable create --name AZURE_TENANT_ID --value TENANT-ID --pipeline-name dagger\n  az pipelines variable create --name AZURE_CLIENT_ID --value CLIENT-ID --pipeline-name dagger\n  az pipelines variable create --name AZURE_CLIENT_SECRET --value CLIENT-SECRET --pipeline-name dagger --secret true\n  az pipelines variable create --name AZURE_SUBSCRIPTION_ID --value SUBSCRIPTION-ID --pipeline-name dagger\n  az pipelines variable create --name DOCKERHUB_USERNAME --value USERNAME --pipeline-name dagger\n  az pipelines variable create --name DOCKERHUB_PASSWORD --value PASSWORD --pipeline-name dagger --secret true\n  ```\n\n1. In the repository, create a new file at `azure-pipelines.yml` with the following content:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```yaml file=./snippets/azure-pipelines-container-instances/azure-pipeline-go.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```yaml file=./snippets/azure-pipelines-container-instances/azure-pipeline-nodejs.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```yaml file=./snippets/azure-pipelines-container-instances/azure-pipeline-python.yml\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This Azure Pipeline runs on every commit to the repository `master` branch. It consists of a single job with four steps, as below:\n    - The first step uses a language-specific task to download and install the programing language on the CI runner.\n    - The second and third steps download and install the required dependencies (such as the Dagger SDK, the Azure SDK and the Dagger CLI) on the CI runner.\n    - The fourth step adds executes the Dagger pipeline. It also explicity adds those variables defined as secret to the CI runner environment (other variables are automatically injected by Azure Pipelines).\n\n  :::tip\n  Azure Pipelines automatically transfers pipeline variables to the CI runner environment, except for those marked as secret. Secret variables need to be explicitly defined in the Azure Pipelines configuration file.\n  :::\n\n## Step 5: Test the Dagger pipeline in Azure Pipelines\n\nTest the Dagger pipeline by committing a change to the repository.\n\nIf you are using the example application described in [Appendix A](#appendix-a-create-an-azure-devops-repository-with-an-example-express-application), the following commands modify and commit a simple change to the application's index page:\n\n```shell\ngit pull\nsed -i 's/Dagger/Dagger on Azure/g' routes/index.js\ngit add routes/index.js\ngit commit -m \"Update welcome message\"\ngit push\n```\n\nThe commit triggers the Azure Pipeline defined in Step 5. The Azure Pipeline runs the various steps of the job, including the Dagger pipeline script.\n\nAt the end of the process, a new version of the built container image is released to Docker Hub and deployed on Azure Container Instances. A message similar to the one below appears in the Azure Pipelines log:\n\n```shell\nDeployment for image docker.io/.../my-app@sha256:... now available at ...\n```\n\nBrowse to the URL shown in the deployment message to see the running application. If you deployed the example application with the additional modification above, you see a page similar to that shown below:\n\n![Result of running pipeline from Azure Pipelines](/img/current_docs/guides/azure-pipelines-container-instances/azure-pipelines-deployment.png)\n\n## Conclusion\n\nThis tutorial walked you through the process of creating a Dagger pipeline to continuously build and deploy a Node.js application on Azure Container Instances. It used the Dagger SDKs and explained key concepts, objects and methods available in the SDKs to construct a Dagger pipeline.\n\nDagger executes your pipelines entirely as standard OCI containers. This means that pipelines can be tested and debugged locally, and that the same pipeline will run consistently on your local machine, a CI runner, a dedicated server, or any container hosting service. This portability is one of Dagger's key advantages, and this tutorial demonstrated it in action by using the same pipeline on the local host and with Azure Pipelines.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.\n\n## Appendix A: Create an Azure DevOps repository with an example Express application\n\nThis tutorial assumes that you have an Azure DevOps repository with a Node.js Web application. If not, follow the steps below to create an Azure DevOps repository and commit an example Express application to it.\n\n1. Create a directory for the Express application:\n\n  ```shell\n  mkdir my-app\n  cd my-app\n  ```\n\n1. Create a skeleton Express application:\n\n  ```shell\n  npx express-generator\n  ```\n\n1. Make a minor modification to the application's index page:\n\n  ```shell\n  sed -i -e 's/Express/Dagger/g' routes/index.js\n  ```\n\n1. Initialize a local Git repository for the application:\n\n  ```shell\n  git init\n  ```\n\n1. Add a `.gitignore` file and commit the application code:\n\n  ```shell\n  echo node_modules >> .gitignore\n  git add .\n  git commit -a -m \"Initial commit\"\n  ```\n\n1. Log in to Azure using the Azure CLI:\n\n  ```shell\n  az login\n  ```\n\n1. Create a new Azure DevOps project and repository in your Azure DevOps organization. Replace the `ORGANIZATION-URL` placeholder with your Azure DevOps organization URL (usually of the form `https://dev.azure.com/...`).\n\n  ```shell\n  az devops configure --defaults organization=ORGANIZATION-URL\n  az devops project create --name my-app\n  ```\n\n1. List the available repositories and note the value of the `sshUrl` and `webUrl` fields:\n\n  ```shell\n  az repos list --project my-app | grep \"sshUrl\\|webUrl\"\n  ```\n\n1. Browse to the URL shown in the `webUrl` field and [configure SSH authentication for the repository](https://learn.microsoft.com/en-us/azure/devops/repos/git/use-ssh-keys-to-authenticate?view=azure-devops).\n\n1. Add the Azure DevOps repository as a remote and push the application code to it. Replace the `SSH-URL` placeholder with the value of the `sshUrl` field from the previous command.\n\n  ```shell\n  git remote add origin SSH-URL\n  git push -u origin --all\n  ```","contentTitle":"Use Dagger with Azure Pipelines and Azure Container Instances","excerpt":"Introduction","timestamp":1685404800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/541047-alternative-runtimes.mdx","frontMatter":{"slug":"/541047/alternative-runtimes","displayed_sidebar":"current","category":"guides","tags":["podman"],"authors":["Vikram Vaswani"],"date":"2023-04-28"},"content":"# Use Dagger with Alternative OCI Runtimes\n\n## Introduction\n\nThis guide explains how to use Dagger with various OCI-compatible Docker alternatives.\n\n## Approaches\n\nIt is possible to run the Dagger Engine container with any other OCI-compatible container runtime, not just those which are CLI-compatible with Docker. There are two possible approaches.\n\n### Use a `docker` symbolic link\n\nBy default, Dagger tries to invoke the `docker` executable. To use a different container runtime instead, create a symbolic link to it in your system path and name it `docker`. This approach is suitable for runtimes which are CLI-compatible with Docker.\n\n### Run the Dagger Engine container manually\n\nAn alternative approach is to run the Dagger Engine container manually and set the `_EXPERIMENTAL_DAGGER_RUNNER_HOST` environment variable to point to the running container. If this variable is set, then Dagger will instead connect to the endpoint specified there. This approach is suitable for runtimes which are not CLI-compatible with Docker, or when using a customized Dagger Engine container.\n\nThe `_EXPERIMENTAL_DAGGER_RUNNER_HOST` variable currently accepts values in the following format:\n\n| Format | Description |\n|------- | ------------|\n| `docker-container://<container-name>` | Connect to the runner inside the given Docker container. Requires the `docker` CLI to be present and usable. Will result in shelling out to `docker exec`. |\n| `podman-container://<container-name>` | Connect to the runner inside the given Podman container. |\n| `kube-pod://<pod-name>?context=<context>&namespace=<namespace>&container=<container>` | Connect to the runner inside the given Kubernetes pod. Query strings params like `context` and `namespace` are optional.|\n| `unix://<path to unix socket>` | Connect to the runner over the provided UNIX socket. |\n| `tcp://<address:port>` | Connect to the runner over TCP using the provided address and port. No encryption is used.\n\n## Runtimes\n\n### Podman\n\n#### Requirements\n\nThis guide assumes that you have Podman installed and running on the host system. If not, [install Podman](https://podman.io/getting-started/installation).\n\n#### Configuration\n\nPodman is CLI-compatible with Docker and therefore can be used by creating a symbolic link to the Podman executable in your system path and naming it `docker`:\n\n```shell\nsudo ln -s $(which podman) /usr/local/bin/docker\n```\n\n:::note\nRHEL 8.x users may need to additionally execute `modprobe iptable_nat`.\n:::\n\n### Containerd (nerdctl)\n\n#### Requirements\n\nThis guide assumes that you have `nerdctl` installed and running on the host system in rootless mode. If not, [install the full release of `nerdctl`](https://github.com/containerd/nerdctl/releases) and [configure rootless mode](https://github.com/containerd/nerdctl/blob/main/docs/rootless.md).\n\n#### Configuration\n\n`nerdctl` is CLI-compatible with Docker and therefore can be used by creating a symbolic link to the `nerdctl` executable in your system path and naming it `docker`.\n\n- To use `nerdctl` directly, create a symbolic link as below:\n\n  ```shell\n  sudo ln -s $(which nerdctl) /usr/local/bin/docker\n  ```\n\n- To use `nerdctl` via `lima`, create the following shell script at `/usr/local/bin/nerdctl`:\n\n    ```shell\n    #!/bin/sh\n    lima nerdctl \"$@\"\n    ```\n\n  Then, create a symbolic link to the shell script and name it `docker`:\n\n  ```shell\n  sudo ln -s /usr/local/bin/nerdctl /usr/local/bin/docker\n  ```\n\n## Conclusion\n\nThis guide described two approaches to using Dagger with other OCI-compatible container runtimes, and provided additional steps and guidance for Podman and `nerdctl`.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Use Dagger with Alternative OCI Runtimes","excerpt":"Introduction","timestamp":1682640000000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/235290-troubleshooting.mdx","frontMatter":{"slug":"/235290/troubleshooting","displayed_sidebar":"current","category":"guides","tags":[],"authors":["Vikram Vaswani"],"date":"2023-04-27"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Troubleshooting Dagger\n\nThis page describes problems you may encounter when using Dagger, and their solutions.\n\n## Dagger pipeline is unresponsive with a BuildKit error\n\nA Dagger pipeline may hang or become unresponsive, eventually generating a BuildKit error such as `buildkit failed to respond` or `container state improper`.\n\nTo resolve this error, you must stop and remove the Dagger Engine container and (optionally) clear the container state.\n\n1. Stop and remove the Dagger Engine container:\n\n  ```shell\n  DAGGER_ENGINE_DOCKER_CONTAINER=\"$(docker container list --all --filter 'name=^dagger-engine-*' --format '{{.Names}}')\"\n  docker container stop \"$DAGGER_ENGINE_DOCKER_CONTAINER\"\n  docker container rm \"$DAGGER_ENGINE_DOCKER_CONTAINER\"\n  ```\n\n1. Clear unused volumes and data:\n\n  :::info\n  This step is optional. It will remove the cache and result in a slow first run when the container is re-provisioned.\n  :::\n\n  ```shell\n  docker volume prune\n  docker system prune\n  ```\n\nYou should now be able to re-run your Dagger pipeline successfully.\n\n:::tip\nIf you have custom-provisioned the Dagger Engine, please adjust the above commands to your environment.\n:::\n\n## Dagger pipeline is unable to resolve host names after network configuration changes\n\nIf the network configuration of the host changes after the Dagger Engine container starts, Docker does not notify the Dagger Engine of the change. This may cause Dagger pipelines to fail with network-related errors.\n\nAs an example, if the nameserver configuration of the host changes after switching to a different network connection or connecting/disconnecting a VPN result, the Dagger pipeline may fail with DNS resolution errors.\n\nTo resolve this error, you must restart the Dagger Engine container after the host network configuration changes.\n\n```shell\nDAGGER_ENGINE_DOCKER_CONTAINER=\"$(docker container list --all --filter 'name=^dagger-engine-*' --format '{{.Names}}')\"\ndocker restart \"$DAGGER_ENGINE_DOCKER_CONTAINER\"\n```\n\nYou should now be able to re-run your Dagger pipeline successfully.","contentTitle":"Troubleshooting Dagger","excerpt":"This page describes problems you may encounter when using Dagger, and their solutions.","timestamp":1682553600000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/128409-build-test-publish-php.mdx","frontMatter":{"slug":"/128409/build-test-publish-php","displayed_sidebar":"current","category":"guides","tags":["php","laravel"],"authors":["Vikram Vaswani"],"date":"2023-04-17"},"content":"import PartialRunApiClient from '../partials/_run_api_client.mdx';\n\n# Build, Test and Publish a Laravel Web Application with Dagger\n\n## Introduction\n\nDagger SDKs are currently available for [Go](../sdk/go/), [Node.js](../sdk/nodejs/) and [Python](../sdk/python/) and make it easy to develop CI/CD pipelines in those languages. However, even if you're using a different language, you can still use Dagger via the [Dagger GraphQL API](../api/). The Dagger GraphQL API is a unified interface for programming the Dagger Engine that can be accessed and used by any standards-compliant GraphQL client.\n\nThis tutorial demonstrates the above by using PHP with a PHP-based GraphQL client to continuously build, test and publish a Laravel Web application using Dagger. You will learn how to:\n\n- Create a custom client for the Dagger GraphQL API in PHP\n- Connect to the Dagger GraphQL API and run GraphQL queries\n- Create a Dagger pipeline to:\n  - Build a container image of your Laravel application with all required tools and dependencies\n  - Run unit tests for your Laravel application image\n  - Publish the final application image to Docker Hub\n- Run the Dagger pipeline locally using the Dagger CLI\n\n:::tip\nGraphQL has a [large and growing list of client implementations](https://graphql.org/code/#language-support) for over 20 languages.\n:::\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a basic understanding of how Dagger works. If not, [read the Dagger Quickstart](../quickstart/index.mdx).\n- You have a PHP development environment with PHP 8.2.x and the Composer package manager installed. If not, [install PHP](https://www.php.net/downloads.php) and [install Composer](https://getcomposer.org/doc/00-intro.md).\n- You have Docker installed and running in your development environment. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have a Docker Hub account. If not, [register for a free Docker Hub account](https://hub.docker.com/signup).\n- You have a [Laravel](https://laravel.com/) 10.x Web application with a Docker entrypoint script for startup operations, such as running database migrations. If not, follow the steps in Appendix A to [create a skeleton Laravel Web application and entrypoint script](#appendix-a-create-a-laravel-web-application).\n\n:::info\nThis tutorial assumes a Laravel Web application, but the steps and code samples described below can easily be adapted for use with any other PHP Web application.\n:::\n\n## Step 1: Install a GraphQL client for PHP\n\nThe first step is to install a GraphQL client for PHP.  This tutorial uses the [php-graphql-client library](https://github.com/mghoneimy/php-graphql-client), available under the MIT License.\n\nAdd the client to your application manifest and install it as follows:\n\n```shell\ncomposer require gmostafa/php-graphql-client --with-all-dependencies\n```\n\n## Step 2: Create the Dagger pipeline\n\nWithin the application directory, create a new directory and file at `ci/dagger.php` and add the following code to it:\n\n```php file=snippets/build-test-publish-php/dagger.php\n```\n\nThis code listing consists of two parts:\n\n- A `DaggerPipeline` class with methods encapsulating the Dagger pipeline operations: build, test and publish.\n- A pipeline script invoking the various class methods.\n\nThe steps performed by the pipeline script are:\n\n- Create a Dagger GraphQL API client\n- Build a test image\n- Run unit tests\n- Build a production image\n- Publish the image\n\nThese steps are visible in the following code extract:\n\n```php\n// run pipeline\ntry {\n  $p = new DaggerPipeline();\n\n  // build test image\n  echo \"Building test image...\" . PHP_EOL;\n  $testImage = $p->buildTestImage();\n  echo \"Test image built.\" . PHP_EOL;\n\n  // test\n  echo \"Running tests in test image...\" . PHP_EOL;\n  $result = $p->runUnitTests($testImage);\n  echo \"Tests completed.\" . PHP_EOL;\n\n  // build production image\n  echo \"Building production image...\" . PHP_EOL;\n  $prodImage = $p->buildProductionImage();\n  echo \"Production image built.\" . PHP_EOL;\n\n  // publish\n  echo \"Publishing production image...\" . PHP_EOL;\n  $address = $p->publishImage($prodImage);\n  echo \"Production image published at: $address\" . PHP_EOL;\n} catch (Exception $e) {\n  print_r($e->getMessage());\n  exit;\n}\n```\n\nIf any of the steps produce an error (for example, due to a unit test failure), the pipeline will terminate.\n\nThe following sections describe these steps in more detail.\n\n### Create a Dagger GraphQL API client\n\nThe `DaggerPipeline` class constructor initializes a new GraphQL client for the Dagger GraphQL API and assigns it as a class member, as shown in the following extract:\n\n```php\nclass DaggerPipeline {\n  // ...\n\n  private $client;\n\n  // constructor\n  public function __construct() {\n    // initialize client with\n    // endpoint from environment\n    $sessionPort = getenv('DAGGER_SESSION_PORT') or throw new Exception(\"DAGGER_SESSION_PORT environment variable must be set\");\n    $sessionToken = getenv('DAGGER_SESSION_TOKEN') or throw new Exception(\"DAGGER_SESSION_TOKEN environment variable must be set\");\n    $this->client = new Client(\n      'http://127.0.0.1:' . $sessionPort . '/query',\n      ['Authorization' => 'Basic ' . base64_encode($sessionToken . ':')]\n    );\n  }\n\n  // ...\n}\n```\n\nThe API endpoint and the HTTP authentication token for the GraphQL client are not statically defined, they must be retrieved at run-time from the special `DAGGER_SESSION_PORT` and `DAGGER_SESSION_TOKEN` environment variables. This is explained in detail later.\n\n### Build a test image\n\nThe `buildTestImage()` method builds an image of the application for testing. Internally, this method calls the `buildApplicationImage()` method, which in turn calls the `buildRuntimeImage()` method. Here's what these methods look like:\n\n```php\nclass DaggerPipeline {\n  // ...\n\n  // build runtime image\n  public function buildRuntimeImage() {\n    // build runtime image\n    // install tools and PHP extensions\n    // configure Apache webserver root and rewriting\n    $runtimeQuery = <<<QUERY\n    query {\n      container (platform: \"linux/amd64\") {\n        from(address: \"$this->phpImage\") {\n          withExec(args: [\"apt-get\", \"update\"]) {\n            withExec(args: [\"apt-get\", \"install\", \"--yes\", \"git-core\"]) {\n              withExec(args: [\"apt-get\", \"install\", \"--yes\", \"zip\"]) {\n                withExec(args: [\"apt-get\", \"install\", \"--yes\", \"curl\"]) {\n                  withExec(args: [\"docker-php-ext-install\", \"pdo\", \"pdo_mysql\", \"mysqli\"]) {\n                    withExec(args: [\"sh\", \"-c\", \"sed -ri -e 's!/var/www/html!/var/www/public!g' /etc/apache2/sites-available/*.conf\"]) {\n                      withExec(args: [\"sh\", \"-c\", \"sed -ri -e 's!/var/www/!/var/www/public!g' /etc/apache2/apache2.conf /etc/apache2/conf-available/*.conf\"]) {\n                        withExec(args: [\"a2enmod\", \"rewrite\"]) {\n                          id\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    QUERY;\n    $runtime = $this->executeQuery($runtimeQuery);\n    return $runtime;\n  }\n\n  // build application image\n  public function buildApplicationImage() {\n    // get runtime image\n    $runtime = $this->buildRuntimeImage();\n\n    // get host working directory\n    $sourceQuery = <<<QUERY\n    query {\n      host {\n        directory (path: \".\", exclude: [\"vendor\", \"ci\"]) {\n          id\n        }\n      }\n    }\n    QUERY;\n    $sourceDir = $this->executeQuery($sourceQuery);\n\n    // add application source code\n    // set file permissions\n    // set environment variables\n    $appQuery = <<<QUERY\n    query {\n      container (id: \"$runtime\") {\n        withDirectory(path: \"/var/www\", directory: \"$sourceDir\") {\n          withWorkdir(path: \"/var/www\") {\n            withExec(args: [\"chown\", \"-R\", \"www-data:www-data\", \"/var/www\"]) {\n              withExec(args: [\"chmod\", \"-R\", \"777\", \"/var/www/storage\"]) {\n                withExec(args: [\"chmod\", \"+x\", \"/var/www/docker-entrypoint.sh\"]) {\n                  id\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    QUERY;\n    $app = $this->executeQuery($appQuery);\n\n    // install Composer\n    // add application dependencies\n    $appWithDepsQuery = <<<QUERY\n    query {\n      container (id: \"$app\") {\n        withExec(args: [\"sh\", \"-c\", \"curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer\"]) {\n          withWorkdir(path: \"/var/www\") {\n            withExec(args: [\"composer\", \"install\"]) {\n              id\n            }\n          }\n        }\n      }\n    }\n    QUERY;\n    $appWithDeps = $this->executeQuery($appWithDepsQuery);\n    return $appWithDeps;\n  }\n\n  // ...\n}\n```\n\n1. The `buildRuntimeImage()` method executes a GraphQL query to construct a runtime image. This runtime image consists of the PHP interpreter, Apache webserver, and required tools and extensions. It uses the `container.from()` method to initialize a new container from the `php:8.2-apache-buster` image. It then chains multiple `container.withExec()` methods to add tools, PHP extensions and Apache configuration to the image.\n1. The `buildApplicationImage()` method uses the image produced by `buildRuntimeImage()` and executes three additional GraphQL queries:\n    - The first query obtains a reference to the source code directory of the application on the host using the `host.directory()` API method.\n    - The next query continues building the image. It uses the `container.withDirectory()` method to return the container with the source code directory written at `/var/www`. It then chains multiple `container.withExec()` methods to set various file permissions and environment variables.\n    - The final query installs Composer in the image and runs `composer install` to download all the required application dependencies.\n\n:::info\nGraphQL query resolution is triggered only when a leaf value (scalar) is requested. Dagger leverages this lazy evaluation model to optimize and parallelize pipelines for maximum speed and performance. This implies that the queries above are not actually executed until necessary to return output (such as the result of a command or an exit code) to the requesting client. [Learn more about lazy evaluation in Dagger](../api/975146-concepts.mdx#lazy-evaluation).\n:::\n\nOnce the application image is constructed, control returns to the `buildTestImage()` method:\n\n```php\nclass DaggerPipeline {\n  // ...\n\n  // build image for testing\n  public function buildTestImage() {\n    // build base image\n    $image = $this->buildApplicationImage();\n\n    // set test-specific variables\n    $appTestQuery = <<<QUERY\n    query {\n      container (id: \"$image\") {\n        withEnvVariable(name: \"APP_DEBUG\", value: \"true\") {\n          withEnvVariable(name: \"LOG_LEVEL\", value: \"debug\") {\n            id\n          }\n        }\n      }\n    }\n    QUERY;\n    $appTest = $this->executeQuery($appTestQuery);\n    return $appTest;\n  }\n\n  // ...\n}\n```\n\nThe `buildTestImage()` method executes one additional GraphQL query to add test-specific configuration to the image. It activates Laravel's detailed error logging by setting the `APP_DEBUG` and `LOG_LEVEL` environment variables in the container, using the `container.withEnvVariable()` API method.\n\n### Run unit tests\n\nThe `runUnitTests()` method accepts an image reference and runs unit tests in the image. Here's what it looks like:\n\n```php\nclass DaggerPipeline {\n  // ...\n\n  // run unit tests\n  public function runUnitTests($image) {\n    // create database service container\n    $dbQuery = <<<QUERY\n    query {\n      container {\n        from(address: \"$this->mariadbImage\") {\n          withEnvVariable(name: \"MARIADB_DATABASE\", value: \"t_db\") {\n            withEnvVariable(name: \"MARIADB_USER\", value: \"t_user\") {\n              withEnvVariable(name: \"MARIADB_PASSWORD\", value: \"t_password\") {\n                withEnvVariable(name: \"MARIADB_ROOT_PASSWORD\", value: \"root\") {\n                  withExposedPort(port: 3306) {\n                    withExec(args: []) {\n                      id\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    QUERY;\n    $db = $this->executeQuery($dbQuery);\n\n    // bind database service to application image\n    // set database credentials for application\n    // run all PHPUnit tests\n    $testQuery = <<<QUERY\n    query {\n      container (id: \"$image\") {\n        withServiceBinding(alias: \"mariadb\", service: \"$db\") {\n          withEnvVariable(name: \"DB_HOST\", value: \"mariadb\") {\n            withEnvVariable(name: \"DB_USERNAME\", value: \"t_user\") {\n              withEnvVariable(name: \"DB_PASSWORD\", value: \"t_password\") {\n                withEnvVariable(name: \"DB_DATABASE\", value: \"t_db\") {\n                  withWorkdir(path: \"/var/www\") {\n                    withExec(args: [\"./vendor/bin/phpunit\"]) {\n                      stdout\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    QUERY;\n    $test = $this->executeQuery($testQuery);\n    return $test;\n  }\n\n  // ...\n}\n```\n\nThe `runUnitTests()` method executes two GraphQL queries:\n\n1. The first query initializes a database service container, against which the application's unit tests will be run. It uses the `container.from()` method to initialize a new container from the `mariadb:10.11.2` image. It then chains multiple `container.withEnvVariable()` methods to configure the database service, and the `container.withExposedPort()` method to ensure that the service is available before allowing clients access.\n1. The second query uses the test image returned by the `buildTestImage()` method and adds a service binding for the database service to it using the `container.withServiceBinding()` API method. It then chains multiple `container.withEnvVariable()` methods to configure the database service credentials for the Laravel application. Finally, it uses the `container.withExec()` method to launch the PHPUnit test runner and return the output stream (the test summary).\n\n:::tip\nWhen creating the database service container, using the `Container.withExposedPort` field is important. Without this field, Dagger will start the service container and immediately allow access to the test runner, without waiting for the service to start listening. This can result in test failures if the test runner is unable to connect to the service. With this field, Dagger will wait for the service to be listening first before allowing the test runner access to it. [Learn more about service containers in Dagger](./757394-use-services.mdx).\n:::\n\n### Build a production image\n\nThe `buildProductionImage()` method builds an image of the application for production. Internally, this method also calls the `buildApplicationImage()` method. Here's what it looks like:\n\n```php\nclass DaggerPipeline {\n  // ...\n\n  // build image for production\n  public function buildProductionImage() {\n    // build base image\n    $image = $this->buildApplicationImage();\n\n    // set production-specific variables\n    $appProductionQuery = <<<QUERY\n    query {\n      container (id: \"$image\") {\n        withEnvVariable(name: \"APP_DEBUG\", value: \"false\") {\n          withLabel(name: \"org.opencontainers.image.title\", value: \"Laravel with Dagger\") {\n            withEntrypoint(args: \"/var/www/docker-entrypoint.sh\") {\n              id\n            }\n          }\n        }\n      }\n    }\n    QUERY;\n    $appProduction = $this->executeQuery($appProductionQuery);\n    return $appProduction;\n  }\n\n  // ...\n}\n```\n\nThe `buildProductionImage()` method references the base image and executes one additional GraphQL query to add production-specific configuration to the image. More specifically, it turns off detailed application error messages for greater security using the `container.withEnvVariable()` API method. It also sets an [OpenContainer annotation](https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys) for the container using the `container.withLabel()` API method.\n\n### Publish the image\n\nThe `publishImage()` method accepts an image reference and publishes the corresponding image to Docker Hub. Here's what it looks like:\n\n```php\nclass DaggerPipeline {\n  // ...\n\n  // publish image to registry\n  public function publishImage($image) {\n    // retrieve registry address and credentials from host environment\n    $registryAddress = getenv(\"REGISTRY_ADDRESS\", true) ?: \"docker.io\";\n    $registryUsername = getenv(\"REGISTRY_USERNAME\") or throw new Exception(\"REGISTRY_USERNAME environment variable must be set\");\n    $registryPassword = getenv(\"REGISTRY_PASSWORD\") or throw new Exception(\"REGISTRY_PASSWORD environment variable must be set\");\n    $containerAddress = getenv(\"CONTAINER_ADDRESS\");\n    if (empty($containerAddress)) {\n      $containerAddress = \"$registryUsername/laravel-dagger\";\n    }\n\n    // set registry password as Dagger secret\n    $registryPasswordSecretQuery = <<<QUERY\n    query {\n      setSecret(name: \"password\", plaintext: \"$registryPassword\") {\n        id\n      }\n    }\n    QUERY;\n    $registryPasswordSecret = $this->executeQuery($registryPasswordSecretQuery);\n\n    // authenticate to registry\n    // publish image\n    $publishQuery = <<<QUERY\n    query {\n      container (id: \"$image\") {\n        withRegistryAuth(address: \"$registryAddress\", username: \"$registryUsername\", secret: \"$registryPasswordSecret\") {\n          publish(address: \"$containerAddress\")\n        }\n      }\n    }\n    QUERY;\n    $address = $this->executeQuery($publishQuery);\n    return $address;\n  }\n\n  // ...\n}\n```\n\nThe `publishImage()` method expects to source the registry credentials from the host environment. It defaults to `docker.io` for the registry address, although this can be overridden from the host environment. It uses PHP's `getenv()` method to retrieve these details and then executes two GraphQL queries:\n\n1. The first query creates a Dagger secret to store the registry password, via the `setSecret()` API method.\n1. The second query authenticates and publishes the image to the specified registry. It uses the `container.withRegistryAuth()` API method for authentication, and the `container.publish()` method for the publishing operation. The `container.publish()` method returns the address and hash for the published image.\n\n:::tip\nUsing a Dagger secret for confidential information ensures that the information is never exposed in plaintext logs, in the filesystem of containers you're building, or in any cache. Dagger also automatically scrubs secrets from its various logs and output streams. This ensures that sensitive data does not leak - for example, in the event of a crash. [Learn more about secrets in Dagger](./723462-use-secrets.mdx).\n:::\n\n## Step 3: Run the Dagger pipeline\n\nConfigure the registry credentials using environment variable on the local host. Although you can use any registry, this guide assumes usage of Docker Hub. Replace the `USERNAME` and `PASSWORD` placeholders with your Docker Hub credentials.\n\n```shell\nexport REGISTRY_USERNAME=USERNAME\nexport REGISTRY_PASSWORD=PASSWORD\n```\n\n<PartialRunApiClient />\n\nRun the pipeline as below:\n\n```shell\ndagger --silent run php ci/dagger.php\n```\n\n:::tip\nFor more detailed logs, remove the `--silent` option and add the `--debug` option to the `dagger run` command. [Learn more about the Dagger CLI](../cli/index.mdx).\n:::\n\nThis command:\n\n- initializes a new Dagger Engine session;\n- sets the `DAGGER_SESSION_PORT` and `DAGGER_SESSION_TOKEN` environment variables;\n- executes the PHP pipeline script in that session.\n\nThe pipeline script, in turn, initializes a new `DaggerPipeline` object, whose constructor:\n\n- reads the above environment variables;\n- creates a new GraphQL API client;\n- connects to the API endpoint specified in the `DAGGER_SESSION_PORT` environment variable;\n- sets an HTTP Basic authentication token with `DAGGER_SESSION_TOKEN`.\n\nThe remainder of the pipeline is then executed as described in the previous section. Here is an example of the output from a successful run:\n\n```shell\nBuilding test image...\nTest image built.\nRunning tests in test image...\nTests completed.\nBuilding production image...\nProduction image built.\nPublishing production image...\nProduction image published at: docker.io/.../laravel-dagger@sha256:aa43...\n```\n\nHere is an example of the output from an unsuccessful run due to a failed unit test:\n\n```shell\nBuilding test image...\nTest image built.\nRunning tests in test image...\nprocess \"docker-php-entrypoint ./vendor/bin/phpunit\" did not complete successfully: exit code: 2\nStdout:\n...\n2) Tests\\Feature\\ProfileTest::test_profile_information_can_be_updated\nFailed asserting that two strings are identical.\n--- Expected\n+++ Actual\n@@ @@\n-'Test User'\n+'Brooke McDermott'\n\n/var/www/tests/Feature/ProfileTest.php:41\n/var/www/vendor/laravel/framework/src/Illuminate/Foundation/Testing/TestCase.php:173\n\nFAILURES!\nTests: 24, Assertions: 52, Failures: 2.\nStderr:\n```\n\nTest the published image by executing the commands below (replace the `USERNAME` placeholder with your registry username) and then browse to `http://localhost` to see the Laravel application running (by default, on port 80 of the Docker host):\n\n```shell\ndocker run --rm --detach -p 3306:3306 --name my-mariadb --env MARIADB_USER=user --env MARIADB_PASSWORD=password --env MARIADB_DATABASE=laravel --env MARIADB_ROOT_PASSWORD=secret  mariadb:10.11.2\n\ndocker run --rm --detach --net=host --name my-app -e DB_HOST=\"127.0.0.1\" -e DB_USERNAME=\"user\" -e DB_PASSWORD=\"password\" -e DB_DATABASE=\"laravel\" USERNAME/laravel-dagger:latest\n```\n\n## Conclusion\n\nDagger SDKs are currently available for Go, Node.js and Python, but that doesn't mean you're restricted to only these languages when defining your Dagger CI/CD pipelines. You can use any standards-compatible GraphQL client to interact with the Dagger Engine from your favorite programming language.\n\nThis tutorial demonstrated by creating a PHP-based Dagger pipeline to build, test and publish a Laravel Web application. A similar approach can be followed for any PHP application, or in any other programming language with a GraphQL client implementation.\n\nUse the [API Reference](https://docs.dagger.io/api/reference) and the [CLI Reference](../cli/979595-reference.mdx) to learn more about the Dagger GraphQL API and the Dagger CLI respectively.\n\n## Appendix A: Create a Laravel Web application\n\nThis tutorial assumes that you have a Laravel 10.x Web application. If not, follow the steps below to create one.\n\n:::info\nThe Laravel CLI requires `npm` for some of its operations, so the following steps assume that you have Node.js 18.x and `npm` installed. If you don't, [install Node.js](https://nodejs.org/en/download/) and [install `npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) before proceeding.\n:::\n\n1. Install the Laravel CLI:\n\n  ```shell\n  composer global require laravel/installer\n  ```\n\n1. Add the Composer vendor directory to your system path:\n\n  ```shell\n  export PATH=$PATH:$HOME/.composer/vendor/bin\n  ```\n\n1. Create a skeleton application with the [Laravel Breeze scaffolding](https://laravel.com/docs/10.x/starter-kits#laravel-breeze):\n\n  ```shell\n  laravel new --breeze --stack=blade --phpunit --no-interaction myapp\n  ```\n\n1. Create a Docker entrypoint script named `docker-entrypoint.sh` in the application directory to handle startup operations, such as running database migrations:\n\n  ```shell\n  cat > docker-entrypoint.sh <<EOF\n  #!/bin/bash\n  php artisan migrate\n  apache2-foreground\n  EOF\n  ```","contentTitle":"Build, Test and Publish a Laravel Web Application with Dagger","excerpt":"Introduction","timestamp":1681689600000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/252029-load-images-local-docker-engine.mdx","frontMatter":{"slug":"/252029/load-images-local-docker-engine","displayed_sidebar":"current","category":"guides","authors":["Vikram Vaswani"],"tags":["go","python","nodejs"],"date":"2023-03-31"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Load Container Images into a Local Docker Engine\n\n## Introduction\n\nThere are two possible approaches to loading container images built with Dagger into a local Docker engine. This tutorial describes them both in detail, although only the first one is recommended.\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n\n## Approach 1: Use an exported tarball\n\nThis approach involves and exporting the container image to the host filesystem as a TAR file with Dagger, and then loading it into Docker. Here's an example:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/load-images-local-docker-engine/export/main.go\n```\n\nThis code listing performs the following operations:\n\n- It imports the Dagger client library.\n- It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n- It uses the client's `Container().From()` method to initialize a new container from a base image (`nginx:1.23-alpine`). The additional `Platform` argument to the `Container()` method instructs Dagger to build for a specific architecture (`linux/amd64`). This method returns a `Container` representing an OCI-compatible container image.\n- It uses the previous `Container` object's `WithNewFile()` method to create a new file at the NGINX web server root and return the result as a new `Container`.\n- It uses the `Export()` method to write the final container image to the host filesystem as a TAR file at `/tmp/my-nginx.tar`.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/load-images-local-docker-engine/export/index.mjs\n```\n\nThis code listing performs the following operations:\n\n- It imports the Dagger client library.\n- It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n- It uses the client's `container().from()` method to initialize a new container from a base image (`nginx:1.23-alpine`). The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture (`linux/amd64`). This method returns a `Container` representing an OCI-compatible container image.\n- It uses the previous `Container` object's `withNewFile()` method to create a new file at the NGINX web server root and return the result as a new `Container`.\n- It uses the `export()` method to write the final container image to the host filesystem as a TAR file at `/tmp/my-nginx.tar`.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/load-images-local-docker-engine/export/main.py\n```\n\nThis code listing performs the following operations:\n\n- It imports the Dagger client library.\n- It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n- It uses the client's `container().from_()` method to initialize a new container from a base image (`nginx:1.23-alpine`). The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture (`linux/amd64`). This method returns a `Container` representing an OCI-compatible container image.\n- It uses the previous `Container` object's `with_new_file()` method to create a new file at the NGINX web server root and return the result as a new `Container`.\n- It uses the `export()` method to write the final container image to the host filesystem as a TAR file at `/tmp/my-nginx.tar`.\n\n</TabItem>\n</Tabs>\n\nOnce exported, the image can be imported into Docker with `docker load`:\n\n```shell\ndocker load -i /tmp/my-nginx.tar\n...\nLoaded image ID: sha256:2e340...\n```\n\nOnce imported, the image can be used like any other Docker image. For example, to run the sample image above, use the following `docker run` command:\n\n```shell\ndocker run --rm --net=host 2e340...\n```\n\n## Approach 2: Use a local registry server\n\n:::danger\nThis approach is significantly more complex than the previous one and is therefore not recommended. It is included only for documentation completeness.\n:::\n\n:::danger\nThe commands in this section deploy a local registry server without authentication or TLS. This is highly insecure and should only be used for local development, debugging and testing. Refer to the Docker documentation for details on [how to deploy a more secure and production-ready registry](https://docs.docker.com/registry/deploying/).\n:::\n\nThis approach involves publishing the container image to a local registry, and then pulling from it as usual with Docker. Follow the steps below:\n\n1. Deploy a local container registry with Docker. This local registry must be configured to run in the same network as the Dagger Engine container, and must use a host volume for the registry data.\n\n  :::tip\n  At this point, the local registry must run in the same network as the Dagger Engine container so that Dagger is able to communicate with it using the `localhost` or `127.0.0.1` network address.\n  :::\n\n  ```shell\n  DAGGER_CONTAINER_NETWORK_NAME=`docker ps --filter \"name=^dagger-engine-*\" --format '{{.Names}}'`\n\n  docker run -d --rm --name registry --network container:$DAGGER_CONTAINER_NETWORK_NAME -v /opt/docker-registry/data:/var/lib/registry registry:2\n  ```\n\n1. Create a Dagger pipeline to build and push a container image to the local registry:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```go file=./snippets/load-images-local-docker-engine/push/main.go\n  ```\n\n  This code listing performs the following operations:\n    - It imports the Dagger client library.\n    - It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `Container().From()` method to initialize a new container from a base image (`nginx:1.23-alpine`). The additional `Platform` argument to the `Container()` method instructs Dagger to build for a specific architecture (`linux/amd64`). This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `WithNewFile()` method to create a new file at the NGINX web server root and return the result as a new `Container`.\n    - It uses the `Publish()` method to publish the final container image to the local registry.\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```javascript file=./snippets/load-images-local-docker-engine/push/index.mjs\n  ```\n\n  This code listing performs the following operations:\n    - It imports the Dagger client library.\n    - It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `container().from()` method to initialize a new container from a base image (`nginx:1.23-alpine`). The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture (`linux/amd64`). This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `withNewFile()` method to create a new file at the NGINX web server root and return the result as a new `Container`.\n    - It uses the `publish()` method to publish the final container image to the local registry.\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```python file=./snippets/load-images-local-docker-engine/push/main.py\n  ```\n\n  This code listing performs the following operations:\n    - It imports the Dagger client library.\n    - It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `container().from_()` method to initialize a new container from a base image (`nginx:1.23-alpine`). The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture (`linux/amd64`). This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `with_new_file()` method to create a new file at the NGINX web server root and return the result as a new `Container`.\n    - It uses the `publish()` method to publish the final container image to the local registry.\n\n  </TabItem>\n  </Tabs>\n\n1. Run the Dagger pipeline:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```shell\n  dagger run go run ci/main.go\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```shell\n  dagger run node ci/index.mjs\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```shell\n  dagger run python ci/main.py\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  The `dagger run` command executes the script in a Dagger session and displays live progress. At the end of the process, the built container is pushed to the local registry and a message similar to the one below appears in the console output:\n\n  ```shell\n  Published at: 127.0.0.1:5000/my-nginx:1.0@sha256:c59a...\n  ```\n\n1. Stop the local registry, then restart it after detaching it from the Dagger Engine container network and publishing the registry server port so that it can be used from the Docker host.\n\n  :::tip\n  At this point, the local registry must run in the same network as the host so that you can use it via the `localhost` or `127.0.0.1` network address.\n  :::\n\n  ```shell\n  docker stop registry\n\n  docker run -d --rm --name registry -p 5000:5000 -v /opt/docker-registry/data:/var/lib/registry  registry:2\n  ```\n\nThe image can now be pulled from the registry and used like any other Docker image. For example, to run the sample image above, use the following `docker run` command:\n\n```shell\ndocker run --net=host 127.0.0.1:5000/my-nginx:1.0@sha256:c59a...\n```\n\n## Conclusion\n\nThis tutorial walked you through two approaches to building container images with Dagger purely for local use: exporting the image as a tarball and loading it into Docker, or pushing the image to a local container registry.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Load Container Images into a Local Docker Engine","excerpt":"Introduction","timestamp":1680220800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/723462-use-secrets.mdx","frontMatter":{"slug":"/723462/use-secrets","displayed_sidebar":"current","category":"guides","tags":["go","python","nodejs"],"authors":["Vikram Vaswani"],"date":"2023-03-28"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Use Secrets in Dagger\n\n## Introduction\n\nDagger allows you to utilize confidential information, such as passwords, API keys, SSH keys and so on, when running your Dagger pipelines, without exposing those secrets in plaintext logs, writing them into the filesystem of containers you're building, or inserting them into cache.\n\nThis tutorial teaches you the basics of using secrets in Dagger.\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n\n## Create and use a secret\n\nThe Dagger API provides the following queries and fields for working with secrets:\n\n- The `setSecret` query creates a new secret from a plaintext value.\n- A `Container`'s `withMountedSecret()` field returns the container with the secret mounted at the named  filesystem path.\n- A `Container`'s `withSecretVariable()` field returns the container with the secret stored in the named container environment variable.\n\nOnce a secret is loaded into Dagger, it can be used in a Dagger pipeline as either a variable or a mounted file. Some Dagger SDK methods additionally accept secrets as native objects.\n\nLet's start with a simple example of setting a secret in a Dagger pipeline and using it in a container.\n\nThe following code listing creates a Dagger secret for a GitHub personal access token and then uses the token to authorize a request to the GitHub API. To use this listing, replace the `TOKEN` placeholder with your personal GitHub access token.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/raw/main.go\n```\n\nIn this code listing:\n\n- The client's `SetSecret()` method accepts two parameters - a unique name for the secret, and the secret value - and returns a new `Secret` object\n- The `WithSecretVariable()` method also accepts two parameters - an environment variable name and a `Secret` object. It returns a container with the secret value assigned to the specified environment variable.\n- The environment variable can then be used in subsequent container operations - for example, in a command-line `curl` request, as shown above.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/raw/index.mjs\n```\n\nIn this code listing:\n\n- The client's `setSecret()` method accepts two parameters - a unique name for the secret, and the secret value - and returns a new `Secret` object\n- The `withSecretVariable()` method also accepts two parameters - an environment variable name and a `Secret` object. It returns a container with the secret value assigned to the specified environment variable.\n- The environment variable can then be used in subsequent container operations - for example, in a command-line `curl` request, as shown above.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/raw/main.py\n```\n\nIn this code listing:\n\n- The client's `set_secret()` method accepts two parameters - a unique name for the secret, and the secret value - and returns a new `Secret` object\n- The `with_secret_variable()` method also accepts two parameters - an environment variable name and a `Secret` object. It returns a container with the secret value assigned to the specified environment variable.\n- The environment variable can then be used in subsequent container operations - for example, in a command-line `curl` request, as shown above.\n\n</TabItem>\n</Tabs>\n\n## Use secrets from the host environment\n\nMost of the time, it's neither practical nor secure to define plaintext secrets directly in your pipeline. That's why Dagger lets you read secrets from the host, either from host environment variables or from the host filesystem, or from external providers.\n\nHere's a revision of the previous example, where the secret is read from a host environment variable. To use this listing, create a host environment variable named `GH_SECRET` and assign it the value of your GitHub personal access token.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/host-env/main.go\n```\n\nThis code listing assumes the existence of a host environment variable named `GH_SECRET` containing the secret value. It performs the following operations:\n\n- It reads the value of the host environment variable using the `os.Getenv()` function.\n- It creates a Dagger secret with that value using the `SetSecret()` method.\n- It uses the `WithSecretVariable()` method to return a container with the secret value assigned to an environment variable in the container.\n- It uses the secret in subsequent container operations, as explained previously.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/host-env/index.mjs\n```\n\nThis code listing assumes the existence of a host environment variable named `GH_SECRET` containing the secret value. It performs the following operations:\n\n- It reads the value of the host environment variable using the `process.env` object.\n- It creates a Dagger secret with that value using the `setSecret()` method.\n- It uses the `withSecretVariable()` method to return a container with the secret value assigned to an environment variable in the container.\n- The environment variable can then be used in subsequent container operations, as explained previously.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/host-env/main.py\n```\n\nThis code listing assumes the existence of a host environment variable named `GH_SECRET` containing the secret value. It performs the following operations:\n\n- It reads the value of the host environment variable using the `os.environ` object.\n- It creates a Dagger secret with that value using the `set_secret()` method.\n- It uses the `with_secret_variable()` method to return a container with the secret value assigned to an environment variable in the container.\n- The environment variable can then be used in subsequent container operations, as explained previously.\n\n</TabItem>\n</Tabs>\n\n## Use secrets from the host filesystem\n\nDagger also lets you mount secrets as files within a container's filesystem. This is useful for tools that look for secrets in a specific filesystem location - for example, GPG keyrings, SSH keys or file-based authentication tokens.\n\nAs an example, consider the [GitHub CLI](https://cli.github.com/), which reads its authentication token from a file stored in the user's home directory at `~/.config/gh/hosts.yml`. The following code listing demonstrates how to mount this file at a specific location in a container as a secret, so that the GitHub CLI is able to find it.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/host-fs/main.go\n```\n\nThis code listing assumes the existence of a GitHub CLI configuration file containing an authentication token at `/home/USER/.config/gh/hosts.yml`. It performs the following operations:\n\n- It reads the host file's contents into a string and loads the string into a secret with the Dagger client's `SetSecret()` method. This method returns a new `Secret` object.\n- It uses the `WithMountedSecret()` method to return a container with the secret mounted as a file at the given location.\n- The GitHub CLI reads this file as needed to perform requested operations. For example, executing the `gh auth status` command in the Dagger pipeline after mounting the secret returns a message indicating that the user is logged-in, testifying to the success of the secret mount.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/host-fs/index.mjs\n```\n\nThis code listing assumes the existence of a GitHub CLI configuration file containing an authentication token at `/home/USER/.config/gh/hosts.yml`. It performs the following operations:\n\n- It reads the host file's contents into a string and loads the string into a secret with the Dagger client's `setSecret()` method. This method returns a new `Secret` object.\n- It uses the `withMountedSecret()` method to return a container with the secret mounted as a file at the given location.\n- The GitHub CLI reads this file as needed to perform requested operations. For example, executing the `gh auth status` command in the Dagger pipeline after mounting the secret returns a message indicating that the user is logged-in, testifying to the success of the secret mount.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/host-fs/main.py\n```\n\nThis code listing assumes the existence of a GitHub CLI configuration file containing an authentication token at `/home/USER/.config/gh/hosts.yml`. It performs the following operations:\n\n- It reads the host file's contents into a string and loads the string into a secret with the Dagger client's `set_secret()` method. This method returns a new `Secret` object.\n- It uses the `with_mounted_secret()` method to return a container with the secret mounted as a file at the given location.\n- The GitHub CLI reads this file as needed to perform requested operations. For example, executing the `gh auth status` command in the Dagger pipeline after mounting the secret returns a message indicating that the user is logged-in, testifying to the success of the secret mount.\n\n</TabItem>\n</Tabs>\n\n## Use secrets from an external secret manager\n\nIt's also possible to read secrets into Dagger from external secret managers. The following code listing provides an example of using a secret from Google Cloud Secret Manager in a Dagger pipeline.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/external/main.go\n```\n\nThis code listing requires the user to replace the `PROJECT-ID` and `SECRET-ID` placeholders with  corresponding Google Cloud project and secret identifiers. It performs the following operations:\n\n- It imports the Dagger and Google Cloud Secret Manager client libraries.\n- It uses the Google Cloud Secret Manager client to access and read the specified secret's payload.\n- It creates a Dagger secret with that payload using the `SetSecret()` method.\n- It uses the `WithSecretVariable()` method to return a container with the secret value assigned to an environment variable in the container.\n- It uses the secret to make an authenticated request to the GitHub API, as explained previously.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/external/index.mjs\n```\n\nThis code listing requires the user to replace the `PROJECT-ID` and `SECRET-ID` placeholders with  corresponding Google Cloud project and secret identifiers. It performs the following operations:\n\n- It imports the Dagger and Google Cloud Secret Manager client libraries.\n- It uses the Google Cloud Secret Manager client to access and read the specified secret's payload.\n- It creates a Dagger secret with that payload using the `setSecret()` method.\n- It uses the `withSecretVariable()` method to return a container with the secret value assigned to an environment variable in the container.\n- It uses the secret to make an authenticated request to the GitHub API, as explained previously.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/external/main.py\n```\n\nThis code listing requires the user to replace the `PROJECT-ID` and `SECRET-ID` placeholders with  corresponding Google Cloud project and secret identifiers. It performs the following operations:\n\n- It imports the Dagger and Google Cloud Secret Manager client libraries.\n- It uses the Google Cloud Secret Manager client to access and read the specified secret's payload.\n- It creates a Dagger secret with that payload using the `set_secret()` method.\n- It uses the `with_secret_variable()` method to return a container with the secret value assigned to an environment variable in the container.\n- It uses the secret to make an authenticated request to the GitHub API, as explained previously.\n\n</TabItem>\n</Tabs>\n\n## Use secrets with Dagger SDK methods\n\nSecrets can also be used natively as inputs to some Dagger SDK methods. Here's an example, which demonstrates logging in to Docker Hub from a Dagger pipeline and publishing a new image. To use this listing, replace the `DOCKER-HUB-USERNAME` and `DOCKER-HUB-PASSWORD` placeholders with your Docker Hub username and password respectively.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/sdk/main.go\n```\n\nIn this code listing:\n\n- The client's `SetSecret()` method returns a new `Secret` representing the Docker Hub password.\n- The `WithRegistryAuth()` method accepts three parameters - the Docker Hub registry address, the username and the password (as a `Secret`) - and returns a container pre-authenticated for the  registry.\n- The `Publish()` method publishes the container to Docker Hub.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/sdk/index.mjs\n```\n\nIn this code listing:\n\n- The client's `setSecret()` method returns a new `Secret` representing the Docker Hub password.\n- The `withRegistryAuth()` method accepts three parameters - the Docker Hub registry address, the username and the password (as a `Secret`) - and returns a container pre-authenticated for the  registry.\n- The `publish()` method publishes the container to Docker Hub.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/sdk/main.py\n```\n\nIn this code listing:\n\n- The client's `set_secret()` method returns a new `Secret` representing the Docker Hub password.\n- The `with_registry_auth()` method accepts three parameters - the Docker Hub registry address, the username and the password (as a `Secret`) - and returns a container pre-authenticated for the  registry.\n- The `publish()` method publishes the container to Docker Hub.\n\n</TabItem>\n</Tabs>\n\n## Use secrets with Dockerfile builds\n\nSecrets can also be passed to Dockerfile builds performed with Dagger. Build secrets set with Dagger are automatically mounted in the build container at the default Dockerfile location of `/run/secrets/SECRET-ID`.\n\nHere's an example, which demonstrates setting a build secret in a Dagger pipeline and using that secret in a Dockerfile build:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/dockerfile/main.go\n```\n\nThis code listing expects a host environment variable named `GH_SECRET` containing the secret value. It performs the following operations:\n\n- It reads the value of the host environment variable using the `os.Getenv()` function.\n- It creates a Dagger secret named `gh-secret` with that value using the `SetSecret()` function.\n- It uses the `DockerBuild()` function to build an image from a Dockerfile. The secret is automatically mounted in the build container at `/run/secrets/gh-secret`.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/dockerfile/index.mjs\n```\n\nThis code listing expects a host environment variable named `GH_SECRET` containing the secret value. It performs the following operations:\n\n- It reads the value of the host environment variable using the `process.env` object.\n- It creates a Dagger secret named `gh-secret` with that value using the `setSecret()` function.\n- It uses the `dockerBuild()` function to build an image from a Dockerfile. The secret is automatically mounted in the build container at `/run/secrets/gh-secret`.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/dockerfile/main.py\n```\n\nThis code listing expects a host environment variable named `GH_SECRET` containing the secret value. It performs the following operations:\n\n- It reads the value of the host environment variable using the `os.environ` object.\n- It creates a Dagger secret named `gh-secret` with that value using the `set_secret()` function.\n- It uses the `docker_build()` function to build an image from a Dockerfile. The secret is automatically mounted in the build container at `/run/secrets/gh-secret`.\n\n</TabItem>\n</Tabs>\n\nThe sample Dockerfile below demonstrates the process of mounting the secret using a [`secret` filesystem mount type](https://docs.docker.com/engine/reference/builder/#run---mounttypesecret) and using it in the Dockerfile build process:\n\n```dockerfile file=./snippets/use-secrets/dockerfile/Dockerfile\n```\n\n## Understand how Dagger secures secrets\n\nDagger automatically scrubs secrets from its various logs and output streams. This ensures that sensitive data does not leak - for example, in the event of a crash. This applies to secrets stored in both environment variables and file mounts.\n\nThe following example demonstrates this feature:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/use-secrets/security/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/use-secrets/security/index.mjs\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/use-secrets/security/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThis listing creates dummy secrets on the host (as an environment variable and a file), loads them into Dagger and then attempts to print them to the console. However, Dagger automatically scrubs the sensitive data before printing it, as shown in the output below:\n\n```shell\nsecret env data: *** || secret file data:\n***\n```\n\n:::danger\nAny secret that is to be read from the container environment should always be loaded using `withSecretVariable()`. If `withEnvVariable()` is used instead, the value of the environment variable may leak via the build history of the container. Using `withSecretVariable()` guarantees that the secret will not leak in the container build history or image layers.\n:::\n\n## Conclusion\n\nThis tutorial walked you through the basics of using secrets in Dagger. It explained the various API methods available to work with secrets and provided examples of using secrets as environment variables, file mounts and native objects.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Use Secrets in Dagger","excerpt":"Introduction","timestamp":1679961600000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/899944-aws-cdk-ecs.mdx","frontMatter":{"slug":"/899944/aws-cdk-ecs","displayed_sidebar":"current","category":"guides","tags":["go","aws","cdk","ecs","fargate"],"authors":["Sam Alba"],"date":"2023-03-16"},"content":"# Use Dagger with the AWS Cloud Development Kit (CDK)\n\n:::note\nView a [demo of using Dagger with the AWS CDK](https://youtu.be/ESZKu8VWSGA).\n:::\n\n## Introduction\n\nThe [AWS Cloud Development Kit (CDK)](https://docs.aws.amazon.com/cdk/v2/guide/home.html) is a framework that enables developers to use their programming language of choice to describe infrastructure resources on AWS.\n\nAlthough the CDK provides several helpers to facilitate building applications, this tutorial demonstrates how to delegate all the CI tasks (building the application, running tests, etc.) to a Dagger pipeline that integrates with the CDK to manage the infrastructure resources.\n\nYou will learn how to:\n\n- Configure the AWS CDK\n- Provision a container image repository on [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/)\n- Provision a cluster on [Amazon Elastic Container Service (ECS)](https://aws.amazon.com/ecs/) using [AWS Fargate](https://aws.amazon.com/fargate/)\n- Build, test and deploy an application to the AWS ECS cluster\n- Run all of the above through a Dagger pipeline\n\n:::tip\nThe concepts demonstrated in this tutorial can be applied to any other Infrastructure as Code (IaC) tool. The code example shown below can also be reused to provision another infrastructure stack (such as Amazon EKS, AWS Lambda and others). Reusing the code example for your own needs is covered in Appendix A.\n:::\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a Go development environment with Go 1.20 or later. If not, install [Go](https://go.dev/doc/install).\n- You have the Dagger Go SDK installed. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have a basic understanding of [AWS CloudFormation](https://aws.amazon.com/cloudformation/getting-started/).\n- You have access to a [AWS IAM user with permissions to create new resources on an AWS region](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console).\n- Your [AWS IAM user has access keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey) and those keys are [configured to be used from your local host](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html).\n- You have [installed the AWS CDK CLI](https://docs.aws.amazon.com/cdk/v2/guide/cli.html) on your local host (only required for bootstrapping the CDK for a specific region).\n\n## Step 1: Bootstrap the AWS CDK\n\nThe AWS CDK stores its state in an AWS CloudFormation stack named `CDKToolkit`. This stack needs to be present on every AWS region where you are managing resources using the AWS CDK.\n\nIn order to bootstrap the AWS CDK for a region, simply run the following command from a terminal. replace the `AWS-ACCOUNT-NUMBER` and `AWS-REGION` placeholders with the corresponding AWS account details.\n\n```shell\ncdk bootstrap AWS-ACCOUNT-NUMBER/AWS-REGION\n```\n\nMore information regarding this step is available in the [AWS CDK CLI documentation](https://docs.aws.amazon.com/cdk/v2/guide/cli.html#cli-bootstrap).\n\n## Step 2: Create the Dagger pipeline\n\nThe example application used in this tutorial is a simple React application. Once the AWS CDK is bootstrapped, the next step is to create a Dagger pipeline to build, publish and deploy this example application.\n\nObtain the Dagger pipeline code and its related helper functions from GitHub, as below:\n\n```shell\ngit clone https://github.com/dagger/examples.git\ncd ./go/aws-cdk\n```\n\nThis code is organized as follows:\n\n- `main.go`: This file contains the Dagger pipeline that builds the application, builds the container image of the application, publishes it and calls the AWS CDK to interface with the AWS infrastructure.\n- `aws.go`: This file contains helper functions for use with the AWS CDK CLI and the AWS API.\n- `registry.go`: This file contains helper functions to initialize the AWS ECR registry.\n- `infra/`: This subdirectory contains all the code related to the AWS CDK stacks. It is a standalone AWS CDK project that can be used directly from the AWS CDK CLI. It describes two AWS CDK stacks: one for the AWS ECR registry and one for the AWS ECS/Fargate cluster.\n\nThis `main.go` file contains three functions:\n\n- The `main()` function creates a Dagger client and an AWS client, initializes an AWS ECR container registry and invokes the `build()` and `deployToEcs()` functions in sequence.\n- The `build()` function obtains the application source code, runs the tests, builds a container image of the application and publishes the image to the AWS ECR registry.\n- The `deployToEcs()` function deploys the built container image to the AWS ECS cluster.\n\n```go file=./snippets/aws-cdk-ecs/main.go\n```\n\nThe `build()` function is the main workhorse here, so let's step through it in detail:\n\n- It uses the Dagger client's `CacheVolume()` method to initialize a new cache volume.\n- It uses the client's `Git()` method to query the Git repository for the example application. This method returns a `GitRepository` object.\n- It uses the `GitRepository` object's `Commit()` method to obtain a reference to the repository tree at a specific commit and then uses the resulting `GitRef` object's `Tree()` method to retrieve the filesystem tree and source code directory root.\n- It uses the client's `Container().From()` method to initialize a new container from a Node.js base image. The `From()` method returns a new `Container` object with the result.\n- It uses the `Container.WithDirectory()` method to write the source code directory on the host to the `/src` mount point in the container, and the `Container.WithMountedCache()` method to mount the cache volume at the `/src/node_modules/` mount point in the container.\n- It uses the `Container.WithWorkdir()` method to set the working directory to the `/src` mount point.\n- It uses the `Container.WithExec()` method to define the `npm install` command. When executed, this command downloads and installs dependencies in the `node_modules/` directory. Since this directory is defined as a cache volume, its contents will persist even after the pipeline terminates and can be reused on the next pipeline run.\n- It chains additional `WithExec()` method calls to run tests and build the application. The build result is stored in the `./build` directory in the container and a reference to this directory is saved in the `buildDir` variable.\n- It uses the `Container.WithDirectory()` method to initialize a new `nginx` container and transfer the filesystem state saved in the `buildDir` variable (the built application) to the container at the path `/usr/share/nginx/html`. The result is a container image with the built application in the NGINX webserver root directory.\n- It then uses the `WithRegistryAuth()` and `Publish()` methods to publish the final container image to AWS ECR.\n\n## Step 3: Test the Dagger pipeline locally\n\nTo build and run the Dagger pipeline from your local host, execute the following commands in a shell, from the `go/aws-cdk` directory. Replace the `AWS-REGION` placeholder with the AWS region you want to use to deploy the ECS cluster. This should be the same region where the CDK was previously bootstrapped (Step 1).\n\n```shell\ngo build -o pipeline\nAWS_REGION=\"AWS-REGION\" ./pipeline\n```\n\nThe first time the pipeline runs, it takes several minutes to complete because the AWS resources (AWS ECR, AWS VPC, AWS ECS...) need to be fully provisioned.\n\nHowever, if you re-run it, it completes almost instantly. This is due to the Dagger cache, which knows which step in the pipeline needs to be executed according to what changed from the previous run.\n\nOnce the pipeline completes, it displays an HTTP URL. Browse to this URL in your web browser to see the example application running on the newly provisioned AWS ECS cluster.\n\n## Conclusion\n\nThis tutorial walked you through the process of integrating the AWS CDK into a Dagger pipeline and building, publishing and deploying an application on AWS infrastructure using Dagger.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go SDK Reference](https://pkg.go.dev/dagger.io/dagger) to learn more about Dagger.\n\n## Appendix A: Repurposing this example for your own needs\n\nThe example in this tutorial implements a Dagger pipeline that builds, tests and deploys a simple application on specific infrastructure. It's likely that it will not correspond exactly to the infrastructure or the pipeline steps you need. This section explains how to reuse and adapt the example code to your own needs.\n\n### Replace AWS CDK stacks with other IaC tools\n\nThe `infra/` directory is a complete AWS CDK project bootstrapped with the AWS CDK CLI. You can start again from an empty `infra/` directory and run:\n\n```shell\ncdk init app --language go\n```\n\nAt this point, you can specify another programming language supported by the AWS CDK.\n\n:::tip\nGiven that the AWS CDK stack is deployed from a container via the Dagger pipeline, the language used for the AWS CDK project need not be the same as the language used for the Dagger pipeline. This means that you can - for example - deploy an AWS CDK stack implemented in Java from a Dagger pipeline written in Python.\n:::\n\nThe same code structure can also be reused to integrate tools like Terraform or Pulumi. Terraform, Pulumi and the AWS CDK share some common structures: a project, a stack, inputs (or parameters) and outputs (among several other concepts that were left out for simplicity). They also provide a CLI to interact with the infrastructure.\n\nAs a result, it is quite simple to swap out the AWS CDK CLI with one of the others mentioned above while interfacing with the Dagger pipeline in a similar way (passing inputs to the IaC tool and using outputs from the infrastructure in another pipeline step).\n\n### Reuse AWS CDK helper functions\n\nThe code in `aws.go` implements helpers to call the AWS CDK CLI and read stack outputs. These helpers can be reused \"as is\" in another project using the AWS CDK.","contentTitle":"Use Dagger with the AWS Cloud Development Kit (CDK)","excerpt":"View a demo of using Dagger with the AWS CDK.","timestamp":1678924800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/759201-gitlab-google-cloud.mdx","frontMatter":{"slug":"/759201/gitlab-google-cloud","displayed_sidebar":"current","category":"guides","tags":["go","python","nodejs","gitlab-ci","google-cloud"],"authors":["Vikram Vaswani"],"date":"2023-02-11"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nimport PartialGoogleCloudServiceAcountKeySetup from '../partials/_google-cloud-service-account-key-setup.mdx';\nimport PartialGoogleCloudApiSetup from '../partials/_google-cloud-api-run-setup.mdx';\nimport PartialGoogleCloudSdkCredentialsSetup from '../partials/_google-cloud-sdk-credentials-setup.mdx';\n\n# Use Dagger with GitLab CI/CD and Google Cloud\n\n## Introduction\n\nThis tutorial teaches you how to use a Dagger pipeline to continuously build and deploy a Go application with GitLab on Google Cloud Run. You will learn how to:\n\n- Configure a Google Cloud service account and assign it the correct roles\n- Create a Google Cloud Run service accessible at a public URL\n- Create a Dagger pipeline using the Dagger SDKs\n- Run the Dagger pipeline on your local host to manually build and deploy the application on Google Cloud Run\n- Use the same Dagger pipeline with GitLab CI/CD to automatically build and deploy the application on Google Cloud Run on every repository commit\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a basic understanding of the Go programming language.\n- You have a basic understanding of GitLab and GitLab CI/CD. If not, [learn about GitLab CI/CD](https://docs.gitlab.com/ee/ci/).\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Google Cloud CLI installed. If not, [install the Google Cloud CLI](https://cloud.google.com/sdk/docs/install).\n- You have a Google Cloud account and a Google Cloud project with billing enabled. If not, [register for a Google Cloud account](https://cloud.google.com/), [create a Google Cloud project](https://console.cloud.google.com/project) and [enable billing](https://support.google.com/cloud/answer/6293499#enable-billing).\n- You have a GitLab account and a GitLab repository containing a Go web application. This repository should also be cloned locally in your development environment. If not, [register for a GitLab account](https://gitlab.com/), [install the GitLab CLI](https://gitlab.com/gitlab-org/cli#installation) and follow the steps in Appendix A to [create and populate a local and GitLab repository with an example Go application](#appendix-a-create-a-gitlab-repository-with-an-example-go-application).\n- You have a GitLab Runner application available to run your GitLab CI/CD pipeline. This could be either a self-hosted runner or a GitLab-managed runner. [Learn about GitLab Runner](https://docs.gitlab.com/runner/) and follow the steps in Appendix B to [configure a self-hosted runner](#appendix-b-configure-a-self-hosted-gitlab-runner-for-use-with-dagger).\n\n## Step 1: Create a Google Cloud service account\n\n<PartialGoogleCloudServiceAcountKeySetup />\n\n## Step 2: Configure Google Cloud APIs and a Google Cloud Run service\n\n<PartialGoogleCloudApiSetup />\n\n## Step 3: Create the Dagger pipeline\n\nThe next step is to create a Dagger pipeline to do the heavy lifting: build a container image of the application, release it to Google Container Registry and deploy it on Google Cloud Run.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. In the application directory, install the Dagger SDK and the Google Cloud Run client library:\n\n  ```shell\n  go get dagger.io/dagger@latest\n  go get cloud.google.com/go/run/apiv2\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.go` and add the following code to it. Replace the PROJECT placeholder with your Google Cloud project identifier and adjust the region (`us-central1`) and service name (`myapp`) if you specified different values when creating the Google Cloud Run service in Step 2.\n\n  ```go file=./snippets/gitlab-google-cloud/main.go\n  ```\n\n  This code listing performs the following operations:\n    - It imports the Dagger and Google Cloud Run client libraries.\n    - It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `Host().Directory()` method to obtain a reference to the current directory on the host, excluding the `ci` directory. This reference is stored in the `source` variable.\n    - In the first stage of the build, it uses the client's `Container().From()` method to initialize a new container from a base image. The additional `Platform` argument to the `Container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `golang:1.20` image and the architecture is `linux/amd64`, which is one of the architectures supported by Google Cloud. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `WithDirectory()` method to return the container image with the host directory written at the `/src` path, and the `WithWorkdir()` method to set the working directory in the container.\n    - It chains the `WithEnvVariable()` method to set the `CGO_ENABLED` variable in the container environment and the `WithExec()` method to compile the Go application with `go build`.\n    - Once the application is built, it moves to the second stage of the build. It again uses the client's `Container().From()` method to initialize a new container from an `alpine` base image.\n    - It uses the previous `Container` object's `WithFile()` method to transfer the compiled binary file from the first stage to the new container filesystem.\n    - It sets the container entrypoint to the binary file using the `WithEntrypoint()` method.\n    - It uses the container object's `Publish()` method to publish the container to Google Container Registry, and prints the SHA identifier of the published image.\n    - It creates a Google Cloud Run client, creates a service request instructing the Google Cloud Run service to use the newly-published container image, and sends the requests to the Google Cloud Run API.\n\n1. Run the following command to update `go.sum`:\n\n  ```shell\n  go mod tidy\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. In the application directory, install the Dagger SDK and the Google Cloud Run client library:\n\n  ```shell\n  npm install @dagger.io/dagger@latest @google-cloud/run --save-dev\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `index.mjs` and add the following code to it. Replace the PROJECT placeholder with your Google Cloud project identifier and adjust the region (`us-central1`) and service name (`myapp`) if you specified different values when creating the Google Cloud Run service in Step 2.\n\n  ```javascript file=./snippets/gitlab-google-cloud/index.mjs\n  ```\n\n  This code listing performs the following operations:\n    - It imports the Dagger and Google Cloud Run client libraries.\n    - It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `ci` directory. This reference is stored in the `source` variable.\n    - In the first stage of the build, it uses the client's `container().from()` method to initialize a new container from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `golang:1.20` image and the architecture is `linux/amd64`, which is one of the architectures supported by Google Cloud. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `withDirectory()` method to return the container image with the host directory written at the `/src` path, and the `withWorkdir()` method to set the working directory in the container.\n    - It chains the `withEnvVariable()` method to set the `CGO_ENABLED` variable in the container environment and the `withExec()` method to compile the Go application with `go build`.\n    - Once the application is built, it moves to the second stage of the build. It again uses the client's `container().from()` method to initialize a new container from an `alpine` base image.\n    - It uses the previous `Container` object's `withFile()` method to transfer the compiled binary file from the first stage to the new container filesystem.\n    - It sets the container entrypoint to the binary file using the `withEntrypoint()` method.\n    - It uses the container object's `publish()` method to publish the container to Google Container Registry, and prints the SHA identifier of the published image.\n    - It creates a Google Cloud Run client, creates a service request instructing the Google Cloud Run service to use the newly-published container image, and sends the requests to the Google Cloud Run API.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. In the application directory, create a virtual environment and install the Dagger SDK and the Google Cloud Run client library:\n\n  ```shell\n  pip install dagger-io google-cloud-run\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.py` and add the following code to it. Replace the PROJECT placeholder with your Google Cloud project identifier and adjust the region (`us-central1`) and service name (`myapp`) if you specified different values when creating the Google Cloud Run service in Step 2.\n\n  ```python file=./snippets/gitlab-google-cloud/main.py\n  ```\n\n  This code listing performs the following operations:\n    - It imports the Dagger and Google Cloud Run client libraries.\n    - It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `ci` directory. This reference is stored in the `source` variable.\n    - In the first stage of the build, it uses the client's `container().from_()` method to initialize a new container from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `golang:1.20` image and the architecture is `linux/amd64`, which is one of the architectures supported by Google Cloud. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `with_directory()` method to mount the host directory into the container at the `/src` mount point, and the `with_workdir()` method to set the working directory in the container.\n    - It chains the `with_env_variable()` method to set the `CGO_ENABLED` variable in the container environment and the `with_exec()` method to compile the Go application with `go build`.\n    - Once the application is built, it moves to the second stage of the build. It again uses the client's `container().from_()` method to initialize a new container from an `alpine` base image.\n    - It uses the previous `Container` object's `with_file()` method to transfer the compiled binary file from the first stage to the new container filesystem.\n    - It sets the container entrypoint to the binary file using the `with_entrypoint()` method.\n    - It uses the container object's `publish()` method to publish the container to Google Container Registry, and prints the SHA identifier of the published image.\n    - It creates a Google Cloud Run client, creates a service request instructing the Google Cloud Run service to use the newly-published container image, and sends the requests to the Google Cloud Run API.\n\n</TabItem>\n</Tabs>\n\n:::tip\nMost `Container` object methods return a revised `Container` object representing the new state of the container. This makes it easy to chain methods together. Dagger evaluates pipelines \"lazily\", so the chained operations are only executed when required - in this case, when the container is published. Learn more about [lazy evaluation in Dagger](../api/975146-concepts.mdx#lazy-evaluation).\n:::\n\n## Step 4: Test the Dagger pipeline on the local host\n\nConfigure credentials for the Google Cloud SDK on the local host, as follows:\n\n<PartialGoogleCloudSdkCredentialsSetup />\n\nOnce credentials are configured, test the Dagger pipeline by running the command below:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\ndagger run go run ci/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```shell\ndagger run node ci/index.mjs\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\ndagger run python ci/main.py\n```\n\n</TabItem>\n</Tabs>\n\nDagger performs the operations defined in the pipeline script, logging each operation to the console. At the end of the process, the built container is deployed to Google Cloud Run and a message similar to the one below appears in the console output:\n\n  ```shell\n  Deployment for image gcr.io/PROJECT/myapp@sha256:b1cf... now available at https://...run.app\n  ```\n\nBrowse to the URL shown in the deployment message to see the running application.\n\nIf you deployed the example application from [Appendix A](#appendix-a-create-a-gitlab-repository-with-an-example-go-application), you see the output below:\n\n```shell\nHello, Dagger!\n```\n\n## Step 5: Create a GitLab CI/CD pipeline\n\nDagger executes your pipelines entirely as standard OCI containers. This means that the same pipeline will run the same, whether on on your local machine or a remote server.\n\nThis also means that it's very easy to move your Dagger pipeline from your local host to GitLab - all that's needed is to transfer the pipeline script from your local clone to your GitLab repository, and then define a GitLab CI/CD pipeline to run it on every commit.\n\n1. Create a new GitLab CI/CD pipeline configuration file in your application directory at `.gitlab-ci.yml` with the following content:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```yaml file=./snippets/gitlab-google-cloud/gitlab-ci-go.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```yaml file=./snippets/gitlab-google-cloud/gitlab-ci-nodejs.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```yaml file=./snippets/gitlab-google-cloud/gitlab-ci-python.yml\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This GitLab CI/CD pipeline runs on every commit to the repository `master` branch. It consists of three jobs, as below:\n    - The first job tells the GitLab runner to use the Docker executor with a Docker-in-Docker (`dind`) service. It also configures TLS and sets the location for Docker to generate its TLS certificates.\n    - The second job adds the Docker CLI and authenticates to Google Container Registry from the GitLab runner. This is necessary because Dagger relies on the host's Docker credentials and authorizations when publishing to remote registries. For authentication, the job relies on the Google Cloud service account credentials, which are stored in the `GOOGLE_APPLICATION_CREDENTIALS` variable (more on this later).\n    - The third and final job executes the Dagger pipeline code.\n\n1. This GitLab CI/CD pipeline looks for a Google Cloud service account key in the `GOOGLE_APPLICATION_CREDENTIALS` GitLab variable. Create this variable as follows:\n\n    1. Navigate to the `Settings` -> `CI/CD` -> `Variables` page in the GitLab Web interface.\n    1. Click `Add variable` to create a new variable.\n    1. Configure the variable with the following inputs:\n        - Name: `GOOGLE_APPLICATION_CREDENTIALS`\n        - Value: The contents of the service account JSON key file downloaded in Step 1\n        - Type: `File`\n        - Flags: `Protect variable`\n    1. Click `Add variable` to save the variable.\n\n    ![Create GitLab variable](/img/current_docs/guides/gitlab-google-cloud/create-gitlab-variable.png)\n\n1. Commit and push the changes to the GitLab repository:\n\n  ```shell\n  git add .\n  git commit -a -m \"Added pipeline and CI code\"\n  git push\n  ```\n\n## Step 6: Test the Dagger pipeline on GitLab\n\n:::info\nThis step requires a properly-configured GitLab Runner. Refer to Appendix B for instructions on how to [configure a self-hosted GitLab Runner for use with Dagger](#appendix-b-configure-a-self-hosted-gitlab-runner-for-use-with-dagger).\n:::\n\nTest the Dagger pipeline by committing a change to the GitLab repository.\n\nIf you are using the example application described in [Appendix A](#appendix-a-create-a-gitlab-repository-with-an-example-go-application), the following commands modify and commit a simple change to the application's index page:\n\n```shell\ngit pull\nsed -i -e 's/Dagger/Dagger on GitLab/g' server.go\ngit commit -a -m \"Update welcome message\"\ngit push\n```\n\nThe commit triggers the GitLab CI/CD pipeline defined in Step 6. The pipeline runs the various jobs, including the Dagger pipeline.\n\nAt the end of the process, a new version of the built container image is released to Google Container Registry and deployed on Google Cloud Run. A message similar to the one below appears in the GitHub Actions log:\n\n```shell\nDeployment for image gcr.io/PROJECT/myapp@sha256:h4si... now available at https://...run.app\n```\n\nBrowse to the URL shown in the deployment message to see the running application. If you deployed the example application with the additional modification above, you see the following output:\n\n```shell\nHello, Dagger on GitLab!\n```\n\n## Conclusion\n\nThis tutorial walked you through the process of creating a Dagger pipeline to continuously build and deploy a Go application on Google Cloud Run. It explained key concepts, objects and methods available in the Dagger SDKs to construct a Dagger pipeline.\n\nDagger executes your pipelines entirely as standard OCI containers. This means that pipelines can be tested and debugged locally, and that the same pipeline will run consistently on your local machine, a CI runner, a dedicated server, or any container hosting service. This portability is one of Dagger's key advantages, and this tutorial demonstrated it in action by using the same pipeline on the local host and on GitLab.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.\n\n## Appendix A: Create a GitLab repository with an example Go application\n\nThis tutorial assumes that you have a GitLab repository with a application. If not, follow the steps below to create a GitLab repository and commit a simple Go web application to it.\n\n1. Log in to GitLab using the GitLab CLI:\n\n  ```shell\n  glab auth login -h gitlab.com\n  ```\n\n1. Create a directory and module for the Go application:\n\n  ```shell\n  mkdir myapp\n  cd myapp\n  go mod init main\n  ```\n\n1. Install the Echo web framework:\n\n  ```shell\n  go get github.com/labstack/echo/v4\n  ```\n\n1. Create a file named `server.go` and add the following code to it to create a skeleton application:\n\n  ```go\n  package main\n\n  import (\n    \"net/http\"\n\n    \"github.com/labstack/echo/v4\"\n  )\n\n  func main() {\n    e := echo.New()\n    e.GET(\"/\", func(c echo.Context) error {\n      return c.String(http.StatusOK, \"Hello, Dagger!\")\n    })\n    e.Logger.Fatal(e.Start(\":1323\"))\n  }\n  ```\n\n1. Create a private repository in your GitLab account:\n\n  ```shell\n  glab repo create myapp\n  ```\n\n1. Commit and push the application code:\n\n  ```shell\n  git add .\n  git commit -a -m \"Initial commit\"\n  git push --set-upstream origin master\n  ```\n\n## Appendix B: Configure a self-hosted GitLab Runner for use with Dagger\n\nThis tutorial assumes that you have a GitLab Runner application to run your GitLab CI/CD pipelines. This could be either a GitLab-managed runner or a self-hosted runner. [Learn about GitLab Runner](https://docs.gitlab.com/runner/).\n\nTo use GitLab's managed runners, you must [associate a valid credit card with your GitLab account](https://about.gitlab.com/pricing/#why-do-i-need-to-enter-credit-debit-card-details-for-free-pipeline-minutes). Alternatively, you can configure a self-hosted runner on your local host by following the steps below.\n\n1. [Install GitLab Runner](https://docs.gitlab.com/runner/install/index.html) for your host's operating system.\n1. Navigate to the `Settings` -> `CI/CD` -> `Runners` page in the GitLab Web interface.\n1. Disable shared runners by unchecking the `Enable shared runners for this project` option.\n\n  ![Disable shared runners](/img/current_docs/guides/gitlab-google-cloud/gitlab-disable-shared-runners.png)\n\n1. Copy the project-specific registration token, as shown below:\n\n  ![Runner registration token](/img/current_docs/guides/gitlab-google-cloud/gitlab-self-hosted-runner-token.png)\n\n1. On your local host, register the runner using the command below. Replace the TOKEN placeholder with the registration token.\n\n  ```shell\n  sudo gitlab-runner register -n \\\n    --name dagger \\\n    --url https://gitlab.com/ \\\n    --executor docker \\\n    --docker-privileged \\\n    --docker-volumes /cache \\\n    --docker-volumes /certs/client \\\n    --docker-image docker:20.10.16 \\\n    --registration-token TOKEN\n  ```\n\n1. Navigate to the `Settings` -> `CI/CD` -> `Runners` page in the GitLab Web interface. Confirm that the newly-registered runner is active for the project, as shown below:\n\n  ![Runner registration](/img/current_docs/guides/gitlab-google-cloud/gitlab-self-hosted-runner-active.png)","contentTitle":"Use Dagger with GitLab CI/CD and Google Cloud","excerpt":"Introduction","timestamp":1676073600000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/205271-replace-dockerfile.mdx","frontMatter":{"slug":"/205271/replace-dockerfile","displayed_sidebar":"current","category":"guides","tags":["go","python","nodejs"],"authors":["Kyle Penfound","Vikram Vaswani"],"date":"2023-01-07"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Replace a Dockerfile with Go (or Python, or Node.js)\n\n## Introduction\n\nThis guide explains how to use a Dagger SDK to perform all the same operations that you would typically perform with a Dockerfile, except using Go, Python or Node.js. You will learn how to:\n\n- Create a Dagger client\n- Write a Dagger pipeline to:\n  - Configure a container with all required dependencies and environment variables\n  - Download and build the application source code in the container\n  - Set the container entrypoint\n  - Publish the built container image to Docker Hub\n- Test the Dagger pipeline locally\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have a Docker Hub account. If not, [register for a Docker Hub account](https://hub.docker.com/signup).\n\n## Step 1: Understand the source Dockerfile\n\nTo illustrate the process, this guide replicates the build process for the popular open source [Memcached caching system](https://www.memcached.org/) using Dagger. It uses the Dockerfile and entrypoint script for the [official Docker Hub Memcached image](https://github.com/docker-library/memcached).\n\nBegin by reviewing the [source Dockerfile](https://github.com/docker-library/memcached/blob/1e3f84629bb2ab9975235401c716c1e00563fa82/alpine/Dockerfile) and corresponding [entrypoint script](https://github.com/docker-library/memcached/blob/1e3f84629bb2ab9975235401c716c1e00563fa82/alpine/docker-entrypoint.sh) to understand how it works. This Dockerfile is current at the time of writing and is available under the BSD 3-Clause License.\n\nBroadly, this Dockerfile performs the following steps:\n\n- It starts from a base `alpine` container image.\n- It adds a `memcache` user and group with defined IDs.\n- It sets environment variables for the Memcached version (`MEMCACHED_VERSION`) and commit hash (`MEMCACHED_SHA1`).\n- It installs dependencies in the container.\n- It downloads the source code archive for the specified version of Memcached, checks the commit hash and extracts the source code into a directory.\n- It configures, builds, tests and installs Memcached from source using `make`.\n- It copies and sets the container entrypoint script.\n- It configures the image to run as the `memcache` user.\n\n## Step 2: Replicate the Dockerfile using a Dagger pipeline\n\nThe Dagger SDK enables you to develop a CI/CD pipeline in one of the supported languages (Go, Python or Node.js) to achieve the same result as using a Dockerfile.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\nTo see how this works, add the following code to your Go module as `main.go`. Replace the DOCKER-HUB-USERNAME placeholder with your Docker Hub username.\n\n```go file=./snippets/replace-dockerfile/main.go\n```\n\nThere's a lot going on here, so let's step through it in detail:\n\n- The Go CI pipeline imports the Dagger SDK and defines a `main()` function. The `main()` function creates a Dagger client with `dagger.Connect()`. This client provides an interface for executing commands against the Dagger engine.\n- It initializes a new container from a base image with the client's `Container().From()` method and returns a new `Container` struct. In this case, the base image is the `alpine:3.17` image.\n- It calls the `WithExec()` method to define the `adduser`, `addgroup` and `apk add` commands for execution, and the `WithEnvVariable()` method to set the `MEMCACHED_VERSION` and `MEMCACHED_SHA1` container environment variables.\n- It calls a custom `setDependencies()` function, which internally uses `WithExec()` to define the `apk add` command that installs all the required dependencies to build and test Memcached in the container.\n- It calls a custom `downloadMemcached()` function, which internally uses `WithExec()` to define the `wget`, `tar` and related commands required to download, verify and extract the Memcached source code archive in the container at the `/usr/src/memcached` container path.\n- It calls a custom `buildMemcached()` function, which internally uses `WithExec()` to define the `configure` and `make` commands required to build, test and install Memcached in the container. The `buildMemcached()` function also takes care of deleting the source code directory at `/usr/src/memcached` in the container and executing `memcached -V` to output the version string to the console.\n- It updates the container filesystem to include the entrypoint script from the host using `WithFile()` and specifies it as the command to be executed when the container runs using `WithEntrypoint()`.\n- Finally, it calls the `Container.Publish()` method, which executes the entire pipeline described above and publishes the resulting container image to Docker Hub.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\nTo see how this works, create a file named `index.mjs` and add the following code to it. Replace the DOCKER-HUB-USERNAME placeholder with your Docker Hub username.\n\n```javascript file=./snippets/replace-dockerfile/index.mjs\n```\n\nThere's a lot going on here, so let's step through it in detail:\n\n- The Node.js CI pipeline imports the Dagger SDK and creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n- It initializes a new container from a base image with the client's `container().from()` method and returns a new `Container` object. In this case, the base image is the `alpine:3.17` image.\n- It calls the `withExec()` method to define the `adduser`, `addgroup` and `apk add` commands for execution, and the `withEnvVariable()` method to set the `MEMCACHED_VERSION` and `MEMCACHED_SHA1` container environment variables.\n- It calls a custom `setDependencies()` function, which internally uses `withExec()` to define the `apk add` command that installs all the required dependencies to build and test Memcached in the container.\n- It calls a custom `downloadMemcached()` function, which internally uses `withExec()` to define the `wget`, `tar` and related commands required to download, verify and extract the Memcached source code archive in the container at the `/usr/src/memcached` container path.\n- It calls a custom `buildMemcached()` function, which internally uses `withExec()` to define the `configure` and `make` commands required to build, test and install Memcached in the container. The `buildMemcached()` function also takes care of deleting the source code directory at `/usr/src/memcached` in the container and executing `memcached -V` to output the version string to the console.\n- It updates the container filesystem to include the entrypoint script from the host using `withFile()` and specifies it as the command to be executed when the container runs using `withEntrypoint()`. The `withDefaultArgs()` methods specifies the entrypoint arguments.\n- Finally, it calls the `Container.publish()` method, which executes the entire pipeline described above and publishes the resulting container image to Docker Hub.\n\n</TabItem>\n<TabItem value=\"Python\">\n\nTo see how this works, create a file named `main.py` and add the following code to it. Replace the DOCKER-HUB-USERNAME placeholder with your Docker Hub username.\n\n```python file=./snippets/replace-dockerfile/main.py\n```\n\nThere's a lot going on here, so let's step through it in detail:\n\n- The Python CI pipeline imports the Dagger SDK and defines a `main()` function. The `main()` function creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n- It initializes a new container from a base image with the client's `container().from_()` method and returns a new `Container`. In this case, the base image is the `alpine:3.17` image.\n- It calls the `with_exec()` method to define the `adduser`, `addgroup` and `apk add` commands for execution, and the `with_env_variable()` method to set the `MEMCACHED_VERSION` and `MEMCACHED_SHA1` container environment variables.\n- It calls a custom `set_dependencies()` function, which internally uses `with_exec()` to define the `apk add` command that installs all the required dependencies to build and test Memcached in the container.\n- It calls a custom `download_memcached()` function, which internally uses `with_exec()` to define the `wget`, `tar` and related commands required to download, verify and extract the Memcached source code archive in the container at the `/usr/src/memcached` container path.\n- It calls a custom `build_memcached()` function, which internally uses `with_exec()` to define the `configure` and `make` commands required to build, test and install Memcached in the container. The `build_memcached()` function also takes care of deleting the source code directory at `/usr/src/memcached` in the container and executing `memcached -V` to output the version string to the console.\n- It updates the container filesystem to include the entrypoint script from the host using `with_file()` and specifies it as the command to be executed when the container runs using `with_entrypoint()`. The `with_default_args()` methods specifies the entrypoint arguments.\n- Finally, it calls the `Container.publish()` method, which executes the entire pipeline described above and publishes the resulting container image to Docker Hub.\n\n</TabItem>\n</Tabs>\n\n:::warning\nLike the source Dockerfile, this pipeline assumes that the entrypoint script exists in the current  working directory on the host as `docker-entrypoint.sh`. You can either create a custom entrypoint script, or use the [entrypoint script from the Docker Hub Memcached image repository](https://github.com/docker-library/memcached/blob/1e3f84629bb2ab9975235401c716c1e00563fa82/alpine/docker-entrypoint.sh).\n:::\n\n## Step 3: Test the Dagger pipeline\n\nTest the Dagger pipeline as follows:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. Log in to Docker on the host:\n\n  ```shell\n  docker login\n  ```\n\n  :::info\n  This step is necessary because Dagger relies on the host's Docker credentials and authorizations when publishing to remote registries.\n  :::\n\n1. Run the pipeline:\n\n  ```shell\n  dagger run go run main.go\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. Log in to Docker on the host:\n\n  ```shell\n  docker login\n  ```\n\n  :::info\n  This step is necessary because Dagger relies on the host's Docker credentials and authorizations when publishing to remote registries.\n  :::\n\n1. Run the pipeline:\n\n  ```shell\n  dagger run node index.mjs\n  ```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. Log in to Docker on the host:\n\n  ```shell\n  docker login\n  ```\n\n  :::info\n  This step is necessary because Dagger relies on the host's Docker credentials and authorizations when publishing to remote registries.\n  :::\n\n1. Run the pipeline:\n\n  ```shell\n  dagger run python main.py\n  ```\n\n</TabItem>\n</Tabs>\n\n:::warning\nVerify that you have an entrypoint script on the host at `./docker-entrypoint.sh` before running the Dagger pipeline.\n:::\n\nThe `dagger run` command executes the script in a Dagger session and displays live progress. This process will take some time. At the end of the process, the built container image is published on Docker Hub and a message similar to the one below appears in the console output:\n\n```shell\nPublished to docker.io/.../my-memcached@sha256:692....\n```\n\nBrowse to your Docker Hub registry to see the published Memcached container image.\n\n## Conclusion\n\nThis tutorial introduced you to the Dagger SDKs. By replacing a Dockerfile with native code, it demonstrated how Dagger SDKs contain everything you need to develop CI/CD pipelines in your favorite language and run them on any OCI-compatible container runtime.\n\nThe advantage of this approach is that it allows you to use powerful native language features, such as (where applicable) static typing, concurrency, programming structures such as loops and conditionals, and built-in testing, to create powerful CI/CD tooling for your project or organization.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Replace a Dockerfile with Go (or Python, or Node.js)","excerpt":"Introduction","timestamp":1673049600000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/145912-ci.mdx","frontMatter":{"slug":"/145912/ci","displayed_sidebar":"current","category":"guides","tags":["python","go","nodejs","gitlab-ci","github-actions","circle-ci","jenkins"],"authors":["Jeremy Adams"],"date":"2022-12-13"},"content":"import Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n# Use Dagger SDKs in CI\n\n## Introduction\n\nThis guide explains how to integrate the Dagger SDKs with various CI services/tools.\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go, Python or Node.js environment on the CI runner.\n- You have a Dagger SDK for one of the above languages installed on the CI runner. If not, install your preferred language SDK within your CI workflow following the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n\n## Use Dagger in CI\n\n### GitHub Actions\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```yaml title=\".github/workflows/dagger.yml\" file=./snippets/ci/go/actions.yml\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```yaml title=\".github/workflows/dagger.yaml\" file=./snippets/ci/nodejs/actions.yml\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```yaml title=\".github/workflows/dagger.yaml\" file=./snippets/ci/python/actions.yml\n```\n\n</TabItem>\n</Tabs>\n\n### GitLab CI\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```yaml title=\".gitlab-ci.yml\" file=./snippets/ci/go/gitlab.yml\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```yaml title=\".gitlab-ci.yml\" file=./snippets/ci/nodejs/gitlab.yml\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```yaml title=\".gitlab-ci.yml\" file=./snippets/ci/python/gitlab.yml\n```\n\n</TabItem>\n</Tabs>\n\n### CircleCI\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```yaml title=\".circleci/config.yml\" file=./snippets/ci/go/circle.yml\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```yaml title=\".circleci/config.yml\" file=./snippets/ci/nodejs/circle.yml\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```yaml title=\".circleci/config.yml\" file=./snippets/ci/python/circle.yml\n```\n\n</TabItem>\n</Tabs>\n\n### Jenkins\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```groovy title=\"Jenkinsfile\" file=./snippets/ci/go/Jenkinsfile\n```\n\nRequires `docker` client and `go` installed on your Jenkins agent, a Docker host available (can be `docker:dind`), and agents labeled in Jenkins with `dagger`.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```groovy title=\"Jenkinsfile\" file=./snippets/ci/nodejs/Jenkinsfile\n```\n\nRequires `docker` client and `node` installed on your Jenkins agent, a Docker host available (can be `docker:dind`), and agents labeled in Jenkins with `dagger`.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```groovy title=\"Jenkinsfile\" file=./snippets/ci/python/Jenkinsfile\n```\n\nRequires `docker` client and `python` installed on your Jenkins agent, a Docker host available (can be `docker:dind`), and agents labeled in Jenkins with `dagger`.\n\n</TabItem>\n</Tabs>\n\n### Azure Pipelines\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```yaml title=\"azure-pipelines.yml\" file=./snippets/ci/go/azure-pipelines.yml\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```yaml title=\"azure-pipelines.yml\" file=./snippets/ci/nodejs/azure-pipelines.yml\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```yaml title=\"azure-pipelines.yml\" file=./snippets/ci/python/azure-pipelines.yml\n```\n\n</TabItem>\n</Tabs>\n\n### AWS CodePipeline\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```yaml title=\"buildspec.yml\" file=./snippets/ci/go/buildspec.yml\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```yaml title=\"buildspec.yml\" file=./snippets/ci/nodejs/buildspec.yml\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```yaml title=\"buildspec.yml\" file=./snippets/ci/python/buildspec.yml\n```\n\n</TabItem>\n</Tabs>","contentTitle":"Use Dagger SDKs in CI","excerpt":"Introduction","timestamp":1670889600000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/620941-github-google-cloud.mdx","frontMatter":{"slug":"/620941/github-google-cloud","displayed_sidebar":"current","category":"guides","tags":["nodejs","go","python","gitlab-ci","google-cloud"],"authors":["Vikram Vaswani"],"date":"2022-12-12"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nimport PartialGoogleCloudServiceAcountKeySetup from '../partials/_google-cloud-service-account-key-setup.mdx';\nimport PartialGoogleCloudApiSetup from '../partials/_google-cloud-api-run-setup.mdx';\nimport PartialGoogleCloudSdkCredentialsSetup from '../partials/_google-cloud-sdk-credentials-setup.mdx';\n\n# Use Dagger with GitHub Actions and Google Cloud\n\n:::note\n[Watch a live demo](https://youtu.be/-pKmv0VDJBg) of this tutorial in the Dagger Community Call (12 Jan 2023). For more demos, [join the next Dagger Community Call](https://dagger.io/events).\n:::\n\n## Introduction\n\nThis tutorial teaches you how to use a Dagger pipeline to continuously build and deploy a Node.js application with GitHub Actions on Google Cloud Run. You will learn how to:\n\n- Configure a Google Cloud service account and assign it the correct roles\n- Create a Google Cloud Run service accessible at a public URL\n- Create a Dagger pipeline using a Dagger SDK\n- Run the Dagger pipeline on your local host to manually build and deploy the application on Google Cloud Run\n- Use the same Dagger pipeline with GitHub Actions to automatically build and deploy the application on Google Cloud Run on every repository commit\n\n## Requirements\n\nThis tutorial assumes that:\n\n- You have a basic understanding of the JavaScript programming language.\n- You have a basic understanding of GitHub Actions. If not, [learn about GitHub Actions](https://docs.github.com/en/actions).\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have the Google Cloud CLI installed. If not, [install the Google Cloud CLI](https://cloud.google.com/sdk/docs/install).\n- You have a Google Cloud account and a Google Cloud project with billing enabled. If not, [register for a Google Cloud account](https://cloud.google.com/), [create a Google Cloud project](https://console.cloud.google.com/project) and [enable billing](https://support.google.com/cloud/answer/6293499#enable-billing).\n- You have a GitHub account and a GitHub repository containing a Node.js Web application. This repository should also be cloned locally in your development environment. If not, [register for a GitHub account](https://github.com/signup), [install the GitHub CLI](https://github.com/cli/cli#installation) and follow the steps in Appendix A to [create and populate a local and GitHub repository with an example Express application](#appendix-a-create-a-github-repository-with-an-example-express-application).\n\n## Step 1: Create a Google Cloud service account\n\n<PartialGoogleCloudServiceAcountKeySetup />\n\n## Step 2: Configure Google Cloud APIs and a Google Cloud Run service\n\n<PartialGoogleCloudApiSetup />\n\n## Step 3: Create the Dagger pipeline\n\nThe next step is to create a Dagger pipeline to do the heavy lifting: build a container image of the application, release it to Google Container Registry and deploy it on Google Cloud Run.\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n1. In the application directory, install the Dagger SDK and the Google Cloud Run client library as development dependencies:\n\n  ```shell\n  go get dagger.io/dagger@latest\n  go get cloud.google.com/go/run/apiv2\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.go` and add the following code to it. Replace the PROJECT placeholder with your Google Cloud project identifier and adjust the region (`us-central1`) and service name (`myapp`) if you specified different values when creating the Google Cloud Run service in Step 2.\n\n  ```go file=./snippets/github-google-cloud/main.go\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger and Google Cloud Run client libraries.\n    - It creates a Dagger client with `Connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `Host().Directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `Container().From()` method to initialize a new container from a base image. The additional `Platform` argument to the `Container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:16` image and the archiecture is `linux/amd64`, which is one of the architectures supported by Google Cloud. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `WithDirectory()` method to return the container image with the host directory written at the `/src` path, and the `WithWorkdir()` method to set the working directory in the container.\n    - It chains the `WithExec()` method to copy the contents of the working directory to the `/home/node` directory in the container and then uses the `WithWorkdir()` method to change the working directory in the container to `/home/node`.\n    - It chains the `WithExec()` method again to install dependencies with `npm install` and sets the container entrypoint using the `WithEntrypoint()` method.\n    - It uses the container object's `Publish()` method to publish the container to Google Container Registry, and prints the SHA identifier of the published image.\n    - It creates a Google Cloud Run client, updates the Google Cloud Run service defined in Step 2 to use the published container image, and requests a service update.\n\n1. Run the following command to update `go.sum`:\n\n  ```shell\n  go mod tidy\n  ```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n1. In the application directory, install the Dagger SDK and the Google Cloud Run client library as development dependencies:\n\n  ```shell\n  npm install @dagger.io/dagger@latest @google-cloud/run --save-dev\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `index.mjs` and add the following code to it. Replace the PROJECT placeholder with your Google Cloud project identifier and adjust the region (`us-central1`) and service name (`myapp`) if you specified different values when creating the Google Cloud Run service in Step 2.\n\n  ```javascript file=./snippets/github-google-cloud/index.mjs\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger and Google Cloud Run client libraries.\n    - It creates a Dagger client with `connect()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `host().workdir()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from()` method to initialize a new container from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:16` image and the archiecture is `linux/amd64`, which is one of the architectures supported by Google Cloud. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `withDirectory()` method to return the container image with the host directory written at the `/src` path, and the `withWorkdir()` method to set the working directory in the container.\n    - It chains the `withExec()` method to copy the contents of the working directory to the `/home/node` directory in the container and then uses the `withWorkdir()` method to change the working directory in the container to `/home/node`.\n    - It chains the `withExec()` method again to install dependencies with `npm install` and sets the container entrypoint using the `withEntrypoint()` method.\n    - It uses the container object's `publish()` method to publish the container to Google Container Registry, and prints the SHA identifier of the published image.\n    - It creates a Google Cloud Run client, updates the Google Cloud Run service defined in Step 2 to use the published container image, and requests a service update.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n1. In the application directory, create a virtual environment and install the Dagger SDK and the Google Cloud Run client library:\n\n  ```shell\n  pip install dagger-io google-cloud-run\n  ```\n\n1. Create a new sub-directory named `ci`. Within the `ci` directory, create a file named `main.py` and add the following code to it. Replace the PROJECT placeholder with your Google Cloud project identifier and adjust the region (`us-central1`) and service name (`myapp`) if you specified different values when creating the Google Cloud Run service in Step 2.\n\n  ```python file=./snippets/github-google-cloud/main.py\n  ```\n\n  This file performs the following operations:\n    - It imports the Dagger and Google Cloud Run client libraries.\n    - It creates a Dagger client with `dagger.Connection()`. This client provides an interface for executing commands against the Dagger engine.\n    - It uses the client's `host().directory()` method to obtain a reference to the current directory on the host, excluding the `node_modules` and `ci` directories. This reference is stored in the `source` variable.\n    - It uses the client's `container().from_()` method to initialize a new container from a base image. The additional `platform` argument to the `container()` method instructs Dagger to build for a specific architecture. In this example, the base image is the `node:16` image and the archiecture is `linux/amd64`, which is one of the architectures supported by Google Cloud. This method returns a `Container` representing an OCI-compatible container image.\n    - It uses the previous `Container` object's `with_directory()` method to mount the host directory into the container at the `/src` mount point, and the `with_workdir()` method to set the working directory in the container.\n    - It chains the `withExec()` method to copy the contents of the working directory to the `/home/node` directory in the container and then uses the `withWorkdir()` method to change the working directory in the container to `/home/node`.\n    - It chains the `with_exec()` method again to install dependencies with `npm install` and sets the container entrypoint using the `with_entrypoint()` method.\n    - It uses the container object's `publish()` method to publish the container to Google Container Registry, and prints the SHA identifier of the published image.\n    - It creates a Google Cloud Run client, updates the Google Cloud Run service defined in Step 2 to use the published container image, and requests a service update.\n\n</TabItem>\n</Tabs>\n\n:::tip\nMost `Container` object methods return a revised `Container` object representing the new state of the container. This makes it easy to chain methods together. Dagger evaluates pipelines \"lazily\", so the chained operations are only executed when required - in this case, when the container is published. Learn more about [lazy evaluation in Dagger](../api/975146-concepts.mdx#lazy-evaluation).\n:::\n\n## Step 4: Test the Dagger pipeline on the local host\n\nConfigure credentials for the Google Cloud SDK on the local host, as follows:\n\n<PartialGoogleCloudSdkCredentialsSetup />\n\nOnce credentials are configured, test the Dagger pipeline by running the command below:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\ndagger run go run ci/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```shell\ndagger run node ci/index.mjs\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\ndagger run python ci/main.py\n```\n\n</TabItem>\n</Tabs>\n\nDagger performs the operations defined in the pipeline script, logging each operation to the console. At the end of the process, the built container is deployed to Google Cloud Run and a message similar to the one below appears in the console output:\n\n  ```shell\n  Deployment for image gcr.io/PROJECT/myapp@sha256:b1cf... now available at https://...run.app\n  ```\n\nBrowse to the URL shown in the deployment message to see the running application.\n\nIf you deployed the example application from [Appendix A](#appendix-a-create-a-github-repository-with-an-example-express-application), you should see a page similar to that shown below:\n\n![Result of running pipeline from local host](/img/current_docs/guides/github-google-cloud/local-deployment.png)\n\n## Step 5: Create a GitHub Actions workflow\n\nDagger executes your pipelines entirely as standard OCI containers. This means that the same pipeline will run the same, whether on on your local machine or a remote server.\n\nThis also means that it's very easy to move your Dagger pipeline from your local host to GitHub Actions - all that's needed is to commit and push the pipeline script from your local clone to your GitHub repository, and then define a GitHub Actions workflow to run it on every commit.\n\n1. Commit and push the pipeline script and related changes to the application's GitHub repository:\n\n  ```shell\n  git add .\n  git commit -a -m \"Added pipeline\"\n  git push\n  ```\n\n1. In the GitHub repository, create a new workflow file at `.github/workflows/main.yml` with the following content:\n\n  <Tabs groupId=\"language\">\n  <TabItem value=\"Go\">\n\n  ```yaml file=./snippets/github-google-cloud/github-go.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Node.js\">\n\n  ```yaml file=./snippets/github-google-cloud/github-nodejs.yml\n  ```\n\n  </TabItem>\n  <TabItem value=\"Python\">\n\n  ```yaml file=./snippets/github-google-cloud/github-python.yml\n  ```\n\n  </TabItem>\n  </Tabs>\n\n  This workflow runs on every commit to the repository `master` branch. It consists of a single job with seven steps, as below:\n    - The first step uses the [Checkout action](https://github.com/marketplace/actions/checkout) to check out the latest source code from the `main` branch to the GitHub runner.\n    - The second step uses the [Authenticate to Google Cloud action](https://github.com/marketplace/actions/authenticate-to-google-cloud) to authenticate to Google Cloud. It requires a service account key in JSON format, which it expects to find in the `GOOGLE_CREDENTIALS` GitHub secret. This step sets various environment variables (including the GOOGLE_APPLICATION_CREDENTIALS variable required by the Google Cloud Run SDK) and returns an access token as output, which is used to authenticate the next step.\n    - The third step uses the [Docker Login action](https://github.com/marketplace/actions/docker-login) and the access token from the previous step to authenticate to Google Container Registry from the GitHub runner. This is necessary because Dagger relies on the host's Docker credentials and authorizations when publishing to remote registries.\n    - The fourth and fifth steps download and install the programming language and required dependencies (such as the Dagger SDK and the Google Cloud Run SDK) on the GitHub runner.\n    - The sixth step downloads and installs the Dagger CLI on the GitHub runner.\n    - The seventh and final step executes the Dagger pipeline.\n\nThe [Authenticate to Google Cloud action](https://github.com/marketplace/actions/authenticate-to-google-cloud) looks for a JSON service account key in the `GOOGLE_CREDENTIALS` GitHub secret. Create this secret as follows:\n\n1. Navigate to the `Settings` -> `Secrets` -> `Actions` page in the GitHub Web interface.\n1. Click `New repository secret` to create a new secret.\n1. Configure the secret with the following inputs:\n    - Name: `GOOGLE_CREDENTIALS`\n    - Secret: The contents of the service account JSON key file downloaded in Step 1.\n1. Click `Add secret` to save the secret.\n\n![Create GitHub secret](/img/current_docs/guides/github-google-cloud/create-github-secret.png)\n\n## Step 6: Test the Dagger pipeline on GitHub\n\nTest the Dagger pipeline by committing a change to the GitHub repository.\n\nIf you are using the example application described in [Appendix A](#appendix-a-create-a-github-repository-with-an-example-express-application), the following commands modify and commit a simple change to the application's index page:\n\n```shell\ngit pull\nsed -i 's/Dagger/Dagger on GitHub/g' routes/index.js\ngit add routes/index.js\ngit commit -a -m \"Update welcome message\"\ngit push\n```\n\nThe commit triggers the GitHub Actions workflow defined in Step 6. The workflow runs the various steps of the `dagger` job, including the pipeline script.\n\nAt the end of the process, a new version of the built container image is released to Google Container Registry and deployed on Google Cloud Run. A message similar to the one below appears in the GitHub Actions log:\n\n```shell\nDeployment for image gcr.io/PROJECT/myapp@sha256:h4si... now available at https://...run.app\n```\n\nBrowse to the URL shown in the deployment message to see the running application. If you deployed the example application with the additional modification above, you see a page similar to that shown below:\n\n![Result of running pipeline from GitHub](/img/current_docs/guides/github-google-cloud/github-actions-deployment.png)\n\n## Conclusion\n\nThis tutorial walked you through the process of creating a Dagger pipeline to continuously build and deploy a Node.js application on Google Cloud Run. It used the Dagger SDKs and explained key concepts, objects and methods available in the SDKs to construct a Dagger pipeline.\n\nDagger executes your pipelines entirely as standard OCI containers. This means that pipelines can be tested and debugged locally, and that the same pipeline will run consistently on your local machine, a CI runner, a dedicated server, or any container hosting service. This portability is one of Dagger's key advantages, and this tutorial demonstrated it in action by using the same pipeline on the local host and on GitHub.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.\n\n## Appendix A: Create a GitHub repository with an example Express application\n\nThis tutorial assumes that you have a GitHub repository with a Node.js Web application. If not, follow the steps below to create a GitHub repository and commit an example Express application to it.\n\n1. Log in to GitHub using the GitHub CLI:\n\n  ```shell\n  gh auth login\n  ```\n\n1. Create a directory for the Express application:\n\n  ```shell\n  mkdir myapp\n  cd myapp\n  ```\n\n1. Create a skeleton Express application:\n\n  ```shell\n  npx express-generator\n  ```\n\n1. Make a minor modification to the application's index page:\n\n  ```shell\n  sed -i -e 's/Express/Dagger/g' routes/index.js\n  ```\n\n1. Initialize a local Git repository for the application:\n\n  ```shell\n  git init\n  ```\n\n1. Add a `.gitignore` file and commit the application code:\n\n  ```shell\n  echo node_modules >> .gitignore\n  git add .\n  git commit -a -m \"Initial commit\"\n  ```\n\n1. Create a private repository in your GitHub account and push the changes to it:\n\n  ```shell\n  gh repo create myapp --push --source . --private\n  ```","contentTitle":"Use Dagger with GitHub Actions and Google Cloud","excerpt":"Watch a live demo of this tutorial in the Dagger Community Call (12 Jan 2023). For more demos, join the next Dagger Community Call.","timestamp":1670803200000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/648384-multi-builds.mdx","frontMatter":{"slug":"/648384/multi-builds","displayed_sidebar":"current","category":"guides","tags":["python","go","nodejs"],"authors":["Helder Correia"],"date":"2022-11-22"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Create a Multi-Build CI Pipeline\n\n## Introduction\n\nThe Dagger SDKs makes it easy to build an application for multiple OS and architecture combinations. This guide provides a working example of a CI tool that performs this task.\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have an application that you wish to build. This guide assumes a Go application, but you can use an application of your choice.\n\n:::tip\nDagger pipelines are executed as standard OCI containers. This portability enables you to do very powerful things. For example, if you're a Python developer, you can use the Python SDK to create a pipeline (written in Python) that builds an application written in a different language (Go) without needing to learn that language.\n:::\n\n## Example\n\nAssume that the Go application to be built is stored in the current directory on the host. The following code listing demonstrates how to build this Go application for multiple OS and architecture combinations using the Dagger SDKs.\n\n<Tabs groupId=\"language\" className=\"embeds\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/multi-builds/main.go\n```\n\nThis code listing does the following:\n\n- It defines the build matrix, consisting of two OSs (`darwin` and `linux`) and two architectures (`amd64` and `arm64`).\n- It creates a Dagger client with `Connect()`.\n- It uses the client's `Host().Directory(\".\")` method to obtain a reference to the current directory on the host. This reference is stored in the `src` variable.\n- It uses the client's `Container().From()` method to initialize a new container from a base image. This base image contains all the tooling needed to build the application - in this case, the `golang:latest` image. This `From()` method returns a new `Container` class with the results.\n- It uses the `Container.Directory()` method to mount the host directory into the container at the `/src` mount point.\n- It uses the `Container.WithWorkdir()` method to set the working directory in the container.\n- It iterates over the build matrix, creating a directory in the container for each OS/architecture combination and building the Go application for each such combination. The Go build process is instructed via the `GOOS` and `GOARCH` build variables, which are reset for each case via the `Container.WithEnvVariable()` method.\n- It obtains a reference to the build output directory in the container with the `WithDirectory()` method, and then uses the `Directory.Export()` method to write the build directory from the container to the host.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```javascript file=./snippets/multi-builds/index.mjs\n```\n\nThis code listing does the following:\n\n- It defines the build matrix, consisting of two OSs (`darwin` and `linux`) and two architectures (`amd64` and `arm64`).\n- It creates a Dagger client with `connect()`.\n- It uses the client's `host().directory(\".\")` method to obtain a reference to the current directory on the host. This reference is stored in the `src` variable.\n- It uses the client's `container().from()` method to initialize a new container from a base image. This base image contains all the tooling needed to build the application - in this case, the `golang:latest` image. This `from()` method returns a new `Container` class with the results.\n- It uses the `Container.withDirectory()` method to return the container image with the host directory written at the `/src` path.\n- It uses the `Container.withWorkdir()` method to set the working directory in the container.\n- It iterates over the build matrix, creating a directory in the container for each OS/architecture combination and building the Go application for each such combination. The Go build process is instructed via the `GOOS` and `GOARCH` build variables, which are reset for each case via the `Container.withEnvVariable()` method.\n- It obtains a reference to the build output directory in the container with the `withDirectory()` method, and then uses the `Directory.export()` method to write the build directory from the container to the host.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/multi-builds/main.py\n```\n\nThis code listing does the following:\n\n- It defines the build matrix, consisting of two OSs (`darwin` and `linux`) and two architectures (`amd64` and `arm64`).\n- It creates a Dagger client with `dagger.Connection()`.\n- It uses the client's `host().directory(\".\")` method to obtain a reference to the current directory on the host. This reference is stored in the `src` variable.\n- It uses the client's `container().from_()` method to initialize a new container from a base image. This base image contains all the tooling needed to build the application - in this case, the `golang:latest` image. This `from_()` method returns a new `Container` class with the results.\n- It uses the `Container.with_directory()` method to mount the host directory into the container at the `/src` mount point.\n- It uses the `Container.with_workdir()` method to set the working directory in the container.\n- It iterates over the build matrix, creating a directory in the container for each OS/architecture combination and building the Go application for each such combination. The Go build process is instructed via the `GOOS` and `GOARCH` build variables, which are reset for each case via the `Container.with_env_variable()` method.\n- It obtains a reference to the build output directory in the container with the `with_directory()` method, and then uses the `Directory.export()` method to write the build directory from the container to the host.\n\n</TabItem>\n</Tabs>\n\n## Conclusion\n\nThis guide showed you how to build an application for multiple OS and architecture combinations with Dagger.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Create a Multi-Build CI Pipeline","excerpt":"Introduction","timestamp":1669075200000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/406009-multiplatform-support.mdx","frontMatter":{"slug":"/406009/multiplatform-support","displayed_sidebar":"current","category":"guides","tags":["go"],"authors":["Erik Sipsma"],"date":"2022-11-20"},"content":"# Understand Multi-Platform Support\n\n## Introduction\n\nDagger supports pulling container images and executing commands on platforms that differ from the underlying host.\n\nFor example, you can use Dagger to compile binaries that target different CPU architectures, test those binaries and push them to a registry bundled as a multi-platform container image, _all on a single host_.\n\nThis document explores these features through the lens of the Go SDK.\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go development environment with Go 1.20 or later. If not, [download and install Go](https://go.dev/doc/install).\n- You are familiar with the basics of the Go SDK and have it installed. If not, read the [Go SDK guide](../sdk/go/959738-get-started.mdx) and the [Go SDK installation instructions](../sdk/go/371491-install.mdx).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have `binfmt_misc` configured on the host kernel (that is, on the machine where Dagger will run containers). This is necessary to execute binaries in containers that use a different architecture than that of the host.\n\n  If the host is running Docker, this one-liner will setup everything (only needed once per boot cycle):\n\n  ```sh\n  docker run --privileged --rm tonistiigi/binfmt --install all\n  ```\n\n  Learn more in the [`binfmt` documentation](https://github.com/tonistiigi/binfmt/).\n\n  :::note\n  Dagger users running on MacOS can ignore these instructions. By default on MacOS, Dagger will run inside a Linux VM that should already be configured with `binfmt_misc`.\n  :::\n\n## Terminology\n\n### Platform\n\n- A combination of OS and CPU architecture that executable code may target.\n- Registries compatible with the OCI Image Spec support pulling and pushing images with layers for different platforms all bundled together ([see the spec here](https://github.com/opencontainers/image-spec/blob/main/image-index.md#image-index-property-descriptions))\n\n### Emulation\n\n- A technique by which a binary built to target one CPU architecture can be executed on a different CPU architecture via automatic conversion of machine code.\n- Typically quite slow relative to executing on a native CPU.\n- For builds, cross-compilation will generally be much faster when it's an option.\n\n### Cross-compilation\n\n- A technique by which you can build a binary that targets Platform A on a machine of Platform B. I.e. cross-compilation enables you to build a Windows x86_64 binary from a Linux aarch64 host.\n\n## Examples\n\n### Pull images and execute commands for multiple architectures\n\nThis example demonstrates how to pull images for multiple different architectures and execute commands on each of them.\n\n```go file=./snippets/multiplatform-support/pull-images/main.go\n\n```\n\nAs illustrated above, you can optionally initialize a `Container` with a specific platform. That platform will be used to pull images and execute any commands.\n\nIf the platform of the `Container` does not match that of the host, then emulation will be used for any commands specified in `WithExec`.\n\nIf you don't specify a platform, the `Container` will be initialized with a platform matching that of the host.\n\n### Create new multi-platform images\n\nThe next step builds on the previous example by:\n\n1. Building binaries for each of the platforms. We'll use Go binaries for this example.\n1. Combining those binaries into a multi-platform image that we push to a registry.\n\nStart by running builds using emulation. The next example will show the changes needed to instead perform cross-compilation while still building a multi-platform image.\n\n:::note\nThis example will fail to push the final image unless you change the registry to one that you control and have write permissions for.\n:::\n\n```go file=./snippets/multiplatform-support/build-images-emulation/main.go\n\n```\n\n### Use cross-compilation\n\nThe previous example results in emulation being used to build the binary for different architectures.\n\nEmulation is great to have because it requires no customization of build options; the exact same build can be run for different platforms.\n\nHowever, emulation has the downside of being quite slow relative to executing native CPU instructions.\n\nWhile cross-compilation is sometimes much easier said than done, it's a great option for speeding up multi-platform builds when feasible.\n\nFortunately, Go has great built-in support for cross-compilation, so modifying the previous example to use this feature instead is straightforward:\n\n```go file=./snippets/multiplatform-support/build-images-cross-compilation/main.go\n\n```\n\nThe only changes we made to enable faster cross-compilation are:\n\n1. Pulling the base `golang` image for the host platform\n1. Configuring the Go compiler to target the specific platform\n\nThe final image is still multi-platform because each `Container` set as a `PlatformVariant` was initialized with a specific platform (after the cross-compilation has occurred, at the bottom of the `for` loop in the code above).\n\n### Add caching\n\nCaching can significantly improve the speed of your builds in Dagger by saving and reusing the results of expensive operations, such as downloading dependencies or compiling code. Dagger supports caching via the `withMountedCache()` function, which mounts a cache directory from the host machine into the container.\n\nThe following code listing compiles the Go project twice. The first compilation will take longer because the cache directories are empty. The second compilation should be faster, as the cache directories will contain the necessary Go modules and build outputs from the first compilation:\n\n```go file=./snippets/multiplatform-support/cache/main.go\n\n```\n\nThis example defines a `build()` function that sets up the container, mounts the cache directories, and compiles the Go project. The function runs twice, and the build time for each run is measured. The second build is faster, demonstrating the benefits of using cache mounts.\n\n## Support for non-Linux platforms\n\nThe previous examples work with different architectures but the OS of the platform is always `linux`.\n\nAs explored in our [Get Started tutorial](../sdk/go/959738-get-started.mdx), Dagger can run cross-compilation builds that create binaries targeting other OSes such as Darwin (MacOS) and Windows.\n\nAdditionally, Dagger has _limited_ support for some operations involving non-Linux container images. Specifically, it is often possible to pull these images and perform basic file operations, but attempting to execute commands will result in an error:\n\n```go file=./snippets/multiplatform-support/non-linux-support/main.go\n\n```\n\n:::note\nLearn more about [support for executing commands on non-Linux OSes in this tracking issue](https://github.com/dagger/dagger/issues/3158).\n:::\n\n## FAQ\n\n### What is the default value of platform if I don't specify it?\n\nThe platform will default to that of the machine running your containers.\n\nIf you are running Dagger from MacOS, by default your containers will run in a Linux virtual machine, so your platform will default to either `linux/amd64` (on Intel Macs) or `linux/arm64` (on ARM Macs).\n\n### How do I know the valid values of platform?\n\nThe names of OSes and CPU architectures that we support are inherited from the [OCI image spec](https://github.com/opencontainers/image-spec/blob/main/image-index.md#image-index-property-descriptions), which in turn inherits names used by Go.\n\nYou can see the full list of valid platform strings by running the command `go tool dist list`. Some examples include:\n\n- `linux/386`\n- `linux/amd64`\n- `linux/arm`\n- `linux/arm64`\n- `linux/mips`\n- `linux/mips64`\n- `linux/mips64le`\n- `linux/mipsle`\n- `linux/ppc64`\n- `linux/ppc64le`\n- `linux/riscv64`\n- `linux/s390x`\n- `windows/386`\n- `windows/amd64`\n- `windows/arm`\n- `windows/arm64`\n\nWhether a particular platform can be used successfully with Dagger depends on several factors:\n\n- Whether an image you are pulling has a published version for that platform\n- Whether QEMU emulation is supported for the architecture and has been configured (as described in Requirements above)\n- Whether the OS is Linux (command execution only works on Linux for now)","contentTitle":"Understand Multi-Platform Support","excerpt":"Introduction","timestamp":1668902400000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/544174-multistage-build.mdx","frontMatter":{"slug":"/544174/multistage-build","displayed_sidebar":"current","category":"guides","tags":["go","python","nodejs"],"authors":["Kyle Penfound"],"date":"2022-11-09"},"content":"# Use Dagger with Multi-stage Container Builds\n\nMulti-stage builds are a common practice when building containers with Docker.\n\n- First, your application is compiled in a context which has tools that are required for building the application, but not necessarily required for running it.\n- Next, to reduce the number of dependencies and hence the size of the image, the compiled application is copied to a different base image which only has the required components to run the application.\n\n:::tip\n[Learn more about multi-stage builds in the Docker documentation](https://docs.docker.com/build/building/multi-stage/).\n:::\n\nThis guide explains how to perform multi-stage builds with the Dagger SDKs.\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n- You have an application that you wish to build. This guide assumes a Go application, but you can use an application of your choice.\n\n## Example\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nThe following code snippet demonstrates a multi-stage build.\n\n<Tabs groupId=\"language\" className=\"embeds\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/multistage-build/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/multistage-build/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/multistage-build/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThis code listing starts by creating a Dagger client and loading the project to be built. It obtains a reference to the project and then builds the application by using the `golang:latest` image to mount the source directory, sets `CGO_ENABLED=` since the binary will be published on `alpine`, and executes `go build`.\n\nNext, the multi-stage build is achieved by transferring the build artifact from the builder image to a runtime image based on `alpine`. The steps are:\n\n- Create a new container image which will be used as the runtime image.\n- Transfer the build artifact from the builder image to the new container image.\n- Set the container entrypoint to the application so that it is executed by default when the container runs.\n\nThe final optimized image can now be pushed to a registry and deployed!\n\n## Conclusion\n\nThis tutorial walked you through the process of performing a multi-stage build with Docker.\n\nUse the [API Key Concepts](../api/975146-concepts.mdx) page and the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Use Dagger with Multi-stage Container Builds","excerpt":"Multi-stage builds are a common practice when building containers with Docker.","timestamp":1667952000000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/110632-embed-directories.mdx","frontMatter":{"slug":"/110632/embed-directories","displayed_sidebar":"current","category":"guides","tags":["go"],"authors":["Guillaume de Rouville"],"date":"2022-11-04"},"content":"# Copy Embedded Directories into a Container\n\nEmbedded directories in Go allow bundling static files with the compiled binary. This guide demonstrates copying embedded directories using Dagger, a [Go-specific](https://pkg.go.dev/embed) feature not available in Python or Node.js.\n\nDagger does not expose the option to copy entire directories as a single step (yet), whether it is between containers or from an embedded directory to a container. It is, however, doable by traversing the directory tree.\n\nAssume that you have a Dagger CI tool containing the following code structure, which contains an example directory:\n\n```shell\ntree\n.\n├── go.mod\n├── go.sum\n├── main.go\n└── example\n    └── foo.go\n```\n\nThe following example demonstrates how to copy an embedded directory:\n\n```go file=./snippets/embed-directories/main.go\n```\n\nAttempt to run the code and print the content of the `/embed` directory:\n\n```shell\n➜  dagger run go run .\n/embed/:\ntotal 4\ndrwxr-xr-x    1 root     root          4096 Oct 31 16:49 example\n\n/embed/example:\ntotal 4\n-rw-r--r--    1 root     root            50 Oct 31 16:49 foo.go\n```\n\nIn this case, the function succeeds in copying the embedded `example` directory.\n\n:::warning\nYou may encounter errors if your directory contains +1000 files, due to the concatenation of the queries.\n:::","contentTitle":"Copy Embedded Directories into a Container","excerpt":"Embedded directories in Go allow bundling static files with the compiled binary. This guide demonstrates copying embedded directories using Dagger, a Go-specific feature not available in Python or Node.js.","timestamp":1667520000000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/421437-work-with-host-filesystem.mdx","frontMatter":{"slug":"/421437/work-with-host-filesystem","displayed_sidebar":"current","category":"guides","tags":["go","python","nodejs"],"authors":["Alex Suraci","Vikram Vaswani"],"date":"2022-11-01"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Work with the Host Filesystem\n\n## Introduction\n\nThis guide explains how to work with the host filesystem using the Dagger SDKs. You will learn how to:\n\n- Set the working directory on the host\n- List host directory entries with include/exclude filters\n- Mount a host directory in a container\n- Export a directory from a container to the host\n\n## Requirements\n\nThis guide assumes that:\n\n- You have a Go, Python or Node.js development environment. If not, install [Go](https://go.dev/doc/install), [Python](https://www.python.org/downloads/) or [Node.js](https://nodejs.org/en/download/).\n- You have a Dagger SDK installed for one of the above languages. If not, follow the installation instructions for the Dagger [Go](../sdk/go/371491-install.mdx), [Python](../sdk/python/866944-install.mdx) or [Node.js](../sdk/nodejs/835948-install.mdx) SDK.\n- You have the Dagger CLI installed in your development environment. If not, [install the Dagger CLI](../cli/465058-install.mdx).\n- You have Docker installed and running on the host system. If not, [install Docker](https://docs.docker.com/engine/install/).\n\n## List directory contents\n\nThe easiest way to set the working directory for the Dagger CI pipeline is at the time of client instantiation, as a client configuration option. By default, Dagger uses the current directory on the host as the working directory.\n\nThe following example shows how to list the contents of the working directory:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/list-dir/main.go\n```\n\nThe `Host` type provides information about the host's execution environment. Its `Directory()` method accepts a path and returns a reference to the corresponding host directory as a `Directory` struct. Entries in the directory can be obtained via the `Directory.Entries()` function.\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/list-dir/index.mts\n```\n\nThe `host` type provides information about the host's execution environment. Its `directory()` method accepts a path and returns a reference to the corresponding host directory as a `Directory` object. Entries in the directory can be obtained via the `directory.entries()` function.\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/list-dir/main.py\n```\n\nThe `host` type provides information about the host's execution environment. Its `directory()` method accepts a path and returns a reference to the corresponding host directory as a `Directory` object. Entries in the directory can be obtained via the `directory.entries()` function.\n\n</TabItem>\n</Tabs>\n\n## List directory contents with filters\n\nIt's possible to restrict a `Directory` to a subset of directory entries, by specifying a list of filename patterns to include or exclude.\n\nThe following example shows how to obtain a reference to the host working directory containing only `*.rar` files:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/list-dir-include/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/list-dir-include/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/list-dir-include/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThe following example shows how to obtain a reference to the host working directory containing all files except `*.txt` files:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/list-dir-exclude/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/list-dir-exclude/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/list-dir-exclude/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThe exclusion pattern overrides the inclusion pattern, but not vice-versa. The following example demonstrates by obtaining a reference to the host working directory containing all files except `*.rar` files:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/list-dir-exclude-include/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/list-dir-exclude-include/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/list-dir-exclude-include/main.py\n```\n\n</TabItem>\n</Tabs>\n\nThe exclusion pattern overrides the inclusion pattern, but not vice-versa. The following example demonstrates by obtaining a reference to the host working directory containing all `.rar` and `.txt` files except `.out` files using glob patterns:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/glob-pattern/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/glob-pattern/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/glob-pattern/main.py\n```\n\n</TabItem>\n</Tabs>\n\n## Export a directory from a container to the host\n\nA directory can be exported to a different path. The destination path is supplied to the method as an argument.\n\nThe following example creates a file in a container's `/tmp` directory and then exports the contents of that directory to the host's temporary directory:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/export-dir/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/export-dir/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/export-dir/main.py\n```\n\n</TabItem>\n</Tabs>\n\n## Write a host directory to a container\n\nA common operation when working with containers is to write a host directory to a path in the container and then perform operations on it. It is necessary to provide the filesystem location in the container and the directory to be written as method arguments.\n\nThe following example shows how to write a host directory to a container at the `/host` container path and then read the contents of the directory:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/transfer-dir/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/transfer-dir/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/transfer-dir/main.py\n```\n\n</TabItem>\n</Tabs>\n\n:::note\nModifications made to a host directory written to a container filesystem path do not appear on the host. Data flows only one way between Dagger operations, because they are connected in a DAG. To write modifications back to the host directory, you must explicitly export the directory back to the host filesystem.\n:::\n\nThe following example shows how to transfer a host directory to a container at the `/host` container path, write a file to it, and then export the modified directory back to the host:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/work-with-host-filesystem/transfer-dir-export/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js\">\n\n```typescript file=./snippets/work-with-host-filesystem/transfer-dir-export/index.mts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/work-with-host-filesystem/transfer-dir-export/main.py\n```\n\n</TabItem>\n</Tabs>\n\n## Important notes\n\nUsing the host filesystem in your Dagger pipeline is convenient, but there are some important considerations to keep in mind:\n\n- With the exception of mounted cache volumes, if a file or directory mounted from the host changes even slightly (including minor changes such as a timestamp change with the file contents left unmodified), then the Dagger pipeline operations cache will be invalidated. An extremely common source of invalidations occurs when loading the `.git` directory from the host filesystem, as that directory will change frequently, including when there have been no actual changes to any source code.\n\n  :::tip\n  To maximize cache re-use, it's important to use the include/exclude options for local directories to only include the files/directories needed for the pipeline. Excluding the `.git` directory is highly advisable unless there's a strong need to be able to perform Git operations on top of the loaded directory inside Dagger.\n  :::\n\n- The host directory is synchronized into the Dagger Engine similar to `rsync` or `scp`; it's not a \"bind mount\". This means that any change you make to the loaded directory in your Dagger pipeline will not result in a change to the directory on the host.\n\n  :::warning\n  If you want the changes made to a loaded local directory inside a Dagger pipeline to be reflected on the host, it needs to be explictly exported to the host. However, this should be approached with caution, since any overlap in the files being exported with the files on the host will result in the host files being overwritten.\n  :::\n\n- Synchronization of a local directory happens once per Dagger client instance (in user-facing terms, once per `dagger.Connect` call in the Dagger SDKs). This means that if you load the local directory, then make changes to it on the host, those changes will not be reloaded within the context of a single Dagger client. Furthermore, due to lazy executions, the loading happens the first time the directory is used in a non-lazy operation.\n\n  :::tip\n  It's safest to not modify a loaded host directory at all while a Dagger client is running, as otherwise it is hard to predict what will be loaded.\n  :::\n\n## Conclusion\n\nThis guide introduced you to the functions available in the Dagger SDKs to work with the host filesystem. It provided explanations and code samples demonstrating how to set the host working directory, read directory contents (with and without pathname filters), mount a host directory in a container and export a directory from a container to the host.\n\nUse the [Go](https://pkg.go.dev/dagger.io/dagger), [Node.js](../sdk/nodejs/reference/modules.md) and [Python](https://dagger-io.readthedocs.org/) SDK References to learn more about Dagger.","contentTitle":"Work with the Host Filesystem","excerpt":"Introduction","timestamp":1667260800000},{"path":"/home/vikram/public/vikram.dagger/docs/current_docs/guides/710884-private-repositories.mdx","frontMatter":{"slug":"/710884/private-repositories","displayed_sidebar":"current","category":"guides","tags":["go"],"authors":["Guillaume de Rouville"],"date":"2022-10-31"},"content":"import Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n\n# Use Dagger with Private Git Repositories\n\nDagger recommends you to rely on your host's SSH authentication agent to securely authenticate against private remote Git repositories.\n\nTo clone private repositories, the only requirements are to run `ssh-add` on the Dagger host (to add your SSH key to the authentication agent), and mount its socket using the `SSHAuthSocket` parameter of the `(Dagger.GitRef).Tree` API.\n\nAssume that you have a Dagger CI tool containing the following code, which references a private repository:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```go file=./snippets/private-repositories/main.go\n```\n\n</TabItem>\n<TabItem value=\"Node.js (TypeScript)\">\n\n```typescript file=./snippets/private-repositories/clone.ts\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```python file=./snippets/private-repositories/clone.py\n```\n\n</TabItem>\n</Tabs>\n\nNow, first remove all the SSH keys from the authentication agent on the Dagger host:\n\n```shell\n➜  ssh-add -D\nAll identities removed.\n```\n\nAttempt to run the CI tool:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\n➜  dagger run go run .\npanic: input:1: git.branch.tree.file.contents failed to load cache key: failed to fetch remote\nexit status 128\n```\n\n</TabItem>\n<TabItem value=\"Node.js (TypeScript)\">\n\n```shell\n➜  dagger run node --loader ts-node/esm clone.ts\n{'message': 'failed to load cache key: failed to fetch remote https://xxxxx@private-repository.git: exit status 128', 'locations': [{'line': 6, 'column': 11}], 'path': ['git', 'branch', 'tree', 'file', 'contents']}\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\n➜  dagger run python clone.py\n{'message': 'failed to load cache key: failed to fetch remote https://xxxxx@private-repository.git: exit status 128', 'locations': [{'line': 6, 'column': 11}], 'path': ['git', 'branch', 'tree', 'file', 'contents']}\n```\n\n</TabItem>\n</Tabs>\n\nThe CI tool fails, as it is unable to find the necessary authentication credentials to read the private repository in the SSH authentication agent.\n\nNow, add the SSH key to the authentication agent on the host and try again:\n\n<Tabs groupId=\"language\">\n<TabItem value=\"Go\">\n\n```shell\n➜ ssh-add\nIdentity added: xxxxx\ndagger run go run .\nreadme #\n```\n\n</TabItem>\n<TabItem value=\"Node.js (TypeScript)\">\n\n```shell\n➜ ssh-add\nIdentity added: xxxxx\n➜ dagger run node --loader ts-node/esm clone.ts\nreadme #\n```\n\n</TabItem>\n<TabItem value=\"Python\">\n\n```shell\n➜ ssh-add\nIdentity added: xxxxx\n➜ dagger run python clone.py\nreadme #\n```\n\n</TabItem>\n</Tabs>\n\nFinally, the CI tool succeeds in reading the private Git repository.","contentTitle":"Use Dagger with Private Git Repositories","excerpt":"Dagger recommends you to rely on your host's SSH authentication agent to securely authenticate against private remote Git repositories.","timestamp":1667174400000}],"allTags":["kubernetes","tekton","go","python","nodejs","github","eks","dagger-cloud","argo","java","spring","aws-lambda","aws","aws-codepipelines","aws-codebuild","azure","azure-pipelines","azure-container-instances","podman","php","laravel","cdk","ecs","fargate","gitlab-ci","google-cloud","github-actions","circle-ci","jenkins"]}